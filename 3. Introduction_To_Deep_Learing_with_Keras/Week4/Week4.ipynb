{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5AF3iIRAtzl"
      },
      "source": [
        "<h2>Convolutional Neural Networks with Keras</h2>\r\n",
        "\r\n",
        "<h3>Objective for this Notebook<h3>    \r\n",
        "<h5> 1. How to use the Keras library to build convolutional neural networks.</h5>\r\n",
        "<h5> 2. Convolutional Neural Network with One Convolutional and Pooling Layers.</h5>\r\n",
        "<h5> 3. Convolutional Neural Network with Two Convolutional and Pooling Layers.</h5>\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7ra-qPPAunM"
      },
      "source": [
        "import keras\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.utils import to_categorical\r\n",
        "from keras.layers.convolutional import Conv2D # to add convolutional layers\r\n",
        "from keras.layers.convolutional import MaxPooling2D # to add pooling layers\r\n",
        "from keras.layers import Flatten # to flatten data for fully connected layers"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D37Z9g9TA0BL"
      },
      "source": [
        "## Convolutional Layer with One set of convolutional and pooling layers\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ipy1UXxA2Gs"
      },
      "source": [
        "# import data\r\n",
        "from keras.datasets import mnist\r\n",
        "\r\n",
        "# load data\r\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\r\n",
        "\r\n",
        "# reshape to be [samples][pixels][width][height]\r\n",
        "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32')\r\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32')"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ocediz15A6FQ"
      },
      "source": [
        "X_train = X_train / 255 # normalize training data\r\n",
        "X_test = X_test / 255 # normalize test data"
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9ufDUFWA7GJ"
      },
      "source": [
        "y_train = to_categorical(y_train)\r\n",
        "y_test = to_categorical(y_test)\r\n",
        "\r\n",
        "num_classes = y_test.shape[1] # number of categories"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKvRHlIwA9ne"
      },
      "source": [
        "def convolutional_model():\r\n",
        "    \r\n",
        "    # create model\r\n",
        "    model = Sequential()\r\n",
        "    model.add(Conv2D(16, (5, 5), strides=(1, 1), activation='relu', input_shape=(28, 28, 1)))\r\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\r\n",
        "    \r\n",
        "    model.add(Flatten())\r\n",
        "    model.add(Dense(100, activation='relu'))\r\n",
        "    model.add(Dense(num_classes, activation='softmax'))\r\n",
        "    \r\n",
        "    # compile model\r\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy',  metrics=['accuracy'])\r\n",
        "    return model"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sYn3qhJBF3i",
        "outputId": "f1b46793-787f-495a-aea1-e8b60db95977"
      },
      "source": [
        "# build the model\r\n",
        "model = convolutional_model()\r\n",
        "\r\n",
        "# fit the model\r\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)\r\n",
        "\r\n",
        "# evaluate the model\r\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\r\n",
        "print(\"Accuracy: {} \\n Error: {}\".format(scores[1], 100-scores[1]*100))"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "300/300 - 16s - loss: 0.2856 - accuracy: 0.9216 - val_loss: 0.0882 - val_accuracy: 0.9747\n",
            "Epoch 2/10\n",
            "300/300 - 16s - loss: 0.0793 - accuracy: 0.9763 - val_loss: 0.0598 - val_accuracy: 0.9818\n",
            "Epoch 3/10\n",
            "300/300 - 16s - loss: 0.0537 - accuracy: 0.9844 - val_loss: 0.0460 - val_accuracy: 0.9846\n",
            "Epoch 4/10\n",
            "300/300 - 16s - loss: 0.0425 - accuracy: 0.9872 - val_loss: 0.0463 - val_accuracy: 0.9841\n",
            "Epoch 5/10\n",
            "300/300 - 16s - loss: 0.0345 - accuracy: 0.9895 - val_loss: 0.0365 - val_accuracy: 0.9875\n",
            "Epoch 6/10\n",
            "300/300 - 16s - loss: 0.0292 - accuracy: 0.9909 - val_loss: 0.0346 - val_accuracy: 0.9883\n",
            "Epoch 7/10\n",
            "300/300 - 16s - loss: 0.0240 - accuracy: 0.9927 - val_loss: 0.0356 - val_accuracy: 0.9882\n",
            "Epoch 8/10\n",
            "300/300 - 16s - loss: 0.0189 - accuracy: 0.9945 - val_loss: 0.0341 - val_accuracy: 0.9891\n",
            "Epoch 9/10\n",
            "300/300 - 16s - loss: 0.0156 - accuracy: 0.9954 - val_loss: 0.0455 - val_accuracy: 0.9859\n",
            "Epoch 10/10\n",
            "300/300 - 16s - loss: 0.0137 - accuracy: 0.9960 - val_loss: 0.0399 - val_accuracy: 0.9879\n",
            "Accuracy: 0.9879000186920166 \n",
            " Error: 1.2099981307983398\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPCvkTtRBQ19"
      },
      "source": [
        "## Convolutional Layer with two sets of convolutional and pooling layers\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ik1OupDBN_E"
      },
      "source": [
        "def convolutional_model():\r\n",
        "    \r\n",
        "    # create model\r\n",
        "    model = Sequential()\r\n",
        "    model.add(Conv2D(16, (5, 5), activation='relu', input_shape=(28, 28, 1)))\r\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\r\n",
        "    \r\n",
        "    model.add(Conv2D(8, (2, 2), activation='relu'))\r\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\r\n",
        "    \r\n",
        "    model.add(Flatten())\r\n",
        "    model.add(Dense(100, activation='relu'))\r\n",
        "    model.add(Dense(num_classes, activation='softmax'))\r\n",
        "    \r\n",
        "    # Compile model\r\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy',  metrics=['accuracy'])\r\n",
        "    return model"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCHYfjFZBbYd",
        "outputId": "9ad59d9c-1a30-450a-91ce-1a6ab86bf04e"
      },
      "source": [
        "# build the model\r\n",
        "model = convolutional_model()\r\n",
        "\r\n",
        "# fit the model\r\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)\r\n",
        "\r\n",
        "# evaluate the model\r\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\r\n",
        "print(\"Accuracy: {} \\n Error: {}\".format(scores[1], 100-scores[1]*100))"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "300/300 - 19s - loss: 0.4484 - accuracy: 0.8677 - val_loss: 0.1492 - val_accuracy: 0.9601\n",
            "Epoch 2/10\n",
            "300/300 - 18s - loss: 0.1191 - accuracy: 0.9648 - val_loss: 0.0863 - val_accuracy: 0.9742\n",
            "Epoch 3/10\n",
            "300/300 - 18s - loss: 0.0802 - accuracy: 0.9760 - val_loss: 0.0635 - val_accuracy: 0.9802\n",
            "Epoch 4/10\n",
            "300/300 - 18s - loss: 0.0635 - accuracy: 0.9803 - val_loss: 0.0547 - val_accuracy: 0.9831\n",
            "Epoch 5/10\n",
            "300/300 - 18s - loss: 0.0549 - accuracy: 0.9833 - val_loss: 0.0476 - val_accuracy: 0.9843\n",
            "Epoch 6/10\n",
            "300/300 - 18s - loss: 0.0478 - accuracy: 0.9856 - val_loss: 0.0452 - val_accuracy: 0.9846\n",
            "Epoch 7/10\n",
            "300/300 - 18s - loss: 0.0426 - accuracy: 0.9869 - val_loss: 0.0347 - val_accuracy: 0.9879\n",
            "Epoch 8/10\n",
            "300/300 - 18s - loss: 0.0382 - accuracy: 0.9886 - val_loss: 0.0446 - val_accuracy: 0.9851\n",
            "Epoch 9/10\n",
            "300/300 - 18s - loss: 0.0341 - accuracy: 0.9894 - val_loss: 0.0387 - val_accuracy: 0.9868\n",
            "Epoch 10/10\n",
            "300/300 - 18s - loss: 0.0309 - accuracy: 0.9907 - val_loss: 0.0325 - val_accuracy: 0.9891\n",
            "Accuracy: 0.9890999794006348 \n",
            " Error: 1.0900020599365234\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}