{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PartB.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ghn3WIOzKELl"
      },
      "source": [
        "# Importing the neccessary libraries\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import statistics as stats\r\n",
        "import os\r\n",
        "import keras\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from sklearn import preprocessing\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import mean_squared_error\r\n",
        "from datetime import datetime\r\n",
        "\r\n",
        "# Forcing keras to use CPU.\r\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "id": "HzGihXDrKInQ",
        "outputId": "a7c8c0bd-eb50-4776-832f-255083d47471"
      },
      "source": [
        "# Reading the Data and storing it in a dataframe\r\n",
        "\r\n",
        "df = pd.read_csv('https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0101EN/labs/data/concrete_data.csv')\r\n",
        "df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Cement</th>\n",
              "      <th>Blast Furnace Slag</th>\n",
              "      <th>Fly Ash</th>\n",
              "      <th>Water</th>\n",
              "      <th>Superplasticizer</th>\n",
              "      <th>Coarse Aggregate</th>\n",
              "      <th>Fine Aggregate</th>\n",
              "      <th>Age</th>\n",
              "      <th>Strength</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>540.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>162.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1040.0</td>\n",
              "      <td>676.0</td>\n",
              "      <td>28</td>\n",
              "      <td>79.99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>540.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>162.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1055.0</td>\n",
              "      <td>676.0</td>\n",
              "      <td>28</td>\n",
              "      <td>61.89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>332.5</td>\n",
              "      <td>142.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>228.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>932.0</td>\n",
              "      <td>594.0</td>\n",
              "      <td>270</td>\n",
              "      <td>40.27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>332.5</td>\n",
              "      <td>142.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>228.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>932.0</td>\n",
              "      <td>594.0</td>\n",
              "      <td>365</td>\n",
              "      <td>41.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>198.6</td>\n",
              "      <td>132.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>192.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>978.4</td>\n",
              "      <td>825.5</td>\n",
              "      <td>360</td>\n",
              "      <td>44.30</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Cement  Blast Furnace Slag  Fly Ash  ...  Fine Aggregate  Age  Strength\n",
              "0   540.0                 0.0      0.0  ...           676.0   28     79.99\n",
              "1   540.0                 0.0      0.0  ...           676.0   28     61.89\n",
              "2   332.5               142.5      0.0  ...           594.0  270     40.27\n",
              "3   332.5               142.5      0.0  ...           594.0  365     41.05\n",
              "4   198.6               132.4      0.0  ...           825.5  360     44.30\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqTYkPvzKLTm",
        "outputId": "7fcfb8b3-413a-48e2-f617-2c8630623e2a"
      },
      "source": [
        "print('\\nShape of dataframe : ',df.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Shape of dataframe :  (1030, 9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "r8niVpLzKMhh",
        "outputId": "aa4620f8-0cb2-4c8b-adb5-ff2e630fdfa0"
      },
      "source": [
        "# Summary of the dataset\r\n",
        "df.describe()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Cement</th>\n",
              "      <th>Blast Furnace Slag</th>\n",
              "      <th>Fly Ash</th>\n",
              "      <th>Water</th>\n",
              "      <th>Superplasticizer</th>\n",
              "      <th>Coarse Aggregate</th>\n",
              "      <th>Fine Aggregate</th>\n",
              "      <th>Age</th>\n",
              "      <th>Strength</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>281.167864</td>\n",
              "      <td>73.895825</td>\n",
              "      <td>54.188350</td>\n",
              "      <td>181.567282</td>\n",
              "      <td>6.204660</td>\n",
              "      <td>972.918932</td>\n",
              "      <td>773.580485</td>\n",
              "      <td>45.662136</td>\n",
              "      <td>35.817961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>104.506364</td>\n",
              "      <td>86.279342</td>\n",
              "      <td>63.997004</td>\n",
              "      <td>21.354219</td>\n",
              "      <td>5.973841</td>\n",
              "      <td>77.753954</td>\n",
              "      <td>80.175980</td>\n",
              "      <td>63.169912</td>\n",
              "      <td>16.705742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>102.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>121.800000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>801.000000</td>\n",
              "      <td>594.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.330000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>192.375000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>164.900000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>932.000000</td>\n",
              "      <td>730.950000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>23.710000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>272.900000</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>185.000000</td>\n",
              "      <td>6.400000</td>\n",
              "      <td>968.000000</td>\n",
              "      <td>779.500000</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>34.445000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>350.000000</td>\n",
              "      <td>142.950000</td>\n",
              "      <td>118.300000</td>\n",
              "      <td>192.000000</td>\n",
              "      <td>10.200000</td>\n",
              "      <td>1029.400000</td>\n",
              "      <td>824.000000</td>\n",
              "      <td>56.000000</td>\n",
              "      <td>46.135000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>540.000000</td>\n",
              "      <td>359.400000</td>\n",
              "      <td>200.100000</td>\n",
              "      <td>247.000000</td>\n",
              "      <td>32.200000</td>\n",
              "      <td>1145.000000</td>\n",
              "      <td>992.600000</td>\n",
              "      <td>365.000000</td>\n",
              "      <td>82.600000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Cement  Blast Furnace Slag  ...          Age     Strength\n",
              "count  1030.000000         1030.000000  ...  1030.000000  1030.000000\n",
              "mean    281.167864           73.895825  ...    45.662136    35.817961\n",
              "std     104.506364           86.279342  ...    63.169912    16.705742\n",
              "min     102.000000            0.000000  ...     1.000000     2.330000\n",
              "25%     192.375000            0.000000  ...     7.000000    23.710000\n",
              "50%     272.900000           22.000000  ...    28.000000    34.445000\n",
              "75%     350.000000          142.950000  ...    56.000000    46.135000\n",
              "max     540.000000          359.400000  ...   365.000000    82.600000\n",
              "\n",
              "[8 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reQaLIoHKOXl",
        "outputId": "e73ad2cf-a27a-498d-edb1-0b9bf8e3c1f1"
      },
      "source": [
        "# Creating dataframes of features (X) and target (Y)\r\n",
        "X = df.iloc[:, 0:8]\r\n",
        "Y = df.iloc[:,8]\r\n",
        "\r\n",
        "# Printing the dataframes X and Y to ensure we have created the dataframes with the correct columns\r\n",
        "print('The features or the predictors (X) are : ', X, '\\n\\n') \r\n",
        "print('The target (Y) is : ', Y, '\\n\\n')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The features or the predictors (X) are :        Cement  Blast Furnace Slag  ...  Fine Aggregate  Age\n",
            "0      540.0                 0.0  ...           676.0   28\n",
            "1      540.0                 0.0  ...           676.0   28\n",
            "2      332.5               142.5  ...           594.0  270\n",
            "3      332.5               142.5  ...           594.0  365\n",
            "4      198.6               132.4  ...           825.5  360\n",
            "...      ...                 ...  ...             ...  ...\n",
            "1025   276.4               116.0  ...           768.3   28\n",
            "1026   322.2                 0.0  ...           813.4   28\n",
            "1027   148.5               139.4  ...           780.0   28\n",
            "1028   159.1               186.7  ...           788.9   28\n",
            "1029   260.9               100.5  ...           761.5   28\n",
            "\n",
            "[1030 rows x 8 columns] \n",
            "\n",
            "\n",
            "The target (Y) is :  0       79.99\n",
            "1       61.89\n",
            "2       40.27\n",
            "3       41.05\n",
            "4       44.30\n",
            "        ...  \n",
            "1025    44.28\n",
            "1026    31.18\n",
            "1027    23.70\n",
            "1028    32.77\n",
            "1029    32.40\n",
            "Name: Strength, Length: 1030, dtype: float64 \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpeIxkgTKSwn"
      },
      "source": [
        "<b>Note 1</b> : Unlike the method in this course, the splitting is done using indexing instead of using the names of the columns. Additionally, a different notation is used. The word <i>features</i> is used instead of <i>predictors</i>.\r\n",
        "\r\n",
        "\r\n",
        "<b>Note 2</b> : Pandas indexes columns starting from 0. Note in the code below for the features (X) indexing is used as `[:, 0:8]`. The first part preceding the coma `(:)` tells pandas to include ALL rows of the original dataframe (df) in the new dataframe called X while the part succedding the comma `(0:8)` tells pandas to include all columns of the original dataframe (df) starting from column with index = 0 and ending with column with index = 7, <b> but not to include the column with index = 8 </b>  \r\n",
        "\r\n",
        "<b>Note 3</b> : In order to split the data into train and test sets, the train_test_split function of the sklearn library is used. `The random_state` is used to ensure that the train and test split is the same each time, i.e. the train set and the test set have the same samples each time the code is run which is good for reproducing the results. If left empty, the random state is used by `np.random`. Since the Project requires splitting data the into <b>random</b> sets, hence `random_state` is not used, i.e. no value is set for random state. As the data hase to be split randomly  into train and test sets <b>50</b> times, a for loop will be used to to split the data in train test sets for <b>each model</b> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GhHM1EdKVKn"
      },
      "source": [
        "def regression_model() :\r\n",
        "    \r\n",
        "    # Create the model\r\n",
        "    model = Sequential()\r\n",
        "    model.add(Dense(10, activation='relu', input_shape=(X.shape[1],)))\r\n",
        "    model.add(Dense(10, activation='relu'))\r\n",
        "    model.add(Dense(1))\r\n",
        "\r\n",
        "    # Compile the model\r\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\r\n",
        "    \r\n",
        "    return model"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1_0sjSwKXxO"
      },
      "source": [
        "def data_split() :\r\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)\r\n",
        "    \r\n",
        "    # Create a list containing X_train, X_test, Y_train, Y_test and return the list\r\n",
        "    splits = [X_train, X_test, Y_train, Y_test] \r\n",
        "    return splits"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXcPvjk6KaJx"
      },
      "source": [
        "<b>Note </b> : In the above function `data_split()` the <i>X_train, X_test, Y_train, Y_test</i> sets are stored in a list and list is returned. This is to ensure that the <i>X_train, X_test, Y_train, Y_test</i> sets are not printed when the function is called"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2zoMyifKZcU"
      },
      "source": [
        "def predict() :\r\n",
        "    return model.predict(X_test)\r\n",
        "\r\n",
        "def calculate_mse() :\r\n",
        "    return mean_squared_error(Y_test,Y_predicted)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ0h9iIhKfhp"
      },
      "source": [
        "<b>Note 1</b> : The function `regression_model` as defined above only **compiles** the model and doesn't fit the model to training set. This is because in PART C, the number of epochs are changed and this would allow to fit the model with new number of epochs. This function will be used for **PART A**, **PART B** and **PART C**.\r\n",
        "\r\n",
        "<b>Note 2</b> : Since the splitting data, prediction and mean squared error calculations are all being used repeatly as well, hence there are separate functions created for them to remove redundancy of typing the same line of code while also making the code more neat. However,  as we the features (X) are to be normalized only **once**, hence there is no need to create a function for it\r\n",
        "\r\n",
        "<b>Note 3</b> : As the split function is executed first, the training and test sets are obtained and hence there is no need to explicitly pass any arguments to `predict()` and `calculate_mse()` functions. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeuIGRd7GPdv"
      },
      "source": [
        "# <font color = blue> PART B : BASELINE MODEL WITH NORMALIZED FEATURES </font>\r\n",
        "\r\n",
        "\r\n",
        "In this part, all the tasks from <b>PART A</b> are performed, but this time the values for the features (X) will be normalized using the formula `X - µ / σ`\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-2OF9aEGR_3"
      },
      "source": [
        "## <font color = darkorange>Task 1 : Train and Test the Baseline Model with Normalized Features</font>\r\n",
        "\r\n",
        "In order to train and test the the baseline model with normalized features, the following steps are performed :\r\n",
        "<ol>\r\n",
        "    <li>Normalize the features (X)</li>\r\n",
        "    <li>Randomly split the data into <i>X_train, X_test, Y_train, Y_test</i> sets</li>\r\n",
        "    <li>Train the model using <i>X_train, Y_train</i> using <b>50</b> epochs</li>\r\n",
        "    <li>Get the predictions on the <i>X_test</i> set </li>\r\n",
        "    <li>Compute the <b>mean squared error</b> on the test set using sklearn</li>\r\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgiA1igYGTQw"
      },
      "source": [
        "### <font color = #2980B9> Step 1 : Normalize the features (X) </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0XMekK5GZl0"
      },
      "source": [
        "X = (X - X.mean()) / X.std()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d77gA8FGb1m"
      },
      "source": [
        "### <font color = #2980B9> Step 2 : Randomly split the data into <i>X_train, X_test, Y_train, Y_test </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhZY7cWsGc9h"
      },
      "source": [
        "# Creating X_train, X_test, Y_train and Y_test sets\r\n",
        "X_train, X_test, Y_train, Y_test = data_split()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLpgTQ4fGetn"
      },
      "source": [
        "### <font color = #2980B9> Step 3 : Train the model using <i>X_train, Y_train</i> using 50 epochs </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6O044NQfGiQl"
      },
      "source": [
        "<b>Note </b> : Since the regression model has already been created using `def regression_model` in <b>TASK 1</b> and there are no changes being made to it, hence there is no need to write the code for the model again. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBqiYsGuGj6e",
        "outputId": "16001fc5-3ccf-4726-e58b-528da693cacf"
      },
      "source": [
        "model = regression_model()\r\n",
        "\r\n",
        "# Fit the model on the train set\r\n",
        "model.fit(X_train, Y_train, validation_split=0.3, epochs=50)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 36ms/step - loss: 1617.6201 - val_loss: 1492.4595\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1558.0160 - val_loss: 1478.9688\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1476.0178 - val_loss: 1464.9487\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1524.2386 - val_loss: 1449.4432\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1417.0921 - val_loss: 1432.4918\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1489.7961 - val_loss: 1412.7939\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1442.0594 - val_loss: 1390.7305\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1407.4251 - val_loss: 1364.8779\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1407.9444 - val_loss: 1334.6074\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1367.2817 - val_loss: 1298.2974\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1322.3243 - val_loss: 1256.0508\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1280.6392 - val_loss: 1206.9709\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1248.7060 - val_loss: 1149.9218\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1163.5941 - val_loss: 1085.9016\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 1073.7160 - val_loss: 1016.0949\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1010.0164 - val_loss: 939.8751\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 981.9398 - val_loss: 860.1501\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 875.7850 - val_loss: 777.8086\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 773.5218 - val_loss: 695.5099\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 677.5656 - val_loss: 615.4893\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 623.2612 - val_loss: 539.5380\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 566.6570 - val_loss: 471.1354\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 504.5173 - val_loss: 411.0634\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 401.4576 - val_loss: 360.3934\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 345.6196 - val_loss: 319.0695\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 313.4271 - val_loss: 284.7831\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 290.8461 - val_loss: 258.8227\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 267.9021 - val_loss: 238.6773\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 261.7458 - val_loss: 225.4999\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 245.1755 - val_loss: 216.0185\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 227.9561 - val_loss: 208.0804\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 212.5373 - val_loss: 201.6730\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 229.1032 - val_loss: 197.1120\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 218.2349 - val_loss: 192.6899\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 229.9933 - val_loss: 189.8218\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 202.9590 - val_loss: 187.4899\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 194.2422 - val_loss: 185.1586\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 206.6535 - val_loss: 182.4618\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 194.1333 - val_loss: 180.2962\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 189.4143 - val_loss: 178.7784\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 182.7684 - val_loss: 176.6116\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 190.5896 - val_loss: 174.9979\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 185.8945 - val_loss: 173.0219\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 194.1663 - val_loss: 171.6483\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 189.7619 - val_loss: 171.1830\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 185.0493 - val_loss: 169.5100\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 168.0080 - val_loss: 168.8159\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 183.2379 - val_loss: 167.3917\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 180.5646 - val_loss: 165.2877\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 172.9431 - val_loss: 164.0739\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f318f02e710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTjWPWFzxFyc"
      },
      "source": [
        "### <font color = #2980B9> Step 4 : Get the predictions on the X_test </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyHNc9PexFdY"
      },
      "source": [
        "# Store the predictions in a variable Y_Predicted\r\n",
        "Y_predicted = predict()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AR86bjIExWSg"
      },
      "source": [
        "### <font color = #2980B9> Step 5 : Compute the <i>mean squared error</i> on the test set using sklearn </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCJUNwmVxZcV",
        "outputId": "711af327-e90a-4944-c2f4-396b68185ade"
      },
      "source": [
        "# Calculate the mean square error\r\n",
        "\r\n",
        "mse = calculate_mse()\r\n",
        "print('Mean Square Error (MSE) of the Baseline Model with Normalized Features is : ' , str(mse))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean Square Error (MSE) of the Baseline Model with Normalized Features is :  178.43257144321913\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4E3moM5txktp"
      },
      "source": [
        "## <font color = darkorange>Task 2 : Create 50 Models and Calculate the <i>µ</i> & <i>σ</i> of their MSE Errors with new Features</font>\r\n",
        "\r\n",
        "In order to train 50 models with new features (normalized features) and calulate the mean (µ) and standard deviation (σ) of their mean square errors (MSE) the following steps are performed :\r\n",
        "<ol>\r\n",
        "    <li>Create an empty list <code>list_of_mse</code> to store the mean square error of each of the models</li>\r\n",
        "    <li>Define a for loop and perform each of the following steps in the loop</li>\r\n",
        "        <ol>\r\n",
        "        <li>Randomly split the data into <i>X_train, X_test, Y_train, Y_test</i> sets</li>\r\n",
        "        <li>Train the model using <i>X_train, Y_train</i> using <b>50</b> epochs</li>\r\n",
        "        <li>Evaluate the model using <i>X_test, Y_test</i></li>\r\n",
        "        <li>Get the predictions on the <i>X_test</i> set </li>\r\n",
        "        <li>Compute the <b>mean squared error</b> on the test set using sklearn</li>\r\n",
        "    </ol>\r\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xD9jYxuxoE9"
      },
      "source": [
        "### <font color = #2980B9> Step 1 : Creating the <i>list_of_mse</i> list</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCH1jwunxrOF"
      },
      "source": [
        "# Create the empty lists\r\n",
        "list_of_mse = []"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j19VfGQSxtK0"
      },
      "source": [
        "### <font color = #2980B9> Step 2 : Creating 50 Models, Calculating The MSE for Each and <i>µ</i> & <i>σ</i> of the 50 MSE Values in <i>list_of_mse</i> </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Or-6aoMcxuYr",
        "outputId": "ecb444f1-06e5-4933-be10-29b57510ea33"
      },
      "source": [
        "# Create the for loop to split the data, create, compile & fit model, evaluate & nake predictions, caluclate mse and store\r\n",
        "# in list_of_mse\r\n",
        "\r\n",
        "start_time = datetime.now() # Starting time of the for loop execution\r\n",
        "\r\n",
        "for i in range(50) :\r\n",
        "    # Split the data into train and test set\r\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)\r\n",
        "    \r\n",
        "    # Create and compile the regression model using the function regression_model as defined in TASK 1\r\n",
        "    model = regression_model()\r\n",
        "\r\n",
        "    # Fit the model on the train set\r\n",
        "    print('\\n\\n\\nTraining Model # ' , i+1 , '\\n\\n') # Print the Model Number that is being trained\r\n",
        "    model.fit(X_train, Y_train, validation_split=0.3, epochs=50)\r\n",
        "    print('\\n')\r\n",
        "    \r\n",
        "    # Make prediction on the test set\r\n",
        "    Y_predicted = model.predict(X_test)\r\n",
        "    \r\n",
        "    # Calculate the mean square error\r\n",
        "    mse = mean_squared_error(Y_test, Y_predicted)\r\n",
        "    \r\n",
        "    # Add the mse to the list_of_mse list\r\n",
        "    list_of_mse.append(mse)\r\n",
        "\r\n",
        "end_time = datetime.now() # Ending time of the for loop execution\r\n",
        "\r\n",
        "# Print time taken for fitting 50 models and calucating the Mean and Standard Deviation of MSE of 50 models\r\n",
        "print('\\n\\nTotal Execution Time : ' , format(end_time - start_time))\r\n",
        "    "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 206.5160 - val_loss: 192.4452\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 197.8638 - val_loss: 190.0580\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 192.0201 - val_loss: 188.1503\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 201.8564 - val_loss: 186.4283\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 184.7096 - val_loss: 184.3759\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 170.8807 - val_loss: 182.6363\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 185.3382 - val_loss: 181.2137\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 181.3650 - val_loss: 179.4454\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 172.6410 - val_loss: 177.7108\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 179.6170 - val_loss: 176.1207\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 169.9534 - val_loss: 174.5918\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 158.0727 - val_loss: 173.2366\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 172.0247 - val_loss: 171.8343\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 149.7280 - val_loss: 170.4220\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  5 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 14ms/step - loss: 1538.2533 - val_loss: 1624.2280\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1489.0493 - val_loss: 1606.8981\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1419.8568 - val_loss: 1588.3188\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1539.6859 - val_loss: 1566.8979\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1474.6994 - val_loss: 1542.6560\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1489.2828 - val_loss: 1512.4640\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1367.2214 - val_loss: 1476.1572\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1344.1892 - val_loss: 1433.0680\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1287.2096 - val_loss: 1382.3087\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1223.5891 - val_loss: 1322.9834\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1230.9420 - val_loss: 1254.3192\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1069.6195 - val_loss: 1178.7792\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1036.0950 - val_loss: 1096.7789\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1064.7856 - val_loss: 1007.1053\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 908.5761 - val_loss: 915.0468\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 826.4859 - val_loss: 821.3359\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 735.5147 - val_loss: 727.7368\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 690.9810 - val_loss: 637.3959\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 601.5237 - val_loss: 556.3030\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 498.0947 - val_loss: 482.9817\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 399.5579 - val_loss: 422.9950\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 392.8309 - val_loss: 370.0144\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 350.1020 - val_loss: 330.4402\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 311.7441 - val_loss: 299.9303\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 286.1764 - val_loss: 276.9067\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 283.7226 - val_loss: 259.0713\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 232.2978 - val_loss: 246.8951\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 250.5865 - val_loss: 236.6232\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 226.1513 - val_loss: 227.7799\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 210.0701 - val_loss: 221.8216\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 222.1273 - val_loss: 215.0505\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 201.5703 - val_loss: 209.4998\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 201.4431 - val_loss: 204.9722\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 194.0129 - val_loss: 200.6862\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 181.6674 - val_loss: 196.9789\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 188.5284 - val_loss: 193.8650\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 194.1458 - val_loss: 189.8619\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 186.2733 - val_loss: 187.0117\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 178.0407 - val_loss: 184.1644\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 169.7057 - val_loss: 182.5647\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 171.2849 - val_loss: 180.2952\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 178.1569 - val_loss: 178.5267\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 174.0320 - val_loss: 176.9872\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 178.2040 - val_loss: 175.6172\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 165.1402 - val_loss: 173.8560\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 167.6945 - val_loss: 172.6318\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 154.3158 - val_loss: 171.2066\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 180.7944 - val_loss: 170.8911\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 168.1175 - val_loss: 169.3498\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 170.6089 - val_loss: 168.3683\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  6 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 14ms/step - loss: 1537.8612 - val_loss: 1554.0818\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1605.4747 - val_loss: 1539.5743\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1607.7835 - val_loss: 1526.7227\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1589.3470 - val_loss: 1514.4089\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1668.1407 - val_loss: 1502.3447\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1530.6140 - val_loss: 1489.8159\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 1501.2124 - val_loss: 1475.8071\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1510.6222 - val_loss: 1460.4514\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1543.3019 - val_loss: 1442.6063\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1491.5182 - val_loss: 1422.5073\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1473.7039 - val_loss: 1399.8247\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1446.4323 - val_loss: 1373.8771\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1376.5633 - val_loss: 1344.3809\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1332.4337 - val_loss: 1311.6934\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1440.2843 - val_loss: 1274.3259\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1273.0262 - val_loss: 1233.8114\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1300.6166 - val_loss: 1189.1232\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1210.3167 - val_loss: 1140.6038\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1226.6785 - val_loss: 1088.2336\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1119.2919 - val_loss: 1033.1514\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1052.4050 - val_loss: 974.3522\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1029.6149 - val_loss: 913.0492\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 912.4233 - val_loss: 851.5774\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 868.6165 - val_loss: 787.7467\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 738.3975 - val_loss: 725.0789\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 738.7877 - val_loss: 661.8721\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 635.1835 - val_loss: 603.6416\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 591.2666 - val_loss: 546.7643\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 531.3825 - val_loss: 494.3018\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 492.9697 - val_loss: 445.4588\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 405.3659 - val_loss: 402.6210\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 394.1289 - val_loss: 364.3543\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 343.8850 - val_loss: 332.0812\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 304.7778 - val_loss: 305.6066\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 277.3505 - val_loss: 283.1685\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 272.1315 - val_loss: 264.3840\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 257.5908 - val_loss: 249.8547\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 224.4817 - val_loss: 238.3882\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 219.7505 - val_loss: 229.1148\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 213.3580 - val_loss: 221.8689\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 196.1411 - val_loss: 216.0146\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 202.3041 - val_loss: 211.1645\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 203.7316 - val_loss: 206.7651\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 207.7855 - val_loss: 203.0123\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 183.5095 - val_loss: 199.6157\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 183.7184 - val_loss: 196.7434\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 193.7286 - val_loss: 193.7897\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 190.8299 - val_loss: 191.5709\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 181.0407 - val_loss: 189.3863\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 182.3176 - val_loss: 187.2711\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  7 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 14ms/step - loss: 1537.9853 - val_loss: 1417.2875\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1434.4006 - val_loss: 1390.6826\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1497.5114 - val_loss: 1359.9216\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 1453.8611 - val_loss: 1325.3473\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1360.2664 - val_loss: 1284.3964\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1304.5774 - val_loss: 1236.6404\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1329.5299 - val_loss: 1182.5319\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1323.8838 - val_loss: 1121.5664\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1191.2488 - val_loss: 1053.7463\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1121.4869 - val_loss: 977.2180\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1018.7405 - val_loss: 895.4334\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 889.6827 - val_loss: 809.2057\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 854.3721 - val_loss: 722.0806\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 717.4468 - val_loss: 637.2151\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 635.8658 - val_loss: 555.1454\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 519.2646 - val_loss: 480.1280\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 488.6463 - val_loss: 416.3338\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 356.3186 - val_loss: 364.9044\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 348.5870 - val_loss: 323.3098\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 269.2468 - val_loss: 292.3385\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 249.7366 - val_loss: 268.7704\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 250.3184 - val_loss: 250.3589\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 234.1145 - val_loss: 236.5838\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 227.6626 - val_loss: 226.0591\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 206.0460 - val_loss: 218.0041\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 188.8069 - val_loss: 210.5178\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 198.3300 - val_loss: 204.2351\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 189.1184 - val_loss: 198.8228\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 177.7301 - val_loss: 193.9096\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 186.3840 - val_loss: 189.8134\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 181.1074 - val_loss: 186.4764\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 177.8994 - val_loss: 183.1174\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 174.6484 - val_loss: 179.9909\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 158.6553 - val_loss: 177.2451\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 176.1637 - val_loss: 174.8281\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 157.8764 - val_loss: 172.3378\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 164.3648 - val_loss: 170.6893\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 154.0069 - val_loss: 168.9939\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 149.9534 - val_loss: 167.4015\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 158.3678 - val_loss: 165.8269\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 156.7045 - val_loss: 164.3297\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 147.5877 - val_loss: 162.9504\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 146.5477 - val_loss: 161.7964\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 142.0902 - val_loss: 160.6856\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 152.8565 - val_loss: 159.2359\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 136.6538 - val_loss: 158.2706\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 146.9128 - val_loss: 157.4675\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 148.2169 - val_loss: 156.3362\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 148.2600 - val_loss: 155.5209\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 141.0702 - val_loss: 154.9315\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  8 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 13ms/step - loss: 1711.5947 - val_loss: 1679.3503\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1653.6894 - val_loss: 1663.6448\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1578.9389 - val_loss: 1650.2938\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1624.1394 - val_loss: 1639.1075\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1631.1299 - val_loss: 1629.5271\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1578.8429 - val_loss: 1621.2848\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1504.8218 - val_loss: 1613.9802\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1537.2468 - val_loss: 1607.0858\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1564.0545 - val_loss: 1600.0432\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1562.7990 - val_loss: 1592.7976\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1649.5386 - val_loss: 1585.1528\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1541.2416 - val_loss: 1576.5939\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1550.8062 - val_loss: 1566.2209\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1507.6035 - val_loss: 1553.9984\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1581.9257 - val_loss: 1539.1267\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1492.0131 - val_loss: 1521.3295\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1505.6824 - val_loss: 1499.7863\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1468.9673 - val_loss: 1474.3688\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1414.0977 - val_loss: 1444.5121\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1395.5610 - val_loss: 1409.5972\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1330.9168 - val_loss: 1369.6484\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1361.9096 - val_loss: 1323.3972\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1299.6762 - val_loss: 1270.7924\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1271.8555 - val_loss: 1208.5562\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1153.2567 - val_loss: 1139.4609\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1081.6766 - val_loss: 1063.1583\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1031.0475 - val_loss: 981.1265\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 926.0611 - val_loss: 897.3639\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 872.0493 - val_loss: 812.8538\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 752.2424 - val_loss: 730.7930\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 695.9810 - val_loss: 653.4564\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 589.1864 - val_loss: 583.1221\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 516.3556 - val_loss: 522.2255\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 418.0192 - val_loss: 469.1161\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 398.9757 - val_loss: 424.1610\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 330.9618 - val_loss: 388.3834\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 338.9970 - val_loss: 360.1887\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 281.0554 - val_loss: 339.3009\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 292.3343 - val_loss: 321.9551\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 255.5055 - val_loss: 308.6087\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 248.3960 - val_loss: 297.1292\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 251.1269 - val_loss: 288.4657\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 234.9981 - val_loss: 279.6615\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 210.4324 - val_loss: 273.0791\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 207.9479 - val_loss: 266.5139\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 202.6475 - val_loss: 260.1812\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 220.7278 - val_loss: 254.4619\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 208.9407 - val_loss: 250.1030\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 177.1734 - val_loss: 244.9137\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 203.9265 - val_loss: 239.9824\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  9 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 14ms/step - loss: 1551.8990 - val_loss: 1660.7402\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1473.4693 - val_loss: 1642.8356\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1487.1564 - val_loss: 1623.1770\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1465.4618 - val_loss: 1601.3379\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1431.6253 - val_loss: 1576.3737\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1387.1339 - val_loss: 1547.9551\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1378.0978 - val_loss: 1515.2468\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1342.8595 - val_loss: 1478.0176\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1340.4875 - val_loss: 1434.8544\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1303.2926 - val_loss: 1384.3567\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1239.5160 - val_loss: 1324.7791\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1170.8023 - val_loss: 1257.3783\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1126.4522 - val_loss: 1181.5710\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1050.6860 - val_loss: 1099.4313\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 924.5580 - val_loss: 1012.7740\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 882.6339 - val_loss: 921.5204\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 805.9472 - val_loss: 829.5391\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 695.9534 - val_loss: 741.1368\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 643.0803 - val_loss: 656.7164\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 534.1974 - val_loss: 579.3610\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 502.4477 - val_loss: 509.2632\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 374.1793 - val_loss: 452.8621\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 346.2139 - val_loss: 404.6100\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 305.2693 - val_loss: 367.2349\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 284.3799 - val_loss: 336.9965\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 252.1701 - val_loss: 311.7195\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 234.7285 - val_loss: 293.6655\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 237.3620 - val_loss: 278.8725\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 215.6917 - val_loss: 267.2994\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 202.7949 - val_loss: 257.7552\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 191.6924 - val_loss: 250.1824\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 198.3552 - val_loss: 242.6603\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 194.4232 - val_loss: 235.6650\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 174.5226 - val_loss: 230.3790\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 192.2728 - val_loss: 225.1801\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 183.7325 - val_loss: 221.1629\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 160.9599 - val_loss: 216.6593\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 187.2355 - val_loss: 212.3189\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 164.9014 - val_loss: 208.7738\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 182.4767 - val_loss: 205.4180\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 178.4036 - val_loss: 201.9068\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 169.9075 - val_loss: 199.2456\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 164.7556 - val_loss: 196.4383\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 169.2284 - val_loss: 193.3179\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 177.2900 - val_loss: 190.8339\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 164.0032 - val_loss: 188.7402\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 164.4770 - val_loss: 186.1884\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 164.5445 - val_loss: 184.0855\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 166.7804 - val_loss: 182.4971\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 161.3923 - val_loss: 180.6455\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  10 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 13ms/step - loss: 1583.4018 - val_loss: 1452.5795\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1530.3010 - val_loss: 1434.3960\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1458.4646 - val_loss: 1414.5787\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1530.6071 - val_loss: 1392.5665\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1527.7652 - val_loss: 1367.5502\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1417.0163 - val_loss: 1339.0945\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1472.8048 - val_loss: 1305.6562\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1396.9878 - val_loss: 1267.5977\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1343.5032 - val_loss: 1224.4364\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1284.5409 - val_loss: 1176.4358\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1292.9863 - val_loss: 1122.8779\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1207.1170 - val_loss: 1065.8962\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1021.9146 - val_loss: 1003.7216\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1041.0330 - val_loss: 938.1735\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 983.6203 - val_loss: 869.7266\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 905.2342 - val_loss: 799.5019\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 835.9747 - val_loss: 729.8274\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 765.5717 - val_loss: 661.4613\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 708.9892 - val_loss: 595.0792\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 620.4391 - val_loss: 533.9371\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 578.3735 - val_loss: 477.6196\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 477.9227 - val_loss: 427.8792\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 464.1979 - val_loss: 382.5319\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 377.9816 - val_loss: 345.2244\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 329.8539 - val_loss: 313.3680\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 311.1695 - val_loss: 286.6827\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 282.5077 - val_loss: 264.9394\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 257.0059 - val_loss: 247.1619\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 214.2601 - val_loss: 233.9661\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 233.0738 - val_loss: 222.6721\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 228.7803 - val_loss: 212.7280\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 213.4468 - val_loss: 205.5527\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 199.5160 - val_loss: 199.0414\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 201.6977 - val_loss: 193.6876\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 195.3862 - val_loss: 188.4426\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 203.5878 - val_loss: 184.1100\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 207.0123 - val_loss: 181.4120\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 183.6865 - val_loss: 178.6341\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 172.4241 - val_loss: 176.0663\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 171.9437 - val_loss: 173.9165\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 171.0018 - val_loss: 172.1914\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 167.2797 - val_loss: 169.4848\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 192.3434 - val_loss: 168.0509\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 170.5847 - val_loss: 166.6149\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 155.7859 - val_loss: 165.6859\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 181.6612 - val_loss: 163.8436\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 164.8813 - val_loss: 163.0165\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 159.1669 - val_loss: 162.1288\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 170.2959 - val_loss: 160.7495\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 159.2827 - val_loss: 159.7055\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  11 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 14ms/step - loss: 1584.3093 - val_loss: 1565.0551\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1578.9405 - val_loss: 1541.3833\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1586.1213 - val_loss: 1520.0262\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1651.4849 - val_loss: 1499.5427\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1542.4171 - val_loss: 1479.5776\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1538.5822 - val_loss: 1459.4319\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1562.9999 - val_loss: 1437.3485\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1617.3239 - val_loss: 1412.6659\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1510.8148 - val_loss: 1385.1323\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1513.6444 - val_loss: 1352.7871\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1479.0729 - val_loss: 1315.5774\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1392.3662 - val_loss: 1272.3328\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1429.6612 - val_loss: 1223.1313\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1318.6915 - val_loss: 1167.4865\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1171.9695 - val_loss: 1105.8254\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1146.7826 - val_loss: 1038.8181\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1225.9505 - val_loss: 965.7491\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 997.0582 - val_loss: 890.2501\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 919.4009 - val_loss: 814.4379\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 867.7237 - val_loss: 738.0222\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 820.1751 - val_loss: 664.8398\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 714.0352 - val_loss: 597.2785\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 586.7148 - val_loss: 536.2590\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 556.0982 - val_loss: 481.9250\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 503.5499 - val_loss: 434.2879\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 435.8599 - val_loss: 394.4745\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 397.6066 - val_loss: 361.5501\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 367.1565 - val_loss: 333.0573\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 331.9850 - val_loss: 309.4261\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 316.1536 - val_loss: 290.6441\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 277.5168 - val_loss: 275.0626\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 263.3527 - val_loss: 262.2783\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 261.0669 - val_loss: 252.1216\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 262.2915 - val_loss: 243.5779\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 227.1422 - val_loss: 238.1309\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 210.2623 - val_loss: 232.2451\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 231.9893 - val_loss: 227.2812\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 202.7316 - val_loss: 222.8624\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 201.2102 - val_loss: 219.2322\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 189.2185 - val_loss: 215.9236\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 192.5278 - val_loss: 211.7171\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 178.0929 - val_loss: 207.8421\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 203.1654 - val_loss: 204.9102\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 197.7870 - val_loss: 201.9338\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 180.6314 - val_loss: 198.6770\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 180.8315 - val_loss: 196.6572\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 176.0770 - val_loss: 194.0276\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 181.5430 - val_loss: 190.9258\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 166.1688 - val_loss: 187.8825\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 204.0437 - val_loss: 185.3815\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  12 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 14ms/step - loss: 1621.2941 - val_loss: 1520.2516\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1638.0399 - val_loss: 1498.7192\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1568.5932 - val_loss: 1475.8116\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1620.3511 - val_loss: 1449.7633\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1533.8588 - val_loss: 1421.3525\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1553.8763 - val_loss: 1389.8892\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1453.8765 - val_loss: 1354.6521\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1459.3038 - val_loss: 1315.0299\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1384.0113 - val_loss: 1270.9150\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1441.5640 - val_loss: 1221.0312\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1307.6223 - val_loss: 1166.5121\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 1209.3522 - val_loss: 1106.5782\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1172.6366 - val_loss: 1042.1862\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1097.7749 - val_loss: 973.5320\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 958.5250 - val_loss: 901.2548\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 886.3085 - val_loss: 825.7834\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 778.8024 - val_loss: 751.1904\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 736.4242 - val_loss: 678.1901\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 629.8472 - val_loss: 608.9503\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 579.1005 - val_loss: 544.0231\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 504.2835 - val_loss: 486.3466\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 424.3322 - val_loss: 435.3458\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 356.9192 - val_loss: 392.1419\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 326.4314 - val_loss: 353.9224\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 300.6396 - val_loss: 323.0230\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 260.3529 - val_loss: 296.9388\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 231.9000 - val_loss: 278.1678\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 246.9445 - val_loss: 260.8715\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 228.1968 - val_loss: 248.1850\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 211.1163 - val_loss: 239.0937\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 213.8132 - val_loss: 231.0778\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 198.7589 - val_loss: 225.2437\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 209.8142 - val_loss: 219.5003\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 177.4166 - val_loss: 215.2259\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 184.1366 - val_loss: 211.9232\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 197.6464 - val_loss: 208.3121\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 174.1210 - val_loss: 206.2391\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 179.0323 - val_loss: 203.5475\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 190.2920 - val_loss: 201.2488\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 177.6768 - val_loss: 199.3808\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 159.4584 - val_loss: 197.3544\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 155.4688 - val_loss: 195.7984\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 161.6979 - val_loss: 193.6175\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 160.1693 - val_loss: 192.6120\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 158.7371 - val_loss: 190.7431\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 158.9965 - val_loss: 189.1464\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 157.3428 - val_loss: 187.6490\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 160.8760 - val_loss: 186.2400\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 163.6932 - val_loss: 184.4315\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 156.7734 - val_loss: 183.3775\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  13 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 14ms/step - loss: 1612.3706 - val_loss: 1493.9882\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1508.4989 - val_loss: 1473.3745\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1462.3243 - val_loss: 1450.2250\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1465.1421 - val_loss: 1423.4181\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1383.0743 - val_loss: 1392.5010\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1331.1386 - val_loss: 1356.7747\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 1433.5347 - val_loss: 1316.7994\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1293.6280 - val_loss: 1270.5396\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1153.5991 - val_loss: 1218.1428\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1203.5444 - val_loss: 1159.3939\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1122.6886 - val_loss: 1094.1483\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1022.6596 - val_loss: 1024.3104\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1023.7213 - val_loss: 950.3015\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 926.1340 - val_loss: 873.2051\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 826.2413 - val_loss: 795.6323\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 748.4117 - val_loss: 716.0589\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 728.3521 - val_loss: 639.1268\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 622.2935 - val_loss: 566.2336\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 543.2800 - val_loss: 500.8828\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 523.0392 - val_loss: 441.8321\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 441.7461 - val_loss: 391.4531\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 375.8673 - val_loss: 349.3694\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 335.0559 - val_loss: 315.1891\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 314.4003 - val_loss: 287.0887\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 273.9345 - val_loss: 268.2898\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 298.2178 - val_loss: 252.8965\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 276.2724 - val_loss: 241.7566\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 242.4216 - val_loss: 234.0771\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 224.1857 - val_loss: 228.4683\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 224.5272 - val_loss: 224.4783\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 195.9314 - val_loss: 221.4413\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 206.6747 - val_loss: 218.6134\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 208.5401 - val_loss: 216.4976\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 200.7474 - val_loss: 214.5002\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 203.3895 - val_loss: 213.0034\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 191.8672 - val_loss: 211.4746\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 179.3067 - val_loss: 209.9438\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 178.7304 - val_loss: 208.6357\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 176.6986 - val_loss: 206.7244\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 158.4782 - val_loss: 205.4581\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 154.3480 - val_loss: 203.9666\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 179.7853 - val_loss: 202.2961\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 162.2795 - val_loss: 200.6904\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 162.3900 - val_loss: 199.2860\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 169.2479 - val_loss: 198.4723\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 155.6085 - val_loss: 196.2567\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 163.7506 - val_loss: 195.0355\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 161.6516 - val_loss: 193.6428\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 152.7436 - val_loss: 192.5683\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 158.3292 - val_loss: 190.9816\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  14 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 24ms/step - loss: 1536.9364 - val_loss: 1780.8932\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1619.2169 - val_loss: 1768.2140\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1510.2898 - val_loss: 1756.6086\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1496.1672 - val_loss: 1745.4135\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1484.3931 - val_loss: 1734.4620\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1456.0834 - val_loss: 1722.7606\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1498.8924 - val_loss: 1710.1652\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1512.6225 - val_loss: 1695.5121\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1477.6451 - val_loss: 1678.5255\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1416.9467 - val_loss: 1659.0582\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1508.6728 - val_loss: 1636.4495\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1416.4041 - val_loss: 1610.8955\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1374.4158 - val_loss: 1582.1824\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1382.8347 - val_loss: 1549.2932\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1262.0595 - val_loss: 1512.6755\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1310.4709 - val_loss: 1472.4045\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1255.2852 - val_loss: 1426.2815\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1250.9861 - val_loss: 1375.7822\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1086.1313 - val_loss: 1318.7565\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1076.2471 - val_loss: 1255.0756\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1085.0714 - val_loss: 1185.2595\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1037.2992 - val_loss: 1112.1075\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 925.4835 - val_loss: 1036.1416\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 852.0332 - val_loss: 960.0293\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 756.5722 - val_loss: 884.4218\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 707.4644 - val_loss: 808.9297\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 679.4493 - val_loss: 737.5806\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 600.1475 - val_loss: 668.6580\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 560.7805 - val_loss: 605.0897\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 511.4284 - val_loss: 547.8388\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 438.7872 - val_loss: 497.5157\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 414.1668 - val_loss: 451.3181\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 398.8313 - val_loss: 409.5355\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 340.9688 - val_loss: 373.3257\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 318.3790 - val_loss: 341.8854\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 304.5036 - val_loss: 316.3452\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 241.5014 - val_loss: 296.2403\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 238.4350 - val_loss: 280.6139\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 229.1884 - val_loss: 267.7880\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 249.1183 - val_loss: 258.8598\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 239.5594 - val_loss: 250.5913\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 224.5109 - val_loss: 244.7848\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 207.8234 - val_loss: 239.5697\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 205.6586 - val_loss: 234.9698\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 207.3227 - val_loss: 230.8907\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 172.2764 - val_loss: 227.4381\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 191.9642 - val_loss: 223.9722\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 200.6962 - val_loss: 220.8582\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 189.7546 - val_loss: 217.7504\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 173.7810 - val_loss: 214.8304\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  15 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 24ms/step - loss: 1516.7571 - val_loss: 1534.4133\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1540.0482 - val_loss: 1523.1962\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1566.5897 - val_loss: 1510.9301\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1496.2880 - val_loss: 1496.8345\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1456.2815 - val_loss: 1480.2822\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1450.2435 - val_loss: 1460.5793\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1408.4936 - val_loss: 1437.3832\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1514.5056 - val_loss: 1409.2106\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1390.4130 - val_loss: 1376.0466\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1334.1311 - val_loss: 1336.7750\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1300.7933 - val_loss: 1291.6996\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1240.8331 - val_loss: 1240.1355\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1323.3561 - val_loss: 1181.7551\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1165.9980 - val_loss: 1117.9398\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1079.2189 - val_loss: 1049.1698\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1043.0335 - val_loss: 976.1016\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 933.1257 - val_loss: 902.9829\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 894.0366 - val_loss: 826.3839\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 762.4169 - val_loss: 752.5494\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 711.2687 - val_loss: 679.2806\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 589.5898 - val_loss: 610.7768\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 494.2762 - val_loss: 546.9970\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 517.9137 - val_loss: 487.1114\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 442.8505 - val_loss: 437.0689\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 344.4607 - val_loss: 394.6101\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 341.4852 - val_loss: 357.7326\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 325.6974 - val_loss: 327.0977\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 271.0309 - val_loss: 304.7969\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 253.9450 - val_loss: 285.9431\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 237.0065 - val_loss: 271.3922\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 245.6591 - val_loss: 259.4210\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 224.9031 - val_loss: 250.1574\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 220.4321 - val_loss: 242.3815\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 213.2772 - val_loss: 236.2099\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 206.9981 - val_loss: 231.2652\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 217.6958 - val_loss: 226.3444\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 206.7860 - val_loss: 223.1250\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 189.1063 - val_loss: 219.4360\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 178.3518 - val_loss: 216.4193\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 179.3682 - val_loss: 213.2930\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 183.5810 - val_loss: 210.4126\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 185.0299 - val_loss: 208.1960\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 180.7579 - val_loss: 205.7787\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 174.6702 - val_loss: 203.1523\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 165.6698 - val_loss: 201.3854\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 176.1213 - val_loss: 199.0437\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 174.1751 - val_loss: 197.0766\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 163.9858 - val_loss: 194.8698\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 184.0287 - val_loss: 193.0740\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 167.1545 - val_loss: 191.4820\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  16 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 14ms/step - loss: 1579.6638 - val_loss: 1491.5448\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1499.7264 - val_loss: 1466.7627\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1555.5702 - val_loss: 1440.8184\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1409.3269 - val_loss: 1413.2485\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1489.3373 - val_loss: 1382.9761\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1436.0546 - val_loss: 1349.0620\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1383.1326 - val_loss: 1311.1885\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1349.1189 - val_loss: 1267.8074\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1294.9235 - val_loss: 1218.5227\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1217.1654 - val_loss: 1163.7461\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1153.6994 - val_loss: 1100.3234\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1130.6532 - val_loss: 1026.8844\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1066.2979 - val_loss: 945.3425\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 899.8668 - val_loss: 861.0765\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 933.4163 - val_loss: 773.4137\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 795.9054 - val_loss: 686.9935\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 745.8319 - val_loss: 602.5192\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 616.9929 - val_loss: 523.2130\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 624.3724 - val_loss: 451.5529\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 467.9144 - val_loss: 391.9846\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 425.3568 - val_loss: 338.9313\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 356.9322 - val_loss: 297.6219\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 293.9075 - val_loss: 265.9004\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 266.1917 - val_loss: 241.7903\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 247.2251 - val_loss: 224.5273\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 226.3902 - val_loss: 212.0359\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 203.9918 - val_loss: 203.6339\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 203.9903 - val_loss: 197.0829\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 204.0190 - val_loss: 192.6465\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 195.6170 - val_loss: 189.0375\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 187.7596 - val_loss: 185.9059\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 189.3412 - val_loss: 183.2691\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 182.4021 - val_loss: 180.9358\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 167.2358 - val_loss: 178.8299\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 164.4178 - val_loss: 176.7489\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 175.7465 - val_loss: 174.8265\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 164.0634 - val_loss: 173.1693\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 161.1451 - val_loss: 171.6252\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 153.4020 - val_loss: 170.0141\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 156.5082 - val_loss: 168.6082\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 145.2933 - val_loss: 167.3123\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 148.0509 - val_loss: 166.1848\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 154.8908 - val_loss: 164.9374\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 149.5847 - val_loss: 163.8679\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 140.1543 - val_loss: 163.1263\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 154.3529 - val_loss: 161.8039\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 145.0104 - val_loss: 160.8088\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 141.7203 - val_loss: 160.2242\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 139.3064 - val_loss: 159.4462\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 133.5462 - val_loss: 158.2173\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  17 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 14ms/step - loss: 1597.9118 - val_loss: 1541.5472\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1722.5731 - val_loss: 1527.4447\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1586.3440 - val_loss: 1514.2264\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1554.4104 - val_loss: 1500.6429\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1557.7943 - val_loss: 1485.7686\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1485.1492 - val_loss: 1469.7118\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1536.0714 - val_loss: 1452.2920\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1462.0993 - val_loss: 1433.4464\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1480.5744 - val_loss: 1412.0631\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1453.3377 - val_loss: 1387.4633\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1416.9030 - val_loss: 1358.2003\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1358.3394 - val_loss: 1324.8889\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1383.6237 - val_loss: 1288.4237\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1321.3714 - val_loss: 1247.2074\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1221.4072 - val_loss: 1202.0947\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1247.1689 - val_loss: 1152.9071\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1195.1768 - val_loss: 1100.0543\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1153.9626 - val_loss: 1044.1456\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1064.3263 - val_loss: 984.4387\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 935.6209 - val_loss: 922.9442\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 932.0106 - val_loss: 859.2695\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 865.2587 - val_loss: 795.5580\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 783.4492 - val_loss: 732.1249\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 701.5614 - val_loss: 669.7234\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 705.0569 - val_loss: 607.6250\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 608.8878 - val_loss: 552.1152\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 515.0067 - val_loss: 500.3581\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 526.8218 - val_loss: 452.1375\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 452.7292 - val_loss: 408.9961\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 421.2947 - val_loss: 371.3559\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 379.9463 - val_loss: 338.7113\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 370.1141 - val_loss: 310.5374\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 296.2441 - val_loss: 287.2969\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 258.4972 - val_loss: 268.5735\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 253.3006 - val_loss: 250.6256\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 254.5825 - val_loss: 236.0664\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 244.7095 - val_loss: 223.5589\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 229.0941 - val_loss: 213.6877\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 252.6914 - val_loss: 204.5671\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 223.1537 - val_loss: 196.8219\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 215.0552 - val_loss: 189.4023\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 198.6604 - val_loss: 183.8353\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 188.7817 - val_loss: 177.8138\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 200.5052 - val_loss: 172.6168\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 208.3162 - val_loss: 168.3481\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 183.0893 - val_loss: 164.3294\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 192.5495 - val_loss: 160.7961\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 176.1755 - val_loss: 157.5348\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 170.6035 - val_loss: 154.9365\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 177.9573 - val_loss: 152.2142\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  18 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 15ms/step - loss: 1533.8641 - val_loss: 1550.4188\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1652.2372 - val_loss: 1528.5927\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1603.3526 - val_loss: 1506.3383\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1510.1136 - val_loss: 1482.0596\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1485.9936 - val_loss: 1455.3242\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1498.4855 - val_loss: 1425.9921\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1561.8692 - val_loss: 1393.4176\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1425.4427 - val_loss: 1357.7299\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1407.5253 - val_loss: 1317.5471\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1297.0460 - val_loss: 1272.9233\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1353.6587 - val_loss: 1223.0056\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1203.1632 - val_loss: 1167.9047\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1210.2702 - val_loss: 1106.4248\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1180.3795 - val_loss: 1038.6846\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1072.4881 - val_loss: 967.1362\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 955.4588 - val_loss: 891.4075\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 903.4193 - val_loss: 809.9009\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 777.8480 - val_loss: 725.9863\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 715.8506 - val_loss: 642.3751\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 646.1066 - val_loss: 561.2419\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 540.6336 - val_loss: 484.2957\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 490.0855 - val_loss: 417.8158\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 451.7820 - val_loss: 361.7546\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 378.2435 - val_loss: 319.6389\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 319.9416 - val_loss: 287.8411\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 310.3059 - val_loss: 263.6902\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 266.8089 - val_loss: 247.7696\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 235.4195 - val_loss: 236.2328\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 245.4984 - val_loss: 226.6711\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 242.3307 - val_loss: 219.4291\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 225.3937 - val_loss: 213.6805\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 237.6310 - val_loss: 209.0069\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 221.7416 - val_loss: 205.0428\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 220.7681 - val_loss: 201.4494\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 217.7728 - val_loss: 198.3462\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 205.0454 - val_loss: 195.6216\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 203.1240 - val_loss: 193.1349\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 200.0193 - val_loss: 190.3379\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 213.2032 - val_loss: 187.4592\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 205.9092 - val_loss: 185.6692\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 177.8374 - val_loss: 183.4683\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 182.3510 - val_loss: 181.4207\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 190.3626 - val_loss: 178.8981\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 193.1165 - val_loss: 177.2302\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 181.6382 - val_loss: 175.2777\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 179.1112 - val_loss: 173.5735\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 188.6901 - val_loss: 172.1212\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 194.8162 - val_loss: 169.9189\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 183.3699 - val_loss: 168.0832\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 171.7796 - val_loss: 166.5236\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  19 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 14ms/step - loss: 1600.1187 - val_loss: 1614.4269\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1648.4925 - val_loss: 1601.4504\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1582.9093 - val_loss: 1588.6899\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1535.1965 - val_loss: 1575.2708\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1603.0962 - val_loss: 1560.9220\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1532.1522 - val_loss: 1544.1826\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1522.7266 - val_loss: 1524.2905\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1518.8924 - val_loss: 1501.6682\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1525.1347 - val_loss: 1475.3219\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1466.7630 - val_loss: 1445.4185\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1497.4076 - val_loss: 1411.0085\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1373.8842 - val_loss: 1371.4871\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1357.7514 - val_loss: 1326.2030\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1276.0257 - val_loss: 1273.9360\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1222.1875 - val_loss: 1214.4688\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1192.0442 - val_loss: 1148.2731\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1148.9736 - val_loss: 1071.0981\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 1081.4441 - val_loss: 991.0572\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 984.7613 - val_loss: 902.4349\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 812.0482 - val_loss: 814.8500\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 746.3579 - val_loss: 727.9167\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 674.9228 - val_loss: 648.1857\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 586.2868 - val_loss: 576.0322\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 555.9859 - val_loss: 510.9651\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 423.0191 - val_loss: 456.6966\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 421.1946 - val_loss: 409.2600\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 344.7895 - val_loss: 370.4874\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 323.7007 - val_loss: 337.3872\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 302.8406 - val_loss: 310.7463\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 299.2499 - val_loss: 289.4512\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 254.1818 - val_loss: 272.8661\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 233.4219 - val_loss: 259.0876\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 222.7953 - val_loss: 246.7451\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 231.6820 - val_loss: 237.2162\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 200.2779 - val_loss: 229.8037\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 217.1483 - val_loss: 223.5466\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 192.1463 - val_loss: 218.8514\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 211.0674 - val_loss: 213.0873\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 179.6107 - val_loss: 209.6715\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 195.0721 - val_loss: 205.8767\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 184.2844 - val_loss: 201.9985\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 188.3438 - val_loss: 199.7812\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 183.3663 - val_loss: 197.0154\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 183.6881 - val_loss: 193.5759\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 187.9913 - val_loss: 191.4678\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 152.4315 - val_loss: 189.5426\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 164.9847 - val_loss: 187.0022\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 164.5997 - val_loss: 185.0574\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 151.4500 - val_loss: 182.4963\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 177.9902 - val_loss: 180.2820\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  20 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 14ms/step - loss: 1685.0890 - val_loss: 1399.3148\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1661.5625 - val_loss: 1379.1754\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1472.2725 - val_loss: 1358.2749\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1584.8172 - val_loss: 1334.4642\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1445.8235 - val_loss: 1307.7385\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1536.0820 - val_loss: 1277.2751\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1461.0363 - val_loss: 1242.5593\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 1452.9353 - val_loss: 1202.9314\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1346.7204 - val_loss: 1157.4480\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1230.0905 - val_loss: 1106.2059\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1236.0781 - val_loss: 1049.3143\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1176.9586 - val_loss: 987.2056\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1058.5429 - val_loss: 920.7547\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1031.2088 - val_loss: 851.1373\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 933.3724 - val_loss: 778.7036\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 863.3714 - val_loss: 704.3845\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 785.8569 - val_loss: 630.9712\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 740.8235 - val_loss: 559.3207\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 608.0899 - val_loss: 492.6996\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 530.6211 - val_loss: 429.9441\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 507.7410 - val_loss: 373.8255\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 383.1234 - val_loss: 326.6511\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 384.1752 - val_loss: 286.2769\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 329.2523 - val_loss: 254.3033\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 278.8267 - val_loss: 229.6339\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 255.4058 - val_loss: 210.8508\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 237.9233 - val_loss: 196.4714\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 218.6388 - val_loss: 185.8831\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 210.4911 - val_loss: 177.9809\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 207.7677 - val_loss: 172.0953\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 186.4249 - val_loss: 167.7312\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 193.0140 - val_loss: 164.5778\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 192.8773 - val_loss: 161.8368\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 190.3744 - val_loss: 159.6708\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 184.7259 - val_loss: 157.8718\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 155.5578 - val_loss: 156.5564\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 158.7616 - val_loss: 155.0728\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 170.6271 - val_loss: 153.7492\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 176.1549 - val_loss: 152.8629\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 174.9534 - val_loss: 151.9533\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 162.6064 - val_loss: 151.0402\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 165.7463 - val_loss: 150.1778\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 171.1997 - val_loss: 149.5908\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 155.5258 - val_loss: 149.0880\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 160.4056 - val_loss: 148.3666\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 152.8354 - val_loss: 147.6718\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 166.2039 - val_loss: 146.8289\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 152.1022 - val_loss: 146.4904\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 155.4885 - val_loss: 146.0657\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 164.8032 - val_loss: 145.6400\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  21 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 15ms/step - loss: 1473.0567 - val_loss: 1547.9827\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1545.8303 - val_loss: 1534.6652\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1477.0550 - val_loss: 1521.3921\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1475.4185 - val_loss: 1505.9642\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 1475.7553 - val_loss: 1487.7203\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1434.9863 - val_loss: 1465.0638\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1439.4886 - val_loss: 1437.7318\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1522.5755 - val_loss: 1404.1232\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1353.8762 - val_loss: 1366.1512\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1433.8104 - val_loss: 1320.8934\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1371.1317 - val_loss: 1271.1074\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1231.8476 - val_loss: 1213.1890\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1196.7317 - val_loss: 1147.2100\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1116.9257 - val_loss: 1074.4209\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1070.6192 - val_loss: 992.0214\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 959.0220 - val_loss: 907.1558\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 909.6292 - val_loss: 817.0217\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 764.0977 - val_loss: 727.8171\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 716.1456 - val_loss: 641.4960\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 612.8079 - val_loss: 558.1457\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 560.8761 - val_loss: 482.6064\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 528.2657 - val_loss: 418.1581\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 432.8710 - val_loss: 363.9255\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 398.4450 - val_loss: 319.3030\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 331.7234 - val_loss: 284.3356\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 278.9102 - val_loss: 256.9551\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 290.8565 - val_loss: 235.1046\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 248.2046 - val_loss: 219.7240\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 255.4616 - val_loss: 207.5142\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 210.2171 - val_loss: 198.0123\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 233.2556 - val_loss: 191.0276\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 215.4038 - val_loss: 185.5002\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 198.5765 - val_loss: 181.0552\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 196.3516 - val_loss: 177.1136\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 196.4251 - val_loss: 173.9498\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 198.3824 - val_loss: 171.2559\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 193.3448 - val_loss: 169.0346\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 195.7243 - val_loss: 167.0353\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 177.3411 - val_loss: 165.5835\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 181.9894 - val_loss: 164.0338\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 180.9366 - val_loss: 162.7070\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 169.7392 - val_loss: 161.2760\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 159.4472 - val_loss: 160.3472\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 180.9546 - val_loss: 159.0788\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 166.0737 - val_loss: 157.9356\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 179.0746 - val_loss: 156.7497\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 170.3687 - val_loss: 155.8702\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 167.3097 - val_loss: 154.8704\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 165.4592 - val_loss: 154.1659\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 163.3445 - val_loss: 153.2681\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  22 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 14ms/step - loss: 1671.2171 - val_loss: 1685.4979\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1503.5380 - val_loss: 1670.0464\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1547.3662 - val_loss: 1655.6921\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1496.3102 - val_loss: 1641.5043\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1487.3326 - val_loss: 1626.1365\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1443.4063 - val_loss: 1608.5127\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1437.2789 - val_loss: 1587.8964\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1465.5365 - val_loss: 1563.5591\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1489.7608 - val_loss: 1534.5082\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1363.4987 - val_loss: 1500.8892\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1352.5030 - val_loss: 1460.5612\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1316.9097 - val_loss: 1415.1421\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1346.0739 - val_loss: 1362.6344\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1309.4434 - val_loss: 1305.0747\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1231.1787 - val_loss: 1241.5688\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1134.5092 - val_loss: 1173.4419\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1000.7064 - val_loss: 1101.2211\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 862.1269 - val_loss: 1025.8024\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 844.1463 - val_loss: 948.6455\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 792.6162 - val_loss: 871.3626\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 778.9748 - val_loss: 795.1479\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 671.4124 - val_loss: 723.0891\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 597.5857 - val_loss: 655.2039\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 496.4471 - val_loss: 593.0236\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 455.6292 - val_loss: 537.2858\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 422.2926 - val_loss: 487.9181\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 382.3648 - val_loss: 443.9220\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 343.1499 - val_loss: 407.4788\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 331.2597 - val_loss: 375.8857\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 286.7627 - val_loss: 349.6339\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 260.7970 - val_loss: 328.8257\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 287.8239 - val_loss: 309.4171\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 253.0929 - val_loss: 294.9945\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 222.1469 - val_loss: 282.7699\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 206.7650 - val_loss: 271.4594\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 199.1889 - val_loss: 261.9043\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 199.8649 - val_loss: 252.6607\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 191.7859 - val_loss: 244.6830\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 190.8666 - val_loss: 238.1976\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 183.9037 - val_loss: 232.2560\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 183.2577 - val_loss: 226.9144\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 176.5045 - val_loss: 222.2230\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 167.2832 - val_loss: 217.5601\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 176.7345 - val_loss: 213.5567\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 168.2531 - val_loss: 209.7349\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 189.2393 - val_loss: 205.8066\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 182.4019 - val_loss: 202.9967\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 143.5475 - val_loss: 200.2977\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 157.3724 - val_loss: 197.0836\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 154.6689 - val_loss: 194.5078\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  23 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 15ms/step - loss: 1728.9530 - val_loss: 1572.0712\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1540.2586 - val_loss: 1554.4744\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1508.1686 - val_loss: 1536.0809\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1559.9161 - val_loss: 1516.1604\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1511.3981 - val_loss: 1493.2615\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1505.8623 - val_loss: 1466.4417\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1390.7456 - val_loss: 1435.0299\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1376.5789 - val_loss: 1398.7225\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1390.3197 - val_loss: 1356.1051\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1341.7251 - val_loss: 1308.1997\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1342.9670 - val_loss: 1254.7422\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1395.3466 - val_loss: 1196.9729\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1184.8808 - val_loss: 1136.4293\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1133.6454 - val_loss: 1070.8748\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1078.3640 - val_loss: 1001.6028\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1050.6260 - val_loss: 930.8405\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 913.8384 - val_loss: 859.1491\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 862.8846 - val_loss: 786.0042\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 805.4008 - val_loss: 717.3122\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 749.4121 - val_loss: 649.8801\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 654.3479 - val_loss: 586.7095\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 572.3889 - val_loss: 529.7271\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 524.8344 - val_loss: 479.1160\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 484.2784 - val_loss: 435.1963\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 402.1037 - val_loss: 398.3997\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 377.5877 - val_loss: 367.4899\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 355.6997 - val_loss: 342.4263\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 355.2273 - val_loss: 323.6066\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 320.1645 - val_loss: 308.0270\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 268.6580 - val_loss: 296.6629\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 285.4191 - val_loss: 287.1140\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 269.0969 - val_loss: 279.2465\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 245.4628 - val_loss: 272.5431\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 231.5317 - val_loss: 267.1443\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 228.5931 - val_loss: 262.3683\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 222.1427 - val_loss: 257.8159\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 242.3288 - val_loss: 253.3608\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 230.4623 - val_loss: 249.3758\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 216.5649 - val_loss: 245.4161\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 219.3039 - val_loss: 241.4601\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 220.9087 - val_loss: 238.0064\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 184.3877 - val_loss: 234.2599\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 205.6581 - val_loss: 230.9253\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 195.9481 - val_loss: 227.7157\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 187.8673 - val_loss: 224.4493\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 185.9898 - val_loss: 221.4608\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 192.2606 - val_loss: 218.2344\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 197.0755 - val_loss: 215.9047\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 188.2262 - val_loss: 212.8661\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 184.8255 - val_loss: 210.1596\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  24 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 15ms/step - loss: 1499.7972 - val_loss: 1530.4713\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1434.0610 - val_loss: 1510.1113\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1499.8988 - val_loss: 1486.3964\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1335.7102 - val_loss: 1459.3555\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1417.5831 - val_loss: 1427.0928\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1372.3878 - val_loss: 1389.7972\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1224.2824 - val_loss: 1346.0098\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1378.5696 - val_loss: 1293.8492\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1275.9801 - val_loss: 1234.3231\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1147.1778 - val_loss: 1166.1801\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1040.7376 - val_loss: 1091.0137\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 999.4861 - val_loss: 1009.3726\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 955.8398 - val_loss: 925.3056\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 824.8312 - val_loss: 841.0566\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 765.0039 - val_loss: 758.3839\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 683.2292 - val_loss: 679.5564\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 543.8964 - val_loss: 606.8795\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 565.3738 - val_loss: 537.3107\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 455.3806 - val_loss: 476.1463\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 391.7617 - val_loss: 422.8585\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 402.0046 - val_loss: 375.0215\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 336.6085 - val_loss: 337.3156\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 288.5378 - val_loss: 305.9969\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 280.6991 - val_loss: 280.0939\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 238.1359 - val_loss: 261.8393\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 234.4154 - val_loss: 248.8096\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 215.4985 - val_loss: 238.6072\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 183.1237 - val_loss: 231.1348\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 181.2373 - val_loss: 225.4662\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 178.3594 - val_loss: 221.2876\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 193.2666 - val_loss: 217.1345\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 176.1937 - val_loss: 213.7749\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 167.6755 - val_loss: 211.2632\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 169.3977 - val_loss: 209.0823\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 166.9004 - val_loss: 206.5647\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 181.1684 - val_loss: 203.9417\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 169.0580 - val_loss: 201.4206\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 157.2070 - val_loss: 199.4335\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 169.6773 - val_loss: 197.8534\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 157.1889 - val_loss: 196.0870\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 145.0468 - val_loss: 194.6241\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 154.6959 - val_loss: 192.5863\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 146.2348 - val_loss: 191.3141\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 151.1642 - val_loss: 189.8498\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 148.2107 - val_loss: 188.7343\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 140.8277 - val_loss: 187.6611\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 144.1438 - val_loss: 186.2783\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 133.3351 - val_loss: 185.5056\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 135.1559 - val_loss: 184.0414\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 144.8393 - val_loss: 183.2181\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  25 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 15ms/step - loss: 1613.6226 - val_loss: 1477.4399\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1567.9622 - val_loss: 1454.7532\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1540.1365 - val_loss: 1429.1985\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1432.2467 - val_loss: 1399.4463\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1414.1157 - val_loss: 1364.3494\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1352.8489 - val_loss: 1322.9070\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1392.4898 - val_loss: 1273.0034\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1321.1194 - val_loss: 1215.2919\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1244.5358 - val_loss: 1149.0679\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1134.1985 - val_loss: 1075.6311\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1085.6679 - val_loss: 994.7263\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 999.2757 - val_loss: 908.1346\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 943.0285 - val_loss: 818.7133\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 766.6935 - val_loss: 729.7585\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 712.5745 - val_loss: 642.6157\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 621.2607 - val_loss: 561.8006\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 551.1055 - val_loss: 488.9417\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 481.7414 - val_loss: 425.7726\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 424.8074 - val_loss: 373.8324\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 388.9165 - val_loss: 332.9713\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 307.2962 - val_loss: 300.7349\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 264.9621 - val_loss: 276.6722\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 254.5850 - val_loss: 258.1913\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 262.9539 - val_loss: 244.6083\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 245.0001 - val_loss: 234.8077\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 251.4481 - val_loss: 226.9926\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 207.7856 - val_loss: 220.7989\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 201.3207 - val_loss: 215.6546\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 224.1351 - val_loss: 211.1566\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 218.7595 - val_loss: 207.1228\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 207.4142 - val_loss: 203.7202\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 205.2021 - val_loss: 200.3160\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 212.0628 - val_loss: 196.9952\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 184.8709 - val_loss: 194.2705\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 177.7353 - val_loss: 191.7028\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 193.1201 - val_loss: 188.6364\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 166.2841 - val_loss: 186.3127\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 177.9096 - val_loss: 183.9288\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 174.4617 - val_loss: 181.3855\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 182.5196 - val_loss: 179.0978\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 186.9557 - val_loss: 177.0043\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 175.1712 - val_loss: 174.9507\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 175.3374 - val_loss: 172.8498\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 186.2679 - val_loss: 170.8736\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 176.8823 - val_loss: 169.0760\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 155.0049 - val_loss: 167.2827\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 167.4175 - val_loss: 165.7107\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 165.2426 - val_loss: 164.1216\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 160.3098 - val_loss: 162.6189\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 153.7138 - val_loss: 161.3043\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  26 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 15ms/step - loss: 1573.4682 - val_loss: 1527.4150\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1638.9364 - val_loss: 1509.4729\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1556.8238 - val_loss: 1492.9594\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1541.6952 - val_loss: 1475.7166\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1503.1953 - val_loss: 1456.7935\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1446.0741 - val_loss: 1435.9012\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1523.2155 - val_loss: 1411.7887\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1386.8062 - val_loss: 1384.7122\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1492.6108 - val_loss: 1352.8734\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1457.7355 - val_loss: 1315.6005\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1318.9778 - val_loss: 1273.3444\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1331.7470 - val_loss: 1225.3223\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1212.6738 - val_loss: 1170.1788\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1189.8167 - val_loss: 1109.5081\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 1196.6302 - val_loss: 1042.1503\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1069.1457 - val_loss: 969.0792\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 976.9311 - val_loss: 892.2452\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 931.1583 - val_loss: 810.9785\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 812.7753 - val_loss: 729.4460\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 767.1855 - val_loss: 648.7075\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 624.3253 - val_loss: 573.9903\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 623.1830 - val_loss: 504.9509\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 536.2291 - val_loss: 444.0247\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 434.1064 - val_loss: 394.3933\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 436.6230 - val_loss: 352.6129\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 395.0109 - val_loss: 320.8503\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 335.4312 - val_loss: 296.5931\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 288.3352 - val_loss: 277.9668\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 314.8681 - val_loss: 263.7877\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 290.3705 - val_loss: 252.0140\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 285.8762 - val_loss: 243.0439\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 274.6710 - val_loss: 235.1344\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 260.4719 - val_loss: 228.4606\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 267.9014 - val_loss: 222.6632\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 226.8883 - val_loss: 217.1531\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 213.5021 - val_loss: 212.0384\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 231.1811 - val_loss: 207.3965\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 250.9605 - val_loss: 203.0436\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 241.7894 - val_loss: 199.2173\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 200.5134 - val_loss: 195.7695\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 190.1862 - val_loss: 192.1296\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 217.2144 - val_loss: 188.7127\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 210.4812 - val_loss: 185.6415\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 217.3571 - val_loss: 182.6666\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 202.5899 - val_loss: 179.9277\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 188.6440 - val_loss: 177.5518\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 190.4372 - val_loss: 174.9308\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 190.1720 - val_loss: 172.6770\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 184.7462 - val_loss: 170.5308\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 183.8430 - val_loss: 168.5801\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  27 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 16ms/step - loss: 1657.7363 - val_loss: 1402.7883\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1603.1524 - val_loss: 1385.8452\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1593.2561 - val_loss: 1369.5770\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1476.4800 - val_loss: 1352.6891\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1534.1602 - val_loss: 1334.0864\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1540.5459 - val_loss: 1312.6681\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1591.2646 - val_loss: 1286.8868\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1492.3501 - val_loss: 1257.1067\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1372.3126 - val_loss: 1222.3705\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 1360.2267 - val_loss: 1182.8906\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1364.3019 - val_loss: 1137.0745\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1327.6931 - val_loss: 1086.3605\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1152.5759 - val_loss: 1031.1166\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1154.6370 - val_loss: 970.1824\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1122.8191 - val_loss: 904.8233\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1010.2647 - val_loss: 837.3688\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 990.0947 - val_loss: 767.2979\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 837.7629 - val_loss: 696.2739\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 758.3884 - val_loss: 626.6666\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 709.1513 - val_loss: 558.4540\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 646.5769 - val_loss: 494.5697\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 565.4393 - val_loss: 436.9565\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 510.0861 - val_loss: 385.4847\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 453.5519 - val_loss: 342.3197\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 370.6173 - val_loss: 305.6342\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 309.5424 - val_loss: 276.8483\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 294.0119 - val_loss: 254.0909\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 286.2586 - val_loss: 236.7664\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 246.3166 - val_loss: 225.2812\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 236.3257 - val_loss: 215.8057\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 229.5110 - val_loss: 209.6680\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 229.5364 - val_loss: 204.8877\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 224.3825 - val_loss: 201.0769\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 234.3113 - val_loss: 197.9962\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 238.4912 - val_loss: 195.6584\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 215.7890 - val_loss: 193.3234\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 197.1231 - val_loss: 190.9012\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 208.4837 - val_loss: 189.0328\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 206.6415 - val_loss: 187.1736\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 206.0569 - val_loss: 185.2865\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 193.3755 - val_loss: 183.3074\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 187.9343 - val_loss: 181.8260\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 168.9217 - val_loss: 180.0383\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 185.3056 - val_loss: 178.4331\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 185.7657 - val_loss: 176.8201\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 187.0000 - val_loss: 175.2375\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 194.0598 - val_loss: 173.6268\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 176.8749 - val_loss: 172.1049\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 174.2128 - val_loss: 170.5843\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 179.5704 - val_loss: 169.3557\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  28 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 25ms/step - loss: 1482.5981 - val_loss: 1555.3311\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1512.0176 - val_loss: 1540.6667\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1514.1445 - val_loss: 1524.0706\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1472.3338 - val_loss: 1505.2139\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1451.0990 - val_loss: 1482.4373\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1394.7689 - val_loss: 1455.3213\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1372.2329 - val_loss: 1422.3613\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1342.9698 - val_loss: 1383.4252\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1351.5305 - val_loss: 1337.9373\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1320.7267 - val_loss: 1286.8894\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1200.3038 - val_loss: 1228.8903\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1162.6915 - val_loss: 1165.9341\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1101.9991 - val_loss: 1097.9158\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1036.7025 - val_loss: 1026.0317\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 958.7199 - val_loss: 951.6027\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 902.0665 - val_loss: 875.8271\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 863.3667 - val_loss: 799.5453\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 796.0770 - val_loss: 724.1097\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 720.0373 - val_loss: 650.7704\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 632.9505 - val_loss: 581.4407\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 524.7276 - val_loss: 518.6948\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 484.8102 - val_loss: 461.6355\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 430.8690 - val_loss: 413.1956\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 398.1791 - val_loss: 373.4751\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 344.4126 - val_loss: 342.6568\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 321.8119 - val_loss: 317.8502\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 290.5188 - val_loss: 296.6922\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 270.6885 - val_loss: 281.5996\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 263.4231 - val_loss: 270.3546\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 228.6537 - val_loss: 260.9931\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 190.2092 - val_loss: 253.9118\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 206.4566 - val_loss: 247.6247\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 206.4412 - val_loss: 243.1317\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 196.6143 - val_loss: 237.9109\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 199.2264 - val_loss: 233.7540\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 193.8823 - val_loss: 230.9710\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 207.6870 - val_loss: 227.6013\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 192.8870 - val_loss: 224.4115\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 196.1418 - val_loss: 221.1899\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 181.9460 - val_loss: 219.4130\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 175.9878 - val_loss: 216.5160\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 176.9753 - val_loss: 214.7003\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 163.5956 - val_loss: 212.9961\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 182.0632 - val_loss: 210.1080\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 183.5229 - val_loss: 207.9748\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 169.7221 - val_loss: 206.8742\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 149.9415 - val_loss: 204.7617\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 155.1206 - val_loss: 202.5979\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 154.8914 - val_loss: 201.1581\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 166.4228 - val_loss: 199.1601\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  29 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 14ms/step - loss: 1567.2200 - val_loss: 1543.0121\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1630.9794 - val_loss: 1524.5879\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1689.8619 - val_loss: 1505.9402\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1647.0393 - val_loss: 1486.2032\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1523.0458 - val_loss: 1465.2852\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1616.5383 - val_loss: 1442.5917\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1609.9040 - val_loss: 1417.3204\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1548.1572 - val_loss: 1388.3605\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1478.2812 - val_loss: 1356.2094\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1350.4858 - val_loss: 1319.8154\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1440.9191 - val_loss: 1278.4706\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1310.0544 - val_loss: 1233.7638\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1293.5077 - val_loss: 1184.2253\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1216.7685 - val_loss: 1130.8303\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1152.1894 - val_loss: 1074.1698\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1156.2988 - val_loss: 1012.5152\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1032.3165 - val_loss: 947.8922\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1104.7459 - val_loss: 880.8237\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 881.3993 - val_loss: 813.0876\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 829.7540 - val_loss: 746.1587\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 808.0493 - val_loss: 677.8757\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 714.2577 - val_loss: 612.1744\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 633.0147 - val_loss: 550.3760\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 572.3719 - val_loss: 493.0356\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 567.8997 - val_loss: 440.9246\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 471.8285 - val_loss: 393.5875\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 444.0835 - val_loss: 353.4263\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 400.7229 - val_loss: 318.3089\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 323.6304 - val_loss: 288.8414\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 321.7563 - val_loss: 263.8382\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 281.5705 - val_loss: 244.5170\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 276.9045 - val_loss: 228.0594\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 229.6002 - val_loss: 216.0312\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 242.3087 - val_loss: 206.2026\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 223.7340 - val_loss: 198.1194\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 213.9803 - val_loss: 191.7156\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 197.1920 - val_loss: 187.3695\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 212.3568 - val_loss: 183.2996\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 203.2355 - val_loss: 180.4664\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 212.4112 - val_loss: 177.7780\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 189.5352 - val_loss: 175.5276\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 175.5872 - val_loss: 173.6694\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 185.9306 - val_loss: 171.7554\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 182.3551 - val_loss: 170.4360\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 195.7280 - val_loss: 168.7442\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 184.0764 - val_loss: 167.2909\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 155.7264 - val_loss: 166.1182\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 166.1922 - val_loss: 164.7567\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 170.1791 - val_loss: 163.6925\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 176.0268 - val_loss: 162.7235\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  30 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 14ms/step - loss: 1359.0840 - val_loss: 1554.8252\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1300.4429 - val_loss: 1532.0519\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1293.7821 - val_loss: 1506.9054\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1334.5394 - val_loss: 1478.4232\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1341.4249 - val_loss: 1445.3420\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1263.7307 - val_loss: 1407.3046\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1267.1192 - val_loss: 1363.8412\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1166.1205 - val_loss: 1313.3815\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1152.7443 - val_loss: 1255.6196\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1066.8706 - val_loss: 1191.5754\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1123.7800 - val_loss: 1120.6338\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1015.9390 - val_loss: 1045.5457\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 920.9235 - val_loss: 966.5269\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 884.3913 - val_loss: 883.8809\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 756.5645 - val_loss: 801.6014\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 730.6649 - val_loss: 721.6393\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 610.4750 - val_loss: 646.9576\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 608.8224 - val_loss: 575.9290\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 566.5315 - val_loss: 512.2244\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 462.9164 - val_loss: 456.7811\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 394.2128 - val_loss: 409.4251\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 393.6032 - val_loss: 367.7325\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 334.8577 - val_loss: 332.9549\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 306.7049 - val_loss: 303.8214\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 281.4785 - val_loss: 281.3181\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 262.1766 - val_loss: 263.4941\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 268.2491 - val_loss: 248.9173\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 223.6816 - val_loss: 236.9674\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 230.8418 - val_loss: 227.6158\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 209.5877 - val_loss: 219.6998\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 193.0372 - val_loss: 213.8457\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 180.6655 - val_loss: 208.8329\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 178.4947 - val_loss: 204.7269\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 178.0214 - val_loss: 200.9120\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 171.1663 - val_loss: 198.1384\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 158.0264 - val_loss: 194.8950\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 152.8799 - val_loss: 192.5240\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 167.2875 - val_loss: 190.2027\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 165.9925 - val_loss: 188.1089\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 170.4234 - val_loss: 186.3566\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 155.3919 - val_loss: 184.4356\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 151.3592 - val_loss: 182.8473\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 155.1013 - val_loss: 181.4768\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 146.6014 - val_loss: 179.5775\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 141.6278 - val_loss: 178.3672\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 147.0304 - val_loss: 176.8649\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 139.2613 - val_loss: 175.5324\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 147.0296 - val_loss: 174.0113\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 132.2784 - val_loss: 172.7295\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 136.2149 - val_loss: 171.7887\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  31 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 14ms/step - loss: 1603.1006 - val_loss: 1665.9845\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1528.5544 - val_loss: 1650.2019\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1573.4772 - val_loss: 1632.8492\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1489.6673 - val_loss: 1614.2235\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1418.9799 - val_loss: 1592.8752\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1491.7828 - val_loss: 1567.5627\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1389.4325 - val_loss: 1537.6057\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1377.9427 - val_loss: 1501.3613\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1421.1955 - val_loss: 1457.0203\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1339.1887 - val_loss: 1404.7498\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1199.5021 - val_loss: 1345.1581\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1177.4794 - val_loss: 1276.4048\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1090.9624 - val_loss: 1200.9146\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1058.3687 - val_loss: 1119.2178\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 989.4420 - val_loss: 1031.5497\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 934.4556 - val_loss: 940.9801\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 883.8806 - val_loss: 848.7153\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 749.4592 - val_loss: 759.1443\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 728.7519 - val_loss: 672.4282\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 577.7691 - val_loss: 593.1351\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 528.2740 - val_loss: 520.0096\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 460.8114 - val_loss: 454.7913\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 386.1941 - val_loss: 401.8995\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 335.1934 - val_loss: 357.4099\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 320.7574 - val_loss: 321.3115\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 314.7937 - val_loss: 294.1663\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 258.5932 - val_loss: 273.4264\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 246.8227 - val_loss: 258.3962\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 244.8310 - val_loss: 247.3550\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 204.0953 - val_loss: 238.6331\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 181.8816 - val_loss: 231.9562\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 243.6045 - val_loss: 225.3141\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 184.6532 - val_loss: 220.7517\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 176.4777 - val_loss: 216.6868\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 183.3559 - val_loss: 213.6640\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 176.7884 - val_loss: 210.1583\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 169.3902 - val_loss: 207.2376\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 175.9307 - val_loss: 204.6422\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 166.0282 - val_loss: 202.6166\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 151.6171 - val_loss: 199.8304\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 157.6073 - val_loss: 197.7296\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 168.1253 - val_loss: 195.2692\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 174.7795 - val_loss: 193.1925\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 158.0133 - val_loss: 191.3553\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 155.1767 - val_loss: 189.1791\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 156.7364 - val_loss: 188.1402\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 162.7417 - val_loss: 186.4866\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 153.2712 - val_loss: 184.6875\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 148.1293 - val_loss: 182.8807\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 153.0863 - val_loss: 180.7379\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  32 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 15ms/step - loss: 1538.1892 - val_loss: 1544.6095\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1486.1712 - val_loss: 1516.6296\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1428.9369 - val_loss: 1486.5138\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1308.3532 - val_loss: 1452.7482\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1393.3452 - val_loss: 1414.7133\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1291.3849 - val_loss: 1372.7817\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1308.8158 - val_loss: 1323.8961\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1344.3331 - val_loss: 1266.4602\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1148.9010 - val_loss: 1203.3020\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1114.4983 - val_loss: 1132.5371\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1050.5235 - val_loss: 1055.6005\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 994.8492 - val_loss: 974.0105\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 922.7648 - val_loss: 889.5574\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 836.8598 - val_loss: 804.8923\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 717.1686 - val_loss: 720.5219\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 681.6450 - val_loss: 637.8555\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 584.8939 - val_loss: 563.3484\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 476.9192 - val_loss: 495.8393\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 443.6027 - val_loss: 437.4928\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 365.9658 - val_loss: 390.8842\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 308.0043 - val_loss: 354.5302\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 293.9372 - val_loss: 326.5344\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 259.8827 - val_loss: 304.8800\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 251.3481 - val_loss: 289.0759\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 214.9393 - val_loss: 277.8128\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 223.4314 - val_loss: 269.5039\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 208.5019 - val_loss: 261.2028\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 223.1673 - val_loss: 254.2396\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 207.7294 - val_loss: 248.8687\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 222.4876 - val_loss: 242.9748\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 211.1612 - val_loss: 238.1324\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 211.9988 - val_loss: 233.8355\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 197.7678 - val_loss: 230.3008\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 203.4345 - val_loss: 224.9810\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 197.6710 - val_loss: 221.9804\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 197.7776 - val_loss: 218.2021\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 208.5420 - val_loss: 214.9919\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 177.1481 - val_loss: 211.8829\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 171.7893 - val_loss: 209.4770\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 185.6661 - val_loss: 206.6698\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 185.1859 - val_loss: 203.8486\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 187.3733 - val_loss: 201.6641\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 164.5042 - val_loss: 199.3358\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 184.4943 - val_loss: 196.2175\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 177.6929 - val_loss: 194.3300\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 189.5011 - val_loss: 192.4788\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 157.0527 - val_loss: 190.9317\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 165.0553 - val_loss: 189.0363\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 148.5701 - val_loss: 186.9131\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 157.6358 - val_loss: 186.0706\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  33 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 25ms/step - loss: 1521.6960 - val_loss: 1409.3951\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1534.7333 - val_loss: 1381.6626\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1463.9204 - val_loss: 1348.1313\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1461.1813 - val_loss: 1306.3141\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1415.6592 - val_loss: 1257.1248\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1401.6056 - val_loss: 1200.7695\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1308.5325 - val_loss: 1138.5925\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1207.6129 - val_loss: 1071.1155\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1088.8499 - val_loss: 998.8600\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1020.2197 - val_loss: 922.6314\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1044.0958 - val_loss: 844.5477\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 893.8331 - val_loss: 767.3699\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 833.9861 - val_loss: 691.7266\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 701.3197 - val_loss: 618.7691\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 680.3710 - val_loss: 550.2462\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 583.4308 - val_loss: 488.5457\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 503.0322 - val_loss: 434.5122\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 492.2845 - val_loss: 387.0387\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 413.8192 - val_loss: 348.4001\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 357.7072 - val_loss: 316.8557\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 346.5134 - val_loss: 290.3684\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 331.8941 - val_loss: 269.3687\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 315.2217 - val_loss: 252.1672\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 274.3436 - val_loss: 238.3568\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 269.2734 - val_loss: 225.6503\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 252.4556 - val_loss: 217.7590\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 234.8383 - val_loss: 209.5586\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 219.9663 - val_loss: 203.2717\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 230.6678 - val_loss: 197.3728\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 231.6259 - val_loss: 191.9972\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 230.2606 - val_loss: 187.5667\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 214.7106 - val_loss: 184.1205\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 204.5145 - val_loss: 180.7558\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 195.6025 - val_loss: 177.7664\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 211.0913 - val_loss: 174.6686\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 204.9654 - val_loss: 171.8790\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 200.1067 - val_loss: 169.7898\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 175.3711 - val_loss: 168.1555\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 192.8836 - val_loss: 166.3593\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 185.4055 - val_loss: 164.6494\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 176.2050 - val_loss: 162.9454\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 171.1063 - val_loss: 161.6277\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 177.1913 - val_loss: 160.1869\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 175.5310 - val_loss: 159.1797\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 166.9635 - val_loss: 157.5587\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 171.5756 - val_loss: 157.1008\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 169.8442 - val_loss: 155.9201\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 187.5010 - val_loss: 154.3277\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 177.3543 - val_loss: 153.2341\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 170.2649 - val_loss: 151.9899\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  34 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 16ms/step - loss: 1611.3034 - val_loss: 1427.4052\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1592.2112 - val_loss: 1417.7230\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1542.2280 - val_loss: 1408.4647\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1567.4203 - val_loss: 1398.8027\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1654.5921 - val_loss: 1387.8589\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1476.2321 - val_loss: 1375.5320\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1541.8920 - val_loss: 1360.6320\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1560.1488 - val_loss: 1343.2468\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1585.7007 - val_loss: 1322.3832\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1425.0701 - val_loss: 1298.2915\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1409.3920 - val_loss: 1269.8951\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1379.3113 - val_loss: 1237.6128\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1432.7694 - val_loss: 1201.7426\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1325.6516 - val_loss: 1162.1577\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1302.6787 - val_loss: 1117.3859\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1138.7574 - val_loss: 1067.9349\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1201.7983 - val_loss: 1013.0440\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1056.6371 - val_loss: 953.7952\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1091.9134 - val_loss: 890.8759\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 933.4405 - val_loss: 826.5973\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 872.8887 - val_loss: 762.1249\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 767.1865 - val_loss: 698.5706\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 700.7171 - val_loss: 637.5242\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 626.6324 - val_loss: 581.2698\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 561.0390 - val_loss: 529.2954\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 482.2342 - val_loss: 483.5963\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 475.1349 - val_loss: 441.8097\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 449.3890 - val_loss: 406.7526\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 378.7228 - val_loss: 376.0081\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 360.6648 - val_loss: 349.8738\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 342.1023 - val_loss: 327.9096\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 341.6887 - val_loss: 309.0115\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 314.7213 - val_loss: 292.3207\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 278.9453 - val_loss: 277.5426\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 250.9044 - val_loss: 264.9107\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 266.0124 - val_loss: 253.3412\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 246.3914 - val_loss: 243.1045\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 227.3419 - val_loss: 233.7302\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 233.6085 - val_loss: 225.3208\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 209.4761 - val_loss: 217.7544\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 200.2587 - val_loss: 210.9888\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 204.2705 - val_loss: 204.7037\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 220.1077 - val_loss: 198.6295\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 209.0355 - val_loss: 193.6457\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 202.7481 - val_loss: 189.1687\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 214.4346 - val_loss: 184.1112\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 183.7345 - val_loss: 180.4476\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 198.5459 - val_loss: 177.0216\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 184.7652 - val_loss: 173.8871\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 173.0310 - val_loss: 170.7371\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  35 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 14ms/step - loss: 1513.8115 - val_loss: 1484.3606\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1434.8501 - val_loss: 1462.7102\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1413.2953 - val_loss: 1437.9116\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1370.1731 - val_loss: 1409.1263\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1431.1179 - val_loss: 1374.9960\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1370.0404 - val_loss: 1334.6780\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1242.0451 - val_loss: 1287.2030\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1242.6939 - val_loss: 1231.0013\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1221.1224 - val_loss: 1167.0209\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1117.5217 - val_loss: 1096.3433\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1014.7892 - val_loss: 1018.8774\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 995.2667 - val_loss: 936.8448\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1003.5925 - val_loss: 853.9624\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 855.5668 - val_loss: 773.7106\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 689.5519 - val_loss: 696.4894\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 660.9265 - val_loss: 623.9823\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 552.0110 - val_loss: 558.9943\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 538.4908 - val_loss: 500.0779\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 472.5768 - val_loss: 448.2127\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 403.6682 - val_loss: 404.7226\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 363.2693 - val_loss: 366.7401\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 307.3803 - val_loss: 334.6957\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 294.3094 - val_loss: 307.9154\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 300.5359 - val_loss: 285.9710\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 249.1697 - val_loss: 269.8219\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 267.1795 - val_loss: 256.8175\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 198.9635 - val_loss: 247.4057\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 196.1799 - val_loss: 239.4584\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 222.1140 - val_loss: 233.9358\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 199.2524 - val_loss: 228.5470\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 187.1621 - val_loss: 224.6784\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 205.8850 - val_loss: 221.5552\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 176.7328 - val_loss: 218.2969\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 171.4518 - val_loss: 215.4862\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 193.1678 - val_loss: 212.2346\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 168.0553 - val_loss: 209.6776\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 177.7966 - val_loss: 207.4365\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 177.8208 - val_loss: 205.3208\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 178.6934 - val_loss: 202.5966\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 166.5354 - val_loss: 200.7973\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 184.3320 - val_loss: 198.8217\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 170.2422 - val_loss: 197.0195\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 176.4481 - val_loss: 194.5969\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 171.4253 - val_loss: 193.0617\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 171.7937 - val_loss: 191.2172\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 160.8430 - val_loss: 189.8491\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 170.1867 - val_loss: 188.4493\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 157.6249 - val_loss: 186.9725\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 147.2902 - val_loss: 185.5433\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 141.2329 - val_loss: 183.7297\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  36 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 17ms/step - loss: 1477.9847 - val_loss: 1439.1803\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1455.9637 - val_loss: 1413.9457\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1480.6422 - val_loss: 1386.8096\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1424.3555 - val_loss: 1356.4485\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1336.7652 - val_loss: 1323.3942\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1398.7141 - val_loss: 1285.6188\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1329.2385 - val_loss: 1243.3342\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1193.7823 - val_loss: 1195.5276\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1243.3096 - val_loss: 1141.4363\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1147.3806 - val_loss: 1081.8768\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1048.0988 - val_loss: 1016.5327\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1052.1250 - val_loss: 944.5712\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 921.4475 - val_loss: 869.0085\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 893.2109 - val_loss: 788.9741\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 801.9718 - val_loss: 706.3375\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 715.2005 - val_loss: 628.1975\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 605.2106 - val_loss: 551.5167\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 524.7288 - val_loss: 480.3389\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 472.3902 - val_loss: 417.1787\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 428.2649 - val_loss: 363.7892\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 369.2591 - val_loss: 319.6512\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 340.4072 - val_loss: 285.7899\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 284.1737 - val_loss: 261.3377\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 270.1248 - val_loss: 241.6617\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 257.9298 - val_loss: 228.1526\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 226.2442 - val_loss: 217.4942\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 198.3749 - val_loss: 209.5999\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 206.2995 - val_loss: 203.6054\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 215.9299 - val_loss: 198.6393\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 212.9119 - val_loss: 194.5801\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 198.0630 - val_loss: 190.2136\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 201.0269 - val_loss: 187.3972\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 192.5689 - val_loss: 185.2715\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 184.1771 - val_loss: 182.6123\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 195.4795 - val_loss: 179.9596\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 175.2827 - val_loss: 177.6159\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 185.9053 - val_loss: 175.9514\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 164.5934 - val_loss: 174.2679\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 181.1918 - val_loss: 172.6685\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 168.0196 - val_loss: 170.7679\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 163.5136 - val_loss: 169.4387\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 167.8148 - val_loss: 168.2074\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 171.6405 - val_loss: 167.4814\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 172.6576 - val_loss: 165.7241\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 159.9786 - val_loss: 164.4849\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 166.8387 - val_loss: 163.7301\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 159.5820 - val_loss: 162.4728\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 157.1626 - val_loss: 161.8426\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 167.5429 - val_loss: 160.9988\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 141.8246 - val_loss: 159.6480\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  37 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 18ms/step - loss: 1521.5943 - val_loss: 1577.5426\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 1567.9929 - val_loss: 1557.1423\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1487.3101 - val_loss: 1536.1132\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1519.1922 - val_loss: 1512.3707\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1456.8650 - val_loss: 1486.1793\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1462.4936 - val_loss: 1455.5754\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1516.4820 - val_loss: 1419.7462\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1428.8431 - val_loss: 1379.1613\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1296.2334 - val_loss: 1331.9702\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1225.1540 - val_loss: 1278.0582\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1182.3874 - val_loss: 1219.1929\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1157.0732 - val_loss: 1154.0789\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1168.2714 - val_loss: 1081.7246\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1073.1365 - val_loss: 1005.2515\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 939.4825 - val_loss: 926.9370\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 910.0118 - val_loss: 845.3550\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 779.5007 - val_loss: 763.7023\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 753.0914 - val_loss: 682.1233\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 643.9596 - val_loss: 605.1302\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 543.3773 - val_loss: 534.4522\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 494.7026 - val_loss: 469.9299\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 436.7008 - val_loss: 415.8633\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 421.1179 - val_loss: 370.1125\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 347.3293 - val_loss: 333.6805\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 290.0937 - val_loss: 306.0689\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 278.4051 - val_loss: 284.5005\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 253.5704 - val_loss: 267.4362\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 234.4546 - val_loss: 253.3389\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 236.1690 - val_loss: 242.5983\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 204.3817 - val_loss: 234.1360\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 203.5031 - val_loss: 227.3917\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 216.8593 - val_loss: 220.8532\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 174.9430 - val_loss: 216.2679\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 219.7314 - val_loss: 212.2583\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 169.7482 - val_loss: 208.2501\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 189.4574 - val_loss: 204.1119\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 182.7158 - val_loss: 201.2209\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 164.8508 - val_loss: 198.5508\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 177.2970 - val_loss: 196.2373\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 166.3222 - val_loss: 193.6264\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 190.4204 - val_loss: 191.4303\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 174.1567 - val_loss: 189.7876\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 175.7841 - val_loss: 187.4217\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 148.8604 - val_loss: 185.4666\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 162.0022 - val_loss: 184.1978\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 157.6457 - val_loss: 182.4920\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 155.9675 - val_loss: 180.9539\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 166.2659 - val_loss: 179.6064\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 161.1065 - val_loss: 178.2567\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 160.1780 - val_loss: 176.8212\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  38 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 18ms/step - loss: 1477.2263 - val_loss: 1527.8419\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1425.2357 - val_loss: 1507.4053\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1458.1150 - val_loss: 1482.9760\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1404.1064 - val_loss: 1454.2678\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1420.6754 - val_loss: 1419.4935\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 1329.1603 - val_loss: 1377.7657\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1300.4056 - val_loss: 1326.1237\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1366.4867 - val_loss: 1264.3181\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1117.8061 - val_loss: 1193.9316\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1061.0961 - val_loss: 1116.5233\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 1000.4056 - val_loss: 1028.7391\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 888.9981 - val_loss: 935.2131\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 812.6824 - val_loss: 836.2459\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 716.6237 - val_loss: 732.8383\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 663.6716 - val_loss: 632.0151\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 575.2963 - val_loss: 540.6465\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 479.8636 - val_loss: 461.2021\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 351.7169 - val_loss: 397.6022\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 356.3946 - val_loss: 342.6773\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 315.6499 - val_loss: 304.3681\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 284.3282 - val_loss: 275.0110\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 287.7010 - val_loss: 254.2063\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 233.7127 - val_loss: 239.9333\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 227.5601 - val_loss: 229.5407\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 208.7619 - val_loss: 222.0325\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 224.7050 - val_loss: 215.4785\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 212.9923 - val_loss: 209.9524\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 196.4506 - val_loss: 205.6653\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 195.2631 - val_loss: 201.8856\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 206.2804 - val_loss: 197.8299\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 206.5639 - val_loss: 194.9481\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 223.3550 - val_loss: 192.3287\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 193.4557 - val_loss: 188.9959\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 185.3190 - val_loss: 186.5143\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 191.2369 - val_loss: 184.4797\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 186.2320 - val_loss: 181.8277\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 200.9545 - val_loss: 179.5987\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 188.8966 - val_loss: 177.8750\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 171.2715 - val_loss: 175.7158\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 189.8599 - val_loss: 173.8298\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 181.7749 - val_loss: 172.1196\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 177.7490 - val_loss: 170.6895\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 170.7453 - val_loss: 169.2796\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 173.6493 - val_loss: 167.5484\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 180.7097 - val_loss: 165.8100\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 154.2404 - val_loss: 164.3246\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 158.4461 - val_loss: 162.9721\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 159.3473 - val_loss: 161.6260\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 165.9495 - val_loss: 160.2123\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 154.4629 - val_loss: 158.9008\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  39 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 15ms/step - loss: 1603.1355 - val_loss: 1551.6625\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1590.4454 - val_loss: 1538.6978\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1535.8148 - val_loss: 1526.9017\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 1533.5606 - val_loss: 1515.1180\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1578.3432 - val_loss: 1503.1157\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1595.0725 - val_loss: 1490.6937\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1516.7417 - val_loss: 1477.5011\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1577.7933 - val_loss: 1463.4253\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1463.6385 - val_loss: 1448.3691\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1521.0748 - val_loss: 1431.5939\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1637.0604 - val_loss: 1413.8933\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1361.7740 - val_loss: 1395.0994\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1331.1687 - val_loss: 1374.4592\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1457.9197 - val_loss: 1351.8083\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1499.3084 - val_loss: 1327.5483\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1367.8598 - val_loss: 1300.6088\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1365.8757 - val_loss: 1271.3008\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1383.8597 - val_loss: 1237.8279\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1270.4622 - val_loss: 1198.8448\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1220.8851 - val_loss: 1155.2070\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1146.3617 - val_loss: 1106.3258\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1159.1683 - val_loss: 1051.9152\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1088.8944 - val_loss: 994.2295\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1062.6058 - val_loss: 932.8968\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1011.3878 - val_loss: 868.1824\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 872.2631 - val_loss: 802.4781\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 817.9036 - val_loss: 735.5128\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 761.0223 - val_loss: 668.4969\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 707.8365 - val_loss: 604.5584\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 593.1356 - val_loss: 543.3297\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 554.0920 - val_loss: 485.4323\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 471.0886 - val_loss: 431.9556\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 465.5220 - val_loss: 384.8709\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 387.7774 - val_loss: 344.1350\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 317.7446 - val_loss: 311.0387\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 321.1965 - val_loss: 282.9617\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 277.4450 - val_loss: 261.6648\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 243.4218 - val_loss: 245.7191\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 234.3291 - val_loss: 233.4537\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 223.5191 - val_loss: 225.3722\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 208.7724 - val_loss: 218.3892\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 213.8236 - val_loss: 213.2316\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 221.2408 - val_loss: 208.8464\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 207.9123 - val_loss: 205.2868\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 196.3846 - val_loss: 202.2729\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 195.5566 - val_loss: 199.7188\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 189.8925 - val_loss: 197.1352\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 187.4628 - val_loss: 195.0403\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 176.4491 - val_loss: 193.0661\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 195.7982 - val_loss: 190.6004\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  40 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 14ms/step - loss: 1583.7249 - val_loss: 1478.6400\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1556.4677 - val_loss: 1459.7534\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1515.9645 - val_loss: 1440.2271\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1497.8679 - val_loss: 1418.7844\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1450.7885 - val_loss: 1394.5190\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1455.6803 - val_loss: 1365.4124\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1427.1500 - val_loss: 1329.9558\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1355.8218 - val_loss: 1287.8719\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1305.4732 - val_loss: 1235.9917\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1273.8872 - val_loss: 1173.2125\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1238.8372 - val_loss: 1101.8311\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1124.0296 - val_loss: 1023.4919\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1041.5190 - val_loss: 939.7696\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 983.6622 - val_loss: 851.5206\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 844.0472 - val_loss: 764.9349\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 815.0588 - val_loss: 676.4662\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 676.3192 - val_loss: 594.5886\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 604.6455 - val_loss: 517.5148\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 533.3288 - val_loss: 450.4975\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 489.3507 - val_loss: 391.9758\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 402.8038 - val_loss: 345.4677\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 354.7550 - val_loss: 310.8453\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 297.3043 - val_loss: 285.9622\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 291.6432 - val_loss: 267.7558\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 272.0733 - val_loss: 254.3579\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 261.3691 - val_loss: 245.2841\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 243.3111 - val_loss: 238.5822\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 256.3445 - val_loss: 232.1903\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 235.5954 - val_loss: 226.8889\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 215.5464 - val_loss: 222.3992\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 212.1879 - val_loss: 217.8954\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 218.1309 - val_loss: 213.7926\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 210.1768 - val_loss: 210.3287\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 212.6646 - val_loss: 206.8113\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 197.7211 - val_loss: 203.9304\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 201.5690 - val_loss: 200.8483\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 194.8176 - val_loss: 198.4976\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 181.0820 - val_loss: 196.2070\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 177.0270 - val_loss: 193.3011\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 195.0985 - val_loss: 190.4747\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 185.1256 - val_loss: 188.3348\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 183.4161 - val_loss: 185.9424\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 176.8579 - val_loss: 183.7660\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 196.6150 - val_loss: 181.9025\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 165.7261 - val_loss: 179.8223\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 175.8065 - val_loss: 177.9725\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 176.8815 - val_loss: 175.9116\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 180.3935 - val_loss: 174.0551\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 174.4095 - val_loss: 172.2830\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 170.7204 - val_loss: 170.7231\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  41 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 15ms/step - loss: 1653.1430 - val_loss: 1470.6299\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1520.0568 - val_loss: 1449.3651\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1533.5491 - val_loss: 1426.0758\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1543.1832 - val_loss: 1400.3741\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1414.1413 - val_loss: 1371.0681\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1415.8190 - val_loss: 1337.2070\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1366.1784 - val_loss: 1298.1653\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1336.2577 - val_loss: 1253.1455\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1322.4170 - val_loss: 1201.4526\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1281.7103 - val_loss: 1142.6548\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1162.0382 - val_loss: 1076.9839\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1048.0372 - val_loss: 1006.9762\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 987.2995 - val_loss: 931.7266\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 930.4795 - val_loss: 852.8898\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 853.8982 - val_loss: 773.5529\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 823.8364 - val_loss: 694.4891\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 686.7968 - val_loss: 620.2187\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 623.1045 - val_loss: 549.0215\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 599.7816 - val_loss: 482.2438\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 466.2606 - val_loss: 425.6897\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 423.0455 - val_loss: 373.8780\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 356.0945 - val_loss: 331.9456\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 334.8414 - val_loss: 294.6738\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 278.5381 - val_loss: 265.0827\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 281.3416 - val_loss: 240.7498\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 237.7333 - val_loss: 223.5605\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 218.6303 - val_loss: 209.6298\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 201.4116 - val_loss: 199.1638\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 211.1349 - val_loss: 191.6649\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 191.2076 - val_loss: 186.1391\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 194.7792 - val_loss: 182.0770\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 194.5676 - val_loss: 178.8696\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 175.1892 - val_loss: 175.9901\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 176.5657 - val_loss: 173.6967\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 171.7464 - val_loss: 171.7470\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 159.6641 - val_loss: 169.6978\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 167.3713 - val_loss: 168.3598\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 179.6290 - val_loss: 166.5366\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 155.3883 - val_loss: 165.0186\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 161.1481 - val_loss: 163.6412\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 159.3258 - val_loss: 162.5298\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 159.5026 - val_loss: 160.9140\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 145.9390 - val_loss: 159.1204\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 174.1909 - val_loss: 157.3450\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 157.1251 - val_loss: 156.1016\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 146.2265 - val_loss: 154.6856\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 151.9959 - val_loss: 153.4617\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 148.4061 - val_loss: 151.8534\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 149.2558 - val_loss: 150.9382\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 145.7292 - val_loss: 149.4387\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  42 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 15ms/step - loss: 1516.8361 - val_loss: 1518.8734\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1567.2332 - val_loss: 1505.6488\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1550.2945 - val_loss: 1491.4458\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1477.9239 - val_loss: 1475.3044\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1549.5910 - val_loss: 1456.5217\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1515.0187 - val_loss: 1434.4529\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1601.2899 - val_loss: 1407.1407\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1495.2747 - val_loss: 1375.2100\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1362.0836 - val_loss: 1337.6273\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1343.5125 - val_loss: 1291.5035\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1284.6444 - val_loss: 1236.8075\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1242.7190 - val_loss: 1173.7931\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1213.4032 - val_loss: 1103.6312\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1129.1576 - val_loss: 1028.9927\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1076.5208 - val_loss: 949.3897\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 927.4258 - val_loss: 867.7007\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 854.7927 - val_loss: 784.6066\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 780.4061 - val_loss: 700.2659\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 705.6239 - val_loss: 617.5348\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 604.1154 - val_loss: 539.6833\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 535.3360 - val_loss: 468.0699\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 471.5816 - val_loss: 406.2484\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 397.5808 - val_loss: 356.0913\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 354.4144 - val_loss: 316.0314\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 337.9905 - val_loss: 287.5601\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 263.4885 - val_loss: 268.4333\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 277.9068 - val_loss: 254.9516\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 261.2674 - val_loss: 246.3482\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 241.3122 - val_loss: 239.7629\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 226.1181 - val_loss: 234.9740\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 226.7849 - val_loss: 230.7092\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 204.3138 - val_loss: 226.6716\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 215.3549 - val_loss: 222.4174\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 224.6856 - val_loss: 219.6344\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 227.0481 - val_loss: 215.5282\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 192.9069 - val_loss: 212.8055\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 203.4441 - val_loss: 209.0639\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 205.1650 - val_loss: 206.0421\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 205.8148 - val_loss: 203.2507\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 196.9519 - val_loss: 200.4693\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 177.0672 - val_loss: 198.9719\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 188.3894 - val_loss: 196.1589\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 188.1598 - val_loss: 193.9689\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 168.1040 - val_loss: 191.8933\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 175.7516 - val_loss: 189.5852\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 174.9322 - val_loss: 187.8935\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 161.6666 - val_loss: 186.6235\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 178.2796 - val_loss: 184.1340\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 160.8588 - val_loss: 182.9230\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 165.1988 - val_loss: 181.1080\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  43 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 16ms/step - loss: 1440.6711 - val_loss: 1664.3160\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1427.6834 - val_loss: 1649.1016\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1397.1145 - val_loss: 1631.2235\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1458.3503 - val_loss: 1609.5112\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1494.7391 - val_loss: 1583.4910\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1375.2645 - val_loss: 1553.2310\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1363.6041 - val_loss: 1516.0352\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1264.7445 - val_loss: 1473.5938\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1348.7538 - val_loss: 1425.8621\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1237.8272 - val_loss: 1372.2115\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1245.1906 - val_loss: 1313.5763\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1125.5015 - val_loss: 1248.2261\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1035.2819 - val_loss: 1176.8451\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 993.0903 - val_loss: 1097.0077\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 856.9312 - val_loss: 1011.9100\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 866.5965 - val_loss: 918.2149\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 753.2145 - val_loss: 821.6305\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 643.0675 - val_loss: 729.5007\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 596.0028 - val_loss: 641.0657\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 558.6665 - val_loss: 562.8915\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 464.2411 - val_loss: 496.4963\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 395.4906 - val_loss: 439.6709\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 320.4438 - val_loss: 394.6797\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 320.3590 - val_loss: 358.0041\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 292.4120 - val_loss: 329.1989\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 285.6697 - val_loss: 307.4382\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 265.1868 - val_loss: 290.8451\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 241.9399 - val_loss: 277.6303\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 223.8109 - val_loss: 267.3760\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 217.3173 - val_loss: 258.7502\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 211.3193 - val_loss: 252.5977\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 214.0674 - val_loss: 247.6101\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 213.2201 - val_loss: 242.7389\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 212.1103 - val_loss: 238.8148\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 182.7600 - val_loss: 235.3058\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 186.3337 - val_loss: 231.8932\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 189.7225 - val_loss: 229.4621\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 168.5554 - val_loss: 226.4381\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 171.5794 - val_loss: 223.0820\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 169.9033 - val_loss: 221.1013\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 163.5653 - val_loss: 218.2359\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 165.5445 - val_loss: 216.6426\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 177.9307 - val_loss: 214.5978\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 175.0343 - val_loss: 212.6323\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 154.6059 - val_loss: 210.8315\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 165.0875 - val_loss: 208.8067\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 161.0105 - val_loss: 206.4045\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 176.2668 - val_loss: 204.8254\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 154.0540 - val_loss: 203.1274\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 151.2765 - val_loss: 201.4692\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  44 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 16ms/step - loss: 1707.3673 - val_loss: 1468.1184\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1729.0594 - val_loss: 1451.5345\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1681.8094 - val_loss: 1434.9244\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1656.3424 - val_loss: 1417.1257\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1580.5023 - val_loss: 1396.8440\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1583.0923 - val_loss: 1373.1210\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1419.9101 - val_loss: 1345.5183\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1549.7436 - val_loss: 1312.5746\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1445.5960 - val_loss: 1274.8959\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1458.1477 - val_loss: 1230.9343\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1334.9668 - val_loss: 1180.8167\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1303.2010 - val_loss: 1123.6245\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1287.0078 - val_loss: 1059.8127\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1112.7051 - val_loss: 990.9816\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 1095.1238 - val_loss: 916.8102\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1045.1792 - val_loss: 837.7807\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 881.1769 - val_loss: 757.1633\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 817.2537 - val_loss: 676.5668\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 748.3691 - val_loss: 595.7441\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 694.5952 - val_loss: 518.5174\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 555.4304 - val_loss: 447.8502\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 493.1487 - val_loss: 384.8084\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 437.8221 - val_loss: 329.7406\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 360.5380 - val_loss: 285.8917\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 305.2609 - val_loss: 251.8632\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 288.0312 - val_loss: 226.6078\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 283.9692 - val_loss: 208.7337\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 271.5466 - val_loss: 197.0266\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 263.6011 - val_loss: 189.6245\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 248.5369 - val_loss: 185.7295\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 228.9630 - val_loss: 182.7729\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 200.9064 - val_loss: 181.3533\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 192.9979 - val_loss: 180.6441\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 202.7971 - val_loss: 179.3303\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 198.4545 - val_loss: 178.2474\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 210.4962 - val_loss: 177.1622\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 188.1786 - val_loss: 176.5922\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 184.1001 - val_loss: 175.5304\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 189.6665 - val_loss: 173.8967\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 194.0625 - val_loss: 172.3269\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 178.4787 - val_loss: 171.5002\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 178.4165 - val_loss: 169.5602\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 169.9348 - val_loss: 168.9058\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 182.8790 - val_loss: 166.9677\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 188.1830 - val_loss: 166.2297\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 175.3906 - val_loss: 164.1552\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 168.8690 - val_loss: 163.2720\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 168.0751 - val_loss: 161.2073\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 167.0658 - val_loss: 160.3205\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 168.4706 - val_loss: 159.3077\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  45 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 16ms/step - loss: 1546.2566 - val_loss: 1348.8229\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1538.1883 - val_loss: 1334.6788\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1545.1850 - val_loss: 1317.9421\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1544.1178 - val_loss: 1297.8236\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1566.7654 - val_loss: 1273.1725\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1439.7327 - val_loss: 1243.3923\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1426.3659 - val_loss: 1208.2012\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1440.0733 - val_loss: 1165.7770\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 1246.0050 - val_loss: 1116.1105\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1236.5259 - val_loss: 1059.8541\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1182.8747 - val_loss: 996.1954\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1106.6058 - val_loss: 926.6467\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1047.5956 - val_loss: 851.6940\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1017.6551 - val_loss: 773.0391\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 894.1827 - val_loss: 693.8910\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 760.7217 - val_loss: 615.6600\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 682.7775 - val_loss: 539.6219\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 548.7632 - val_loss: 470.1437\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 513.5698 - val_loss: 408.1136\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 435.0040 - val_loss: 355.1274\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 392.9420 - val_loss: 310.1652\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 374.9977 - val_loss: 275.1017\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 367.7975 - val_loss: 246.6272\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 318.4361 - val_loss: 226.7425\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 290.9769 - val_loss: 210.3102\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 278.2822 - val_loss: 197.6564\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 252.0637 - val_loss: 187.5907\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 239.2912 - val_loss: 180.7219\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 223.5830 - val_loss: 174.9444\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 206.7317 - val_loss: 169.9183\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 233.1417 - val_loss: 165.1758\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 219.5491 - val_loss: 161.6883\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 234.5314 - val_loss: 158.4942\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 215.3869 - val_loss: 155.0389\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 227.3410 - val_loss: 152.6888\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 222.8816 - val_loss: 150.2422\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 214.5049 - val_loss: 148.0522\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 217.7276 - val_loss: 145.9535\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 213.2073 - val_loss: 143.4511\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 195.2098 - val_loss: 141.9323\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 206.6808 - val_loss: 140.1007\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 200.7619 - val_loss: 138.1651\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 225.4358 - val_loss: 135.8150\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 205.1726 - val_loss: 134.3118\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 203.3831 - val_loss: 132.4998\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 192.5456 - val_loss: 130.9544\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 187.8570 - val_loss: 129.6482\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 188.0592 - val_loss: 128.3429\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 204.9994 - val_loss: 126.8211\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 176.7582 - val_loss: 125.5971\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  46 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 25ms/step - loss: 1472.4302 - val_loss: 1375.3499\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1559.1260 - val_loss: 1353.5343\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1485.9858 - val_loss: 1327.7594\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1432.8085 - val_loss: 1297.0482\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1459.2275 - val_loss: 1261.3184\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1319.8704 - val_loss: 1218.4725\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1348.0033 - val_loss: 1169.9683\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1243.3024 - val_loss: 1114.6824\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1201.5606 - val_loss: 1054.5548\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1167.1394 - val_loss: 986.8630\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1072.4395 - val_loss: 914.7355\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 972.0095 - val_loss: 836.0894\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1014.9476 - val_loss: 754.0999\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 858.3442 - val_loss: 670.5103\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 753.4964 - val_loss: 588.4157\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 644.4419 - val_loss: 507.1597\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 585.1291 - val_loss: 432.1048\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 504.3608 - val_loss: 365.3563\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 447.4762 - val_loss: 307.7760\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 361.9731 - val_loss: 263.7978\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 311.7934 - val_loss: 229.8575\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 265.4967 - val_loss: 206.5352\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 232.7204 - val_loss: 192.4212\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 196.9167 - val_loss: 183.9436\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 209.7038 - val_loss: 179.8168\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 187.4666 - val_loss: 178.0207\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 191.4375 - val_loss: 177.2025\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 181.0042 - val_loss: 177.5323\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 163.0328 - val_loss: 177.9020\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 158.4917 - val_loss: 177.4417\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 164.4417 - val_loss: 175.8905\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 172.6957 - val_loss: 175.7225\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 158.3979 - val_loss: 175.5575\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 155.2814 - val_loss: 174.6028\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 165.8194 - val_loss: 173.4903\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 154.7268 - val_loss: 172.4884\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 152.7949 - val_loss: 171.1925\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 153.2499 - val_loss: 171.1211\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 150.7805 - val_loss: 170.8239\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 136.7696 - val_loss: 170.1077\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 148.2575 - val_loss: 168.0941\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 153.9602 - val_loss: 166.8319\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 151.5286 - val_loss: 167.1174\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 160.8666 - val_loss: 166.2300\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 149.5529 - val_loss: 165.7098\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 139.1338 - val_loss: 165.3708\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 148.8613 - val_loss: 165.1863\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 136.3640 - val_loss: 164.6842\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 147.2294 - val_loss: 163.3398\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 143.2801 - val_loss: 162.5527\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  47 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 25ms/step - loss: 1621.1019 - val_loss: 1538.5616\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1564.1917 - val_loss: 1518.5394\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1583.5581 - val_loss: 1497.4525\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 1504.0239 - val_loss: 1475.6770\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1436.7492 - val_loss: 1451.3940\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1386.6045 - val_loss: 1424.5726\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1386.6136 - val_loss: 1393.6674\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1331.4012 - val_loss: 1360.5636\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1377.5044 - val_loss: 1322.2262\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1310.9171 - val_loss: 1279.9939\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1275.3108 - val_loss: 1233.9595\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1204.6657 - val_loss: 1184.7911\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1163.2779 - val_loss: 1130.5619\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1151.3496 - val_loss: 1072.6830\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1079.7276 - val_loss: 1013.1278\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 971.9205 - val_loss: 952.4812\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 943.7358 - val_loss: 891.4968\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 851.1632 - val_loss: 830.4219\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 790.0332 - val_loss: 770.5948\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 711.6927 - val_loss: 714.9243\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 624.6991 - val_loss: 660.6401\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 591.8407 - val_loss: 609.8870\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 504.1846 - val_loss: 563.6443\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 514.8833 - val_loss: 521.4120\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 436.3151 - val_loss: 483.8809\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 415.4194 - val_loss: 450.0204\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 389.7543 - val_loss: 420.0605\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 373.3593 - val_loss: 394.3646\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 386.3463 - val_loss: 371.3810\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 326.6485 - val_loss: 351.9675\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 331.6553 - val_loss: 334.0470\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 287.1460 - val_loss: 318.6461\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 307.2701 - val_loss: 303.6146\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 263.3461 - val_loss: 290.5544\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 264.9266 - val_loss: 279.0821\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 259.2517 - val_loss: 268.5715\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 266.5926 - val_loss: 259.1076\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 222.5785 - val_loss: 249.4612\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 239.9558 - val_loss: 241.4008\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 222.1820 - val_loss: 233.9628\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 256.9891 - val_loss: 226.2718\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 225.9108 - val_loss: 220.6399\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 208.5005 - val_loss: 214.5339\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 220.3329 - val_loss: 209.7799\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 212.6360 - val_loss: 204.6048\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 200.6121 - val_loss: 199.8119\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 206.4616 - val_loss: 195.5716\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 183.5652 - val_loss: 191.8559\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 186.2777 - val_loss: 188.9522\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 161.4136 - val_loss: 185.4475\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  48 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 23ms/step - loss: 1575.2400 - val_loss: 1449.1466\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1515.1321 - val_loss: 1426.3761\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1462.0020 - val_loss: 1401.2635\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1438.2698 - val_loss: 1372.3828\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1424.9809 - val_loss: 1338.6421\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1373.3749 - val_loss: 1299.8293\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1303.2344 - val_loss: 1254.6639\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1273.8144 - val_loss: 1201.7455\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1203.2821 - val_loss: 1141.7354\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1198.6853 - val_loss: 1072.5746\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1125.2686 - val_loss: 997.1089\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1007.6195 - val_loss: 916.4948\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 887.1659 - val_loss: 834.5618\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 828.9574 - val_loss: 752.8342\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 757.7864 - val_loss: 670.4750\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 685.0075 - val_loss: 593.3846\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 581.5597 - val_loss: 524.9218\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 482.4240 - val_loss: 465.3695\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 424.3206 - val_loss: 414.7963\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 364.8840 - val_loss: 372.5573\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 352.5667 - val_loss: 339.9117\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 344.7656 - val_loss: 313.8057\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 327.6667 - val_loss: 294.0591\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 297.1082 - val_loss: 278.2169\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 281.4782 - val_loss: 265.5251\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 249.3072 - val_loss: 256.7131\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 238.1569 - val_loss: 249.4379\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 240.4876 - val_loss: 242.1628\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 237.0496 - val_loss: 236.7600\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 227.4734 - val_loss: 230.8775\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 231.0884 - val_loss: 226.8308\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 210.1841 - val_loss: 223.5223\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 182.4366 - val_loss: 220.1883\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 204.5126 - val_loss: 217.0681\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 206.9733 - val_loss: 213.6046\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 191.2154 - val_loss: 210.6770\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 206.9263 - val_loss: 207.6951\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 186.1381 - val_loss: 205.7067\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 190.3653 - val_loss: 203.6591\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 188.7617 - val_loss: 201.5017\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 191.8204 - val_loss: 199.6781\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 185.9650 - val_loss: 197.6334\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 199.2198 - val_loss: 195.5883\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 183.7847 - val_loss: 194.2234\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 172.4853 - val_loss: 192.4179\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 181.6760 - val_loss: 190.9314\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 166.0805 - val_loss: 189.3002\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 168.7182 - val_loss: 187.9216\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 177.9978 - val_loss: 186.5604\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 157.4322 - val_loss: 185.5031\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  49 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 15ms/step - loss: 1581.9540 - val_loss: 1495.5601\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1514.7470 - val_loss: 1468.6794\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1457.2597 - val_loss: 1437.7208\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1354.5523 - val_loss: 1401.9255\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1394.3204 - val_loss: 1360.3979\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1332.4742 - val_loss: 1312.7573\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1439.4730 - val_loss: 1257.4049\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1241.9735 - val_loss: 1195.0590\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1163.3329 - val_loss: 1124.0439\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1082.4352 - val_loss: 1043.4858\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1014.3198 - val_loss: 955.7167\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 924.9187 - val_loss: 863.0617\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 812.1446 - val_loss: 767.6683\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 722.2312 - val_loss: 670.1545\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 628.4899 - val_loss: 575.5880\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 516.1335 - val_loss: 491.1883\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 494.0736 - val_loss: 416.7924\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 419.6130 - val_loss: 356.6100\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 348.2498 - val_loss: 309.2913\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 319.1416 - val_loss: 272.7750\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 264.5524 - val_loss: 246.4584\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 263.1477 - val_loss: 229.3128\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 254.5886 - val_loss: 215.6636\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 237.2503 - val_loss: 207.1605\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 235.9881 - val_loss: 200.0880\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 236.0499 - val_loss: 194.7067\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 227.2964 - val_loss: 191.0293\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 219.2742 - val_loss: 187.2497\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 211.1218 - val_loss: 183.3352\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 192.7523 - val_loss: 180.6543\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 212.3795 - val_loss: 178.5203\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 196.1642 - val_loss: 176.2190\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 209.6016 - val_loss: 173.5360\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 214.7452 - val_loss: 170.8124\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 221.5187 - val_loss: 168.9546\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 206.4165 - val_loss: 167.2344\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 191.6540 - val_loss: 165.0984\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 200.1771 - val_loss: 163.9867\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 172.7216 - val_loss: 162.4627\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 175.5956 - val_loss: 160.9033\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 198.5189 - val_loss: 159.9125\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 180.3384 - val_loss: 158.3610\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 186.3979 - val_loss: 157.5486\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 174.6191 - val_loss: 156.5770\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 182.9840 - val_loss: 155.7681\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 176.6612 - val_loss: 154.6039\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 160.9672 - val_loss: 153.7886\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 170.3851 - val_loss: 153.1220\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 190.1294 - val_loss: 152.1742\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 164.7997 - val_loss: 151.1855\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  50 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 16ms/step - loss: 1485.4865 - val_loss: 1643.8724\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1515.3988 - val_loss: 1622.5089\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1368.9194 - val_loss: 1600.6100\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1406.3892 - val_loss: 1576.1289\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 1385.3397 - val_loss: 1548.1899\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1448.5512 - val_loss: 1516.9971\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1454.5197 - val_loss: 1481.1936\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1366.4971 - val_loss: 1439.6882\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1314.0062 - val_loss: 1391.3676\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1286.9794 - val_loss: 1334.9825\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1164.0231 - val_loss: 1271.2633\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1138.3789 - val_loss: 1198.2614\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1098.9737 - val_loss: 1117.8247\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 927.3740 - val_loss: 1033.8572\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 927.2565 - val_loss: 942.0524\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 812.2685 - val_loss: 849.3632\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 691.5225 - val_loss: 757.4915\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 699.2574 - val_loss: 665.4377\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 557.8487 - val_loss: 585.2524\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 526.8818 - val_loss: 511.3344\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 439.0659 - val_loss: 446.8930\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 396.3213 - val_loss: 396.3979\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 361.2562 - val_loss: 355.7869\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 327.1854 - val_loss: 322.2806\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 277.4084 - val_loss: 298.6819\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 271.1733 - val_loss: 279.4909\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 258.2679 - val_loss: 265.8756\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 237.0097 - val_loss: 255.4968\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 251.2911 - val_loss: 247.2681\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 219.3609 - val_loss: 240.3278\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 205.0926 - val_loss: 235.4122\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 210.6648 - val_loss: 230.9994\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 186.8343 - val_loss: 226.6350\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 210.1506 - val_loss: 223.2702\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 197.2003 - val_loss: 219.6798\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 173.3802 - val_loss: 216.4756\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 181.4132 - val_loss: 213.9400\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 188.9371 - val_loss: 211.5251\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 184.9798 - val_loss: 208.6220\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 168.1423 - val_loss: 205.8961\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 184.7884 - val_loss: 203.3314\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 171.1820 - val_loss: 200.5504\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 176.4714 - val_loss: 198.0934\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 167.8169 - val_loss: 195.9330\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 165.6178 - val_loss: 193.3892\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 170.1620 - val_loss: 191.4613\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 157.3756 - val_loss: 188.8165\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 159.6847 - val_loss: 186.9102\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 167.1407 - val_loss: 184.7875\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 153.1591 - val_loss: 182.5789\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Total Execution Time :  0:04:32.561869\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psU0tqXszgFY",
        "outputId": "8e2e8a6c-4c57-4509-b79e-2f35a8cf0758"
      },
      "source": [
        "# Calculate the Mean of the MSE of 50 models\r\n",
        "mean_of_mse = stats.mean(list_of_mse)\r\n",
        "\r\n",
        "# Calculate the Standard Deviation of the MSE of 50 models\r\n",
        "std_of_mse = stats.stdev(list_of_mse)\r\n",
        "\r\n",
        "# Print the Mean and Standard Deviation of MSE of 50 models\r\n",
        "print('\\n\\nMean of the MSE of 50 Models : ' , str(mean_of_mse))\r\n",
        "print('Standard Deviation of MSE of 50 Models : ' + str(std_of_mse))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Mean of the MSE of 50 Models :  176.2670903912207\n",
            "Standard Deviation of MSE of 50 Models : 15.40801984587299\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWwyc9LMx2kU"
      },
      "source": [
        "#### <font color = green> Comparision of Mean of MSE with Mean of MSE from PART A </font>\r\n",
        "<p/>\r\n",
        "\r\n",
        "<table style=\"width:20%\">\r\n",
        "  <tr>\r\n",
        "    <th>Mean of MSE of PART A</th>\r\n",
        "    <th>Mean of MSE of PART B</th>\r\n",
        "  </tr>\r\n",
        "  <tr>\r\n",
        "    <td>177.27</td>\r\n",
        "    <td>176.27</td>\r\n",
        "  </tr>\r\n",
        "</table>\r\n",
        "\r\n",
        "The table above compares the Mean of **MSE for PART A** and **Mean of MSE for PART B**. As can be seen, the value of Mean of MSE of PART B is significantly smaller than that of PART A. The mean squared error tells how close a regression line is to a set of points. The smaller the value, the closer the model is to finding the line of best fit. So the smaller value for PART B shows that the model is slightly closer to finding a line of best fit.\r\n",
        "\r\n",
        "To summarize, normalizing the features had a significant effect in reducing the MSE and finding the line of best fit\r\n",
        "\r\n",
        "<b>Note</b> : Depending on the data, it may be impossible to get a very small value for MSE."
      ]
    }
  ]
}