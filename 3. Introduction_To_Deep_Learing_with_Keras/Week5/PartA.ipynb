{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PartA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ghn3WIOzKELl"
      },
      "source": [
        "# Importing the neccessary libraries\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import statistics as stats\r\n",
        "import os\r\n",
        "import keras\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from sklearn import preprocessing\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import mean_squared_error\r\n",
        "from datetime import datetime\r\n",
        "\r\n",
        "# Forcing keras to use CPU.\r\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "HzGihXDrKInQ",
        "outputId": "1b6b3551-4299-454b-ddd5-28c389dda5a9"
      },
      "source": [
        "# Reading the Data and storing it in a dataframe\r\n",
        "\r\n",
        "df = pd.read_csv('https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0101EN/labs/data/concrete_data.csv')\r\n",
        "df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Cement</th>\n",
              "      <th>Blast Furnace Slag</th>\n",
              "      <th>Fly Ash</th>\n",
              "      <th>Water</th>\n",
              "      <th>Superplasticizer</th>\n",
              "      <th>Coarse Aggregate</th>\n",
              "      <th>Fine Aggregate</th>\n",
              "      <th>Age</th>\n",
              "      <th>Strength</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>540.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>162.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1040.0</td>\n",
              "      <td>676.0</td>\n",
              "      <td>28</td>\n",
              "      <td>79.99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>540.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>162.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1055.0</td>\n",
              "      <td>676.0</td>\n",
              "      <td>28</td>\n",
              "      <td>61.89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>332.5</td>\n",
              "      <td>142.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>228.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>932.0</td>\n",
              "      <td>594.0</td>\n",
              "      <td>270</td>\n",
              "      <td>40.27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>332.5</td>\n",
              "      <td>142.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>228.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>932.0</td>\n",
              "      <td>594.0</td>\n",
              "      <td>365</td>\n",
              "      <td>41.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>198.6</td>\n",
              "      <td>132.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>192.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>978.4</td>\n",
              "      <td>825.5</td>\n",
              "      <td>360</td>\n",
              "      <td>44.30</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Cement  Blast Furnace Slag  Fly Ash  ...  Fine Aggregate  Age  Strength\n",
              "0   540.0                 0.0      0.0  ...           676.0   28     79.99\n",
              "1   540.0                 0.0      0.0  ...           676.0   28     61.89\n",
              "2   332.5               142.5      0.0  ...           594.0  270     40.27\n",
              "3   332.5               142.5      0.0  ...           594.0  365     41.05\n",
              "4   198.6               132.4      0.0  ...           825.5  360     44.30\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqTYkPvzKLTm",
        "outputId": "3cd7bd0b-0ff0-4f38-e39f-cfd6ec9aae59"
      },
      "source": [
        "print('\\nShape of dataframe : ',df.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Shape of dataframe :  (1030, 9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "r8niVpLzKMhh",
        "outputId": "8cdd70e1-56e4-4a55-ba6a-e0d33fc5a366"
      },
      "source": [
        "# Summary of the dataset\r\n",
        "df.describe()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Cement</th>\n",
              "      <th>Blast Furnace Slag</th>\n",
              "      <th>Fly Ash</th>\n",
              "      <th>Water</th>\n",
              "      <th>Superplasticizer</th>\n",
              "      <th>Coarse Aggregate</th>\n",
              "      <th>Fine Aggregate</th>\n",
              "      <th>Age</th>\n",
              "      <th>Strength</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>281.167864</td>\n",
              "      <td>73.895825</td>\n",
              "      <td>54.188350</td>\n",
              "      <td>181.567282</td>\n",
              "      <td>6.204660</td>\n",
              "      <td>972.918932</td>\n",
              "      <td>773.580485</td>\n",
              "      <td>45.662136</td>\n",
              "      <td>35.817961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>104.506364</td>\n",
              "      <td>86.279342</td>\n",
              "      <td>63.997004</td>\n",
              "      <td>21.354219</td>\n",
              "      <td>5.973841</td>\n",
              "      <td>77.753954</td>\n",
              "      <td>80.175980</td>\n",
              "      <td>63.169912</td>\n",
              "      <td>16.705742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>102.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>121.800000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>801.000000</td>\n",
              "      <td>594.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.330000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>192.375000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>164.900000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>932.000000</td>\n",
              "      <td>730.950000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>23.710000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>272.900000</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>185.000000</td>\n",
              "      <td>6.400000</td>\n",
              "      <td>968.000000</td>\n",
              "      <td>779.500000</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>34.445000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>350.000000</td>\n",
              "      <td>142.950000</td>\n",
              "      <td>118.300000</td>\n",
              "      <td>192.000000</td>\n",
              "      <td>10.200000</td>\n",
              "      <td>1029.400000</td>\n",
              "      <td>824.000000</td>\n",
              "      <td>56.000000</td>\n",
              "      <td>46.135000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>540.000000</td>\n",
              "      <td>359.400000</td>\n",
              "      <td>200.100000</td>\n",
              "      <td>247.000000</td>\n",
              "      <td>32.200000</td>\n",
              "      <td>1145.000000</td>\n",
              "      <td>992.600000</td>\n",
              "      <td>365.000000</td>\n",
              "      <td>82.600000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Cement  Blast Furnace Slag  ...          Age     Strength\n",
              "count  1030.000000         1030.000000  ...  1030.000000  1030.000000\n",
              "mean    281.167864           73.895825  ...    45.662136    35.817961\n",
              "std     104.506364           86.279342  ...    63.169912    16.705742\n",
              "min     102.000000            0.000000  ...     1.000000     2.330000\n",
              "25%     192.375000            0.000000  ...     7.000000    23.710000\n",
              "50%     272.900000           22.000000  ...    28.000000    34.445000\n",
              "75%     350.000000          142.950000  ...    56.000000    46.135000\n",
              "max     540.000000          359.400000  ...   365.000000    82.600000\n",
              "\n",
              "[8 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reQaLIoHKOXl",
        "outputId": "7ed31678-9157-48f0-aeb6-b99fdb2124c3"
      },
      "source": [
        "# Creating dataframes of features (X) and target (Y)\r\n",
        "X = df.iloc[:, 0:8]\r\n",
        "Y = df.iloc[:,8]\r\n",
        "\r\n",
        "# Printing the dataframes X and Y to ensure we have created the dataframes with the correct columns\r\n",
        "print('The features or the predictors (X) are : ', X, '\\n\\n') \r\n",
        "print('The target (Y) is : ', Y, '\\n\\n')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The features or the predictors (X) are :        Cement  Blast Furnace Slag  ...  Fine Aggregate  Age\n",
            "0      540.0                 0.0  ...           676.0   28\n",
            "1      540.0                 0.0  ...           676.0   28\n",
            "2      332.5               142.5  ...           594.0  270\n",
            "3      332.5               142.5  ...           594.0  365\n",
            "4      198.6               132.4  ...           825.5  360\n",
            "...      ...                 ...  ...             ...  ...\n",
            "1025   276.4               116.0  ...           768.3   28\n",
            "1026   322.2                 0.0  ...           813.4   28\n",
            "1027   148.5               139.4  ...           780.0   28\n",
            "1028   159.1               186.7  ...           788.9   28\n",
            "1029   260.9               100.5  ...           761.5   28\n",
            "\n",
            "[1030 rows x 8 columns] \n",
            "\n",
            "\n",
            "The target (Y) is :  0       79.99\n",
            "1       61.89\n",
            "2       40.27\n",
            "3       41.05\n",
            "4       44.30\n",
            "        ...  \n",
            "1025    44.28\n",
            "1026    31.18\n",
            "1027    23.70\n",
            "1028    32.77\n",
            "1029    32.40\n",
            "Name: Strength, Length: 1030, dtype: float64 \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpeIxkgTKSwn"
      },
      "source": [
        "<b>Note 1</b> : Unlike the method in this course, the splitting is done using indexing instead of using the names of the columns. Additionally, a different notation is used. The word <i>features</i> is used instead of <i>predictors</i>.\r\n",
        "\r\n",
        "\r\n",
        "<b>Note 2</b> : Pandas indexes columns starting from 0. Note in the code below for the features (X) indexing is used as `[:, 0:8]`. The first part preceding the coma `(:)` tells pandas to include ALL rows of the original dataframe (df) in the new dataframe called X while the part succedding the comma `(0:8)` tells pandas to include all columns of the original dataframe (df) starting from column with index = 0 and ending with column with index = 7, <b> but not to include the column with index = 8 </b>  \r\n",
        "\r\n",
        "<b>Note 3</b> : In order to split the data into train and test sets, the train_test_split function of the sklearn library is used. `The random_state` is used to ensure that the train and test split is the same each time, i.e. the train set and the test set have the same samples each time the code is run which is good for reproducing the results. If left empty, the random state is used by `np.random`. Since the Project requires splitting data the into <b>random</b> sets, hence `random_state` is not used, i.e. no value is set for random state. As the data hase to be split randomly  into train and test sets <b>50</b> times, a for loop will be used to to split the data in train test sets for <b>each model</b> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GhHM1EdKVKn"
      },
      "source": [
        "def regression_model() :\r\n",
        "    \r\n",
        "    # Create the model\r\n",
        "    model = Sequential()\r\n",
        "    model.add(Dense(10, activation='relu', input_shape=(X.shape[1],)))\r\n",
        "    model.add(Dense(10, activation='relu'))\r\n",
        "    model.add(Dense(1))\r\n",
        "\r\n",
        "    # Compile the model\r\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\r\n",
        "    \r\n",
        "    return model"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1_0sjSwKXxO"
      },
      "source": [
        "def data_split() :\r\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)\r\n",
        "    \r\n",
        "    # Create a list containing X_train, X_test, Y_train, Y_test and return the list\r\n",
        "    splits = [X_train, X_test, Y_train, Y_test] \r\n",
        "    return splits"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXcPvjk6KaJx"
      },
      "source": [
        "<b>Note </b> : In the above function `data_split()` the <i>X_train, X_test, Y_train, Y_test</i> sets are stored in a list and list is returned. This is to ensure that the <i>X_train, X_test, Y_train, Y_test</i> sets are not printed when the function is called"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2zoMyifKZcU"
      },
      "source": [
        "def predict() :\r\n",
        "    return model.predict(X_test)\r\n",
        "\r\n",
        "def calculate_mse() :\r\n",
        "    return mean_squared_error(Y_test,Y_predicted)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ0h9iIhKfhp"
      },
      "source": [
        "<b>Note 1</b> : The function `regression_model` as defined above only **compiles** the model and doesn't fit the model to training set. This is because in PART C, the number of epochs are changed and this would allow to fit the model with new number of epochs. This function will be used for **PART A**, **PART B** and **PART C**.\r\n",
        "\r\n",
        "<b>Note 2</b> : Since the splitting data, prediction and mean squared error calculations are all being used repeatly as well, hence there are separate functions created for them to remove redundancy of typing the same line of code while also making the code more neat. However,  as we the features (X) are to be normalized only **once**, hence there is no need to create a function for it\r\n",
        "\r\n",
        "<b>Note 3</b> : As the split function is executed first, the training and test sets are obtained and hence there is no need to explicitly pass any arguments to `predict()` and `calculate_mse()` functions. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_I4y62lKjHq"
      },
      "source": [
        "# <font color = blue> PART A : BUILDING A BASELINE MODEL </font>\r\n",
        "\r\n",
        "\r\n",
        "<b>The baseline model consists of the following : </b>\r\n",
        "    <ul>\r\n",
        "        <li> Input layer with 10 nodes </li>\r\n",
        "        <li> A single hidden layer with 10 nodes and ReLU activation function </li>\r\n",
        "        <li> Adam optimizer and mean squared error loss function </li>\r\n",
        "    </ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zZdEyPhKkpB"
      },
      "source": [
        "## <font color = darkorange>Task 1 : Train and Test the Baseline Model</font>\r\n",
        "\r\n",
        "In order to train and test the the baseline model, the following steps are performed :\r\n",
        "<ol>\r\n",
        "    <li>Randomly split the data into <i>X_train, X_test, Y_train, Y_test</i> sets</li>\r\n",
        "    <li>Train the model using <i>X_train, Y_train</i> using <b>50</b> epochs</li>\r\n",
        "    <li>Get the predictions on the <i>X_test</i> set </li>\r\n",
        "    <li>Compute the <b>mean squared error</b> on the test set using sklearn</li>\r\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiRiQwFNKpSG"
      },
      "source": [
        "### <font color = #2980B9> Step 1 : Randomly split the data into <i>X_train, X_test, Y_train, Y_test </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oHZ3P66KqN1"
      },
      "source": [
        "# Split data into X_train, X_test, Y_train, Y_test\r\n",
        "X_train, X_test, Y_train, Y_test = data_split()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYkgAjRxSalS"
      },
      "source": [
        "### <font color = #2980B9> Step 2 : Train the model using <i>X_train, Y_train</i> using 50 epochs </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHmyXrRESc8I",
        "outputId": "3f9a4698-585a-49aa-b3ef-fae846c43659"
      },
      "source": [
        "# Create the model\r\n",
        "model = regression_model()\r\n",
        "\r\n",
        "# Fit the model on the train set\r\n",
        "model.fit(X_train, Y_train, validation_split=0.3, epochs=50)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 36ms/step - loss: 1054.8488 - val_loss: 428.6785\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 379.4397 - val_loss: 396.2218\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 350.9806 - val_loss: 374.9810\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 335.5570 - val_loss: 370.7472\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 306.2295 - val_loss: 363.8943\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 287.5440 - val_loss: 356.9263\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 326.7784 - val_loss: 352.4452\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 284.6088 - val_loss: 346.8658\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 301.8264 - val_loss: 341.9259\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 278.0017 - val_loss: 338.7064\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 278.2065 - val_loss: 331.9346\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 291.6934 - val_loss: 328.8265\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 279.6492 - val_loss: 324.9456\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 282.5742 - val_loss: 318.8976\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 301.6086 - val_loss: 316.0385\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 272.8912 - val_loss: 312.2323\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 255.7053 - val_loss: 307.9617\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 275.2050 - val_loss: 306.1264\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 271.3730 - val_loss: 301.5262\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 289.1884 - val_loss: 300.0665\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 279.1531 - val_loss: 296.2084\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 282.0288 - val_loss: 292.7735\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 227.0537 - val_loss: 290.3448\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 275.1495 - val_loss: 287.9096\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 260.8046 - val_loss: 284.8369\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 261.9089 - val_loss: 280.6069\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 266.7458 - val_loss: 277.7860\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 224.5407 - val_loss: 275.0746\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 222.6418 - val_loss: 270.1203\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 241.3062 - val_loss: 271.4356\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 243.0491 - val_loss: 263.0360\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 216.5793 - val_loss: 260.4737\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 227.5014 - val_loss: 254.2151\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 238.0839 - val_loss: 250.7896\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 231.6145 - val_loss: 247.5598\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 211.8485 - val_loss: 239.2723\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 234.1742 - val_loss: 238.7780\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 211.5571 - val_loss: 228.4138\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 209.9631 - val_loss: 225.5342\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 193.3025 - val_loss: 214.8161\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 194.2552 - val_loss: 211.5028\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 194.5152 - val_loss: 200.7514\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 177.7893 - val_loss: 194.4565\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 181.1683 - val_loss: 187.4010\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 161.5419 - val_loss: 181.7514\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 168.1057 - val_loss: 178.8819\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 135.2792 - val_loss: 165.6999\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 128.2257 - val_loss: 159.6650\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 133.9134 - val_loss: 153.7249\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 121.2924 - val_loss: 148.3907\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5f9dbabba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4Z2gfGKSi9G"
      },
      "source": [
        "### <font color = #2980B9> Step 3 : Get the predictions on the X_test </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icSuTFNYSjsr"
      },
      "source": [
        "# Store the predictions in a variable Y_Predicted\r\n",
        "Y_predicted = predict()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-K2xPCbSn_R"
      },
      "source": [
        "<b>Note </b> : Y_test or the original values are also sometimes refered to as Y_true and this is the notation used in the examples found on the [mean square error page of sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html). However, in this notebook the Y_test notations is used for original values "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhkqRIjBSrpH"
      },
      "source": [
        "### <font color = #2980B9> Step 4 : Compute the <i>mean squared error</i> on the test set using sklearn </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2yuAObHSoyf",
        "outputId": "6454deca-7373-4d75-c674-9183900bf39c"
      },
      "source": [
        "# Calculate the mean square error\r\n",
        "mse = calculate_mse()\r\n",
        "#mse = mean_squared_error(Y_test,Y_predicted)\r\n",
        "print('Mean Square Error (MSE) of the Baseline Model is : ' , mse)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean Square Error (MSE) of the Baseline Model is :  131.57530123855128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fu6FawzASwDx"
      },
      "source": [
        "## <font color = darkorange>Task 2 : Create 50 Models and Calculate the <i>µ</i> & <i>σ</i> of their MSE Errors</font>\r\n",
        "\r\n",
        "In order to train 50 models and calulate the mean (µ) and standard deviation (σ) of their mean square errors (MSE)  :\r\n",
        "<ol>\r\n",
        "    <li>Create an empty list <code>list_of_mse</code> to store the mean square error of each of the models</li>\r\n",
        "    <li>Define a for loop and perform each of the following steps in the loop</li>\r\n",
        "        <ol>\r\n",
        "        <li>Randomly split the data into <i>X_train, X_test, Y_train, Y_test</i> sets</li>\r\n",
        "        <li>Train the model using <i>X_train, Y_train</i> using <b>50</b> epochs</li>\r\n",
        "        <li>Evaluate the model using <i>X_test, Y_test</i></li>\r\n",
        "        <li>Get the predictions on the <i>X_test</i> set </li>\r\n",
        "        <li>Compute the <b>mean squared error</b> on the test set using sklearn</li>\r\n",
        "    </ol>\r\n",
        "</ol>\r\n",
        "\r\n",
        "<b>Note</b> : To calcuate the mean and standard deviation of the mean square errors (mse) of 50 models which are stored in <code>list_of_means</code>, I will be using the python library <code>statistics</code> which has builtin functions to help caluclate the mean and standard deviation of a list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOdlduviS1ch"
      },
      "source": [
        "### <font color = #2980B9> Step 1 : Creating the <i>list_of_mse</i> list</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5L8Nvl5MSy8d"
      },
      "source": [
        "# Create the empty lists\r\n",
        "list_of_mse = []"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orbAgCEJS4A-"
      },
      "source": [
        "### <font color = #2980B9> Step 2 : Creating 50 Models, Calculating The MSE for Each and <i>µ</i> & <i>σ</i> of the 50 MSE Values in <i>list_of_mse</i> </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9e81cNcfS4xu",
        "outputId": "8a798e75-8fe4-4217-babf-692908c8987e"
      },
      "source": [
        "# Create the for loop to split the data, create, compile & fit model, evaluate & nake predictions, caluclate mse and store\r\n",
        "# in list_of_mse\r\n",
        "\r\n",
        "start_time = datetime.now() # Starting time of the for loop execution\r\n",
        "\r\n",
        "for i in range(50) :\r\n",
        "    # Split the data into train and test set\r\n",
        "    data_split()\r\n",
        "    \r\n",
        "    # Create and compile the regression model using the function regression_model as defined in TASK 1\r\n",
        "    model = regression_model()\r\n",
        "\r\n",
        "    # Fit the model on the train set\r\n",
        "    print('\\n\\n\\nTraining Model # ' , i+1 , '\\n\\n') # Print the Model Number that is being trained\r\n",
        "    model.fit(X_train, Y_train, validation_split=0.3, epochs=50)\r\n",
        "    print('\\n')\r\n",
        "    \r\n",
        "    # Make prediction on the test set\r\n",
        "    Y_predicted = predict()\r\n",
        "    \r\n",
        "    # Calculate the mean square error\r\n",
        "    mse = calculate_mse()\r\n",
        "    print('Mean Squared Error for Training Model # ', i+1 , ' : ', mse)\r\n",
        "    \r\n",
        "    # Add the mse to the list_of_mse list\r\n",
        "    list_of_mse.append(mse)\r\n",
        "\r\n",
        "end_time = datetime.now() # Ending time of the for loop execution\r\n",
        "\r\n",
        "# Print time taken for fitting 50 models and calucating the Mean and Standard Deviation of MSE of 50 models\r\n",
        "print('\\n\\nTotal Execution Time : ' , format(end_time - start_time))\r\n",
        "    "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 488.6582 - val_loss: 512.4053\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 414.8227 - val_loss: 478.4882\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 458.1747 - val_loss: 448.7540\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 383.7593 - val_loss: 421.2603\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 376.3768 - val_loss: 405.1071\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 328.8726 - val_loss: 376.6814\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 340.1771 - val_loss: 359.9291\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 313.9383 - val_loss: 352.7042\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 298.2150 - val_loss: 328.5667\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 267.1456 - val_loss: 317.5994\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 280.3886 - val_loss: 312.5466\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 263.0121 - val_loss: 297.5966\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 274.6263 - val_loss: 294.2711\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 250.7885 - val_loss: 284.9120\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 224.2714 - val_loss: 274.8867\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 238.9115 - val_loss: 270.2336\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 222.0566 - val_loss: 262.9917\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 232.2149 - val_loss: 261.7298\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 200.5151 - val_loss: 253.7503\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 228.3450 - val_loss: 246.9316\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 216.6827 - val_loss: 259.0903\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 205.1518 - val_loss: 237.8510\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 203.9670 - val_loss: 234.3767\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 189.5383 - val_loss: 228.4312\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 177.5724 - val_loss: 224.2003\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 188.7573 - val_loss: 220.5094\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 175.4261 - val_loss: 215.8427\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 183.0845 - val_loss: 229.2338\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 191.3430 - val_loss: 208.6249\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 186.7080 - val_loss: 204.5932\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 176.8509 - val_loss: 203.6732\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 172.1996 - val_loss: 199.2271\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 152.9681 - val_loss: 194.7090\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 151.6086 - val_loss: 191.6977\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 156.3552 - val_loss: 190.3657\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 139.9378 - val_loss: 184.6362\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 150.1899 - val_loss: 184.4232\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 154.6398 - val_loss: 177.5804\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 133.9990 - val_loss: 174.0851\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 144.1046 - val_loss: 174.8935\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 141.4015 - val_loss: 173.7495\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 137.5126 - val_loss: 165.6695\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 133.6563 - val_loss: 165.7444\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 126.2733 - val_loss: 158.4553\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 126.3954 - val_loss: 164.1400\n",
            "\n",
            "\n",
            "Mean Squared Error for Training Model #  5  :  147.55520684546502\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  6 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 13ms/step - loss: 9332.2813 - val_loss: 1746.1964\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1177.3832 - val_loss: 773.6780\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 885.4476 - val_loss: 589.0225\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 621.2721 - val_loss: 557.5702\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 582.9970 - val_loss: 510.0495\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 507.7945 - val_loss: 479.8537\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 541.9489 - val_loss: 457.2078\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 507.9857 - val_loss: 435.1552\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 497.3199 - val_loss: 421.0394\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 461.2625 - val_loss: 403.8795\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 399.5913 - val_loss: 392.2285\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 394.0266 - val_loss: 380.9381\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 369.3666 - val_loss: 370.7648\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 362.3368 - val_loss: 359.7000\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 356.7678 - val_loss: 352.4515\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 342.2309 - val_loss: 343.6163\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 338.3306 - val_loss: 337.0364\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 331.1848 - val_loss: 328.7819\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 327.8512 - val_loss: 324.5406\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 304.0907 - val_loss: 317.5086\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 294.5258 - val_loss: 314.2694\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 313.5111 - val_loss: 310.1616\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 261.9962 - val_loss: 305.3149\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 311.9735 - val_loss: 299.8047\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 292.1142 - val_loss: 298.9449\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 293.7254 - val_loss: 293.0980\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 281.2631 - val_loss: 289.3375\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 291.6524 - val_loss: 287.8626\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 273.6255 - val_loss: 283.7192\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 262.3718 - val_loss: 281.0797\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 266.0042 - val_loss: 279.6003\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 232.7968 - val_loss: 275.1695\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 253.3393 - val_loss: 274.2260\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 239.1109 - val_loss: 270.5028\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 254.8426 - val_loss: 270.6089\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 246.9260 - val_loss: 265.6831\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 249.8087 - val_loss: 263.3969\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 249.5330 - val_loss: 264.9580\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 253.7418 - val_loss: 258.9391\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 225.3469 - val_loss: 259.6777\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 243.7908 - val_loss: 255.8885\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 219.0122 - val_loss: 253.7449\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 227.6861 - val_loss: 253.8177\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 237.6046 - val_loss: 249.0235\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 227.8862 - val_loss: 248.5542\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 210.5010 - val_loss: 245.8731\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 229.3688 - val_loss: 249.2983\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 219.5431 - val_loss: 241.9608\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 201.1735 - val_loss: 243.0633\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 215.6751 - val_loss: 239.6509\n",
            "\n",
            "\n",
            "Mean Squared Error for Training Model #  6  :  223.82475320538404\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  7 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 13ms/step - loss: 6263.1530 - val_loss: 1937.1804\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1413.4990 - val_loss: 794.3782\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 825.1747 - val_loss: 689.1652\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 738.1611 - val_loss: 576.6494\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 605.2189 - val_loss: 502.1664\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 511.4935 - val_loss: 429.0313\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 449.9968 - val_loss: 348.3472\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 327.0814 - val_loss: 286.6421\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 250.9532 - val_loss: 259.1617\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 212.6979 - val_loss: 248.4872\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 230.2712 - val_loss: 239.0413\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 225.6576 - val_loss: 233.4870\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 207.3751 - val_loss: 229.1215\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 191.6903 - val_loss: 223.4170\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 191.8133 - val_loss: 219.0278\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 198.9378 - val_loss: 214.4558\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 183.4114 - val_loss: 210.9855\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 173.8265 - val_loss: 207.2216\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 173.3946 - val_loss: 204.2240\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 164.9738 - val_loss: 200.2677\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 168.2282 - val_loss: 196.2003\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 166.7816 - val_loss: 194.0694\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 163.9309 - val_loss: 190.3305\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 165.5714 - val_loss: 187.5886\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 171.6897 - val_loss: 187.8858\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 159.5532 - val_loss: 182.6461\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 163.7027 - val_loss: 180.9913\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 160.6379 - val_loss: 178.9586\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 162.7093 - val_loss: 178.9733\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 142.4427 - val_loss: 176.0002\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 158.6620 - val_loss: 179.3308\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 160.1838 - val_loss: 171.0617\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 150.6826 - val_loss: 168.5478\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 129.4565 - val_loss: 166.9248\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 131.4646 - val_loss: 165.6236\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 148.4906 - val_loss: 164.4735\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 136.0994 - val_loss: 163.1283\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 138.3134 - val_loss: 161.1735\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 130.4810 - val_loss: 160.0845\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 132.6815 - val_loss: 158.1756\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 142.4576 - val_loss: 156.6213\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 149.8400 - val_loss: 155.5377\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 146.0811 - val_loss: 160.1376\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 133.5173 - val_loss: 157.3243\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 153.1718 - val_loss: 157.0537\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 126.3518 - val_loss: 150.5730\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 124.0533 - val_loss: 149.7283\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 130.6218 - val_loss: 148.1305\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 112.4950 - val_loss: 148.3819\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 113.6720 - val_loss: 145.9733\n",
            "\n",
            "\n",
            "Mean Squared Error for Training Model #  7  :  133.2665105818114\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  8 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 13ms/step - loss: 178543.4761 - val_loss: 82183.8672\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 65618.5859 - val_loss: 26015.6562\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 20785.2686 - val_loss: 7009.0063\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 5620.1819 - val_loss: 1978.3776\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1608.6551 - val_loss: 1187.7021\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 992.6591 - val_loss: 1178.9479\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 982.3359 - val_loss: 1166.7775\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 906.1496 - val_loss: 1116.7950\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 946.4963 - val_loss: 978.0725\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 848.3308 - val_loss: 602.8818\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 580.2853 - val_loss: 550.6443\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 534.5862 - val_loss: 470.9036\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 460.3508 - val_loss: 433.7551\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 422.6126 - val_loss: 411.0096\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 407.1595 - val_loss: 395.4995\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 384.3175 - val_loss: 371.0054\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 363.2302 - val_loss: 359.9510\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 342.0475 - val_loss: 359.5464\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 355.3281 - val_loss: 335.7145\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 314.0878 - val_loss: 326.9364\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 311.8938 - val_loss: 334.7126\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 294.7105 - val_loss: 312.1010\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 312.2840 - val_loss: 328.4134\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 302.3805 - val_loss: 302.0277\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 299.7378 - val_loss: 298.0302\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 285.4041 - val_loss: 291.0587\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 270.4638 - val_loss: 287.1000\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 281.3384 - val_loss: 282.9133\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 254.4759 - val_loss: 283.1223\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 232.2435 - val_loss: 275.4275\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 258.4452 - val_loss: 274.1568\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 230.6451 - val_loss: 269.4549\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 234.4130 - val_loss: 266.8641\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 245.7082 - val_loss: 264.1278\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 234.4789 - val_loss: 264.1997\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 222.3749 - val_loss: 275.1707\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 213.0779 - val_loss: 256.8201\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 239.8001 - val_loss: 257.4668\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 216.1149 - val_loss: 256.4924\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 228.2585 - val_loss: 249.9910\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 208.1653 - val_loss: 249.1225\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 210.3530 - val_loss: 247.1859\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 222.6513 - val_loss: 265.4803\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 231.1500 - val_loss: 247.2437\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 229.2510 - val_loss: 243.7318\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 221.9354 - val_loss: 241.6310\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 208.4772 - val_loss: 240.5832\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 190.3284 - val_loss: 237.5937\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 219.9200 - val_loss: 236.5371\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 196.3533 - val_loss: 235.3192\n",
            "\n",
            "\n",
            "Mean Squared Error for Training Model #  8  :  211.6171749506229\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  9 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 15ms/step - loss: 7882.6739 - val_loss: 2580.1045\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2164.9581 - val_loss: 1445.2223\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1480.6714 - val_loss: 1219.7941\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1184.9128 - val_loss: 1055.9819\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1009.9274 - val_loss: 913.0029\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 964.7684 - val_loss: 791.5389\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 821.2619 - val_loss: 696.5657\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 681.9966 - val_loss: 618.8440\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 672.4320 - val_loss: 561.1724\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 608.0056 - val_loss: 507.4878\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 505.9782 - val_loss: 468.5828\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 516.2212 - val_loss: 431.5486\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 526.1710 - val_loss: 405.3259\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 425.7834 - val_loss: 396.7065\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 420.0397 - val_loss: 363.2872\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 379.7094 - val_loss: 345.8347\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 339.6824 - val_loss: 333.2089\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 318.5641 - val_loss: 318.5964\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 336.8040 - val_loss: 307.2819\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 308.9562 - val_loss: 294.8888\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 285.0331 - val_loss: 291.5897\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 303.8741 - val_loss: 274.4312\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 257.4275 - val_loss: 268.5632\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 274.3821 - val_loss: 258.6203\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 248.9427 - val_loss: 247.4862\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 268.1751 - val_loss: 241.6485\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 230.6902 - val_loss: 233.8148\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 246.0309 - val_loss: 227.0591\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 217.3812 - val_loss: 223.6855\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 223.5711 - val_loss: 214.6089\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 235.5220 - val_loss: 211.7463\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 207.1323 - val_loss: 205.2888\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 200.6236 - val_loss: 199.7296\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 194.2087 - val_loss: 195.5643\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 179.1647 - val_loss: 197.3424\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 189.1369 - val_loss: 189.8401\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 171.7787 - val_loss: 185.0091\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 169.6700 - val_loss: 181.0774\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 162.2444 - val_loss: 180.8968\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 179.0539 - val_loss: 174.5350\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 156.7887 - val_loss: 171.4711\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 172.3477 - val_loss: 167.9150\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 167.6684 - val_loss: 169.5857\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 156.4824 - val_loss: 162.1436\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 147.5097 - val_loss: 162.2901\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 136.7796 - val_loss: 158.0819\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 141.4683 - val_loss: 155.8131\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 150.2395 - val_loss: 152.4433\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 138.8670 - val_loss: 148.9170\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 121.9545 - val_loss: 146.7685\n",
            "\n",
            "\n",
            "Mean Squared Error for Training Model #  9  :  149.6227356637645\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  10 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 14ms/step - loss: 4233.7364 - val_loss: 1879.7158\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1267.1083 - val_loss: 587.4464\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 371.7025 - val_loss: 273.1956\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 244.8761 - val_loss: 258.3159\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 248.9414 - val_loss: 252.8253\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 246.8622 - val_loss: 251.8951\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 205.3836 - val_loss: 249.4203\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 226.1636 - val_loss: 246.4322\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 197.3581 - val_loss: 245.3731\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 216.7537 - val_loss: 244.8220\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 197.3226 - val_loss: 242.6346\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 189.2755 - val_loss: 240.4569\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 214.7562 - val_loss: 238.8368\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 175.7444 - val_loss: 237.1284\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 224.5784 - val_loss: 234.3792\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 212.2686 - val_loss: 232.1485\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 182.4826 - val_loss: 229.1113\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 190.1539 - val_loss: 224.9974\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 209.1132 - val_loss: 223.0457\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 176.8348 - val_loss: 218.2583\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 175.4897 - val_loss: 213.7070\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 173.3664 - val_loss: 211.3875\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 185.5383 - val_loss: 207.3619\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 173.8002 - val_loss: 204.9700\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 186.8008 - val_loss: 203.3997\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 158.9608 - val_loss: 201.7218\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 178.3913 - val_loss: 197.6454\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 166.7586 - val_loss: 193.8938\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 146.1758 - val_loss: 190.9135\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 153.8806 - val_loss: 186.0684\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 166.7465 - val_loss: 184.7998\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 143.3187 - val_loss: 176.7943\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 146.8947 - val_loss: 174.4633\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 141.7794 - val_loss: 168.6330\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 140.8315 - val_loss: 166.2883\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 129.3937 - val_loss: 161.5835\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 129.5769 - val_loss: 159.2620\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 139.4258 - val_loss: 156.1713\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 128.2340 - val_loss: 153.1051\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 123.5546 - val_loss: 148.6761\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 119.2976 - val_loss: 145.7547\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 113.3405 - val_loss: 145.7694\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 116.3873 - val_loss: 140.7160\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 115.0595 - val_loss: 145.9966\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 113.6006 - val_loss: 139.5165\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 127.0959 - val_loss: 135.4595\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 113.1294 - val_loss: 134.8152\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 106.8695 - val_loss: 132.5141\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 111.4840 - val_loss: 138.0685\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 114.3970 - val_loss: 130.6318\n",
            "\n",
            "\n",
            "Mean Squared Error for Training Model #  10  :  114.56860599782217\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  11 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 22ms/step - loss: 3873.9783 - val_loss: 2288.2502\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1823.2073 - val_loss: 1490.6929\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1223.8379 - val_loss: 1057.0546\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 773.9027 - val_loss: 821.8813\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 669.9500 - val_loss: 637.7512\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 545.5916 - val_loss: 466.8245\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 394.5016 - val_loss: 363.3324\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 323.1073 - val_loss: 322.2766\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 308.7791 - val_loss: 289.5334\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 267.8089 - val_loss: 271.4175\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 272.0055 - val_loss: 262.0619\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 212.9904 - val_loss: 241.0186\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 225.0772 - val_loss: 230.2757\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 231.8601 - val_loss: 207.8740\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 198.9400 - val_loss: 216.1267\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 182.7819 - val_loss: 197.2676\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 179.7977 - val_loss: 198.6653\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 178.6768 - val_loss: 202.9579\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 173.8474 - val_loss: 184.9805\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 162.1402 - val_loss: 184.1788\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 173.8971 - val_loss: 180.4031\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 168.0140 - val_loss: 177.1838\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 159.6430 - val_loss: 173.3261\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 159.2504 - val_loss: 171.3899\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 165.5733 - val_loss: 166.8060\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 157.1187 - val_loss: 159.8797\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 155.7874 - val_loss: 156.3933\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 151.9559 - val_loss: 153.0078\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 146.2645 - val_loss: 150.5416\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 135.1928 - val_loss: 148.7052\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 152.1812 - val_loss: 150.3052\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 142.9701 - val_loss: 141.5478\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 132.3373 - val_loss: 140.2796\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 150.3366 - val_loss: 143.3348\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 130.2640 - val_loss: 141.1437\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 134.7555 - val_loss: 140.1152\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 133.4015 - val_loss: 137.8346\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 132.3027 - val_loss: 137.1651\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 123.1996 - val_loss: 133.0067\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 115.2208 - val_loss: 129.9739\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 118.1755 - val_loss: 129.1203\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 113.6907 - val_loss: 127.5677\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 123.7881 - val_loss: 127.2345\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 106.4388 - val_loss: 124.8576\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 117.9113 - val_loss: 123.7872\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 117.7087 - val_loss: 121.4286\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 116.8486 - val_loss: 121.8254\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 127.8069 - val_loss: 126.6320\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 118.4497 - val_loss: 127.0963\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 117.1805 - val_loss: 118.0780\n",
            "\n",
            "\n",
            "Mean Squared Error for Training Model #  11  :  114.63324763507754\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  12 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 14ms/step - loss: 853.7284 - val_loss: 405.0143\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 371.8574 - val_loss: 308.3654\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 334.6359 - val_loss: 276.2156\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 254.4202 - val_loss: 252.4814\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 243.8355 - val_loss: 232.0425\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 202.1793 - val_loss: 214.3871\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 209.4845 - val_loss: 202.0728\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 196.1881 - val_loss: 193.9500\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 187.9408 - val_loss: 188.5501\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 178.8364 - val_loss: 178.0056\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 176.8917 - val_loss: 171.7001\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 178.3149 - val_loss: 185.6738\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 160.4775 - val_loss: 162.5123\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 149.0768 - val_loss: 160.2347\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 165.5594 - val_loss: 156.9665\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 148.6494 - val_loss: 146.8683\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 150.4158 - val_loss: 141.1424\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 129.3966 - val_loss: 140.9888\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 139.2900 - val_loss: 135.5166\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 121.3116 - val_loss: 129.9598\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 131.4472 - val_loss: 134.0110\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 108.8283 - val_loss: 124.5372\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 109.4417 - val_loss: 133.7934\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 122.5093 - val_loss: 118.4358\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 106.6713 - val_loss: 118.9371\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 109.5216 - val_loss: 114.4617\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 97.3488 - val_loss: 112.0145\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 103.6686 - val_loss: 110.2788\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 107.6042 - val_loss: 108.3030\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 101.6075 - val_loss: 109.4832\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 105.0719 - val_loss: 106.2205\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 97.0434 - val_loss: 106.4396\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 95.0288 - val_loss: 105.0354\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 98.3089 - val_loss: 103.9610\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 91.2117 - val_loss: 101.5961\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 87.1220 - val_loss: 103.1466\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 99.7093 - val_loss: 99.5305\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 85.8987 - val_loss: 101.2804\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 99.1276 - val_loss: 97.4414\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 77.9023 - val_loss: 96.4071\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 82.1231 - val_loss: 96.6015\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 80.9807 - val_loss: 99.9552\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 90.0260 - val_loss: 94.5546\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 75.7677 - val_loss: 93.6120\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 85.3293 - val_loss: 92.7674\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 85.2705 - val_loss: 92.6224\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 79.8530 - val_loss: 92.2553\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 76.5068 - val_loss: 91.9488\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 81.0402 - val_loss: 91.9706\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 81.3476 - val_loss: 94.0236\n",
            "\n",
            "\n",
            "Mean Squared Error for Training Model #  12  :  86.38938120430976\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  13 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 13ms/step - loss: 124155.9586 - val_loss: 42067.8789\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 28630.2111 - val_loss: 3442.3799\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1878.4480 - val_loss: 1262.1324\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1421.7051 - val_loss: 841.7704\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 784.7463 - val_loss: 712.0969\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 682.7053 - val_loss: 671.7952\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 705.6285 - val_loss: 629.9736\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 594.0271 - val_loss: 601.5670\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 602.9155 - val_loss: 563.4738\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 514.8739 - val_loss: 530.5799\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 458.5064 - val_loss: 505.3986\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 464.5611 - val_loss: 487.3152\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 441.1156 - val_loss: 473.9290\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 428.4400 - val_loss: 463.9920\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 413.3227 - val_loss: 450.3069\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 367.4148 - val_loss: 442.3809\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 404.2623 - val_loss: 432.2023\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 377.6156 - val_loss: 420.8062\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 358.1563 - val_loss: 410.8848\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 375.1192 - val_loss: 401.6658\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 378.9544 - val_loss: 395.1893\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 344.6119 - val_loss: 383.7071\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 345.8445 - val_loss: 375.9708\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 321.2622 - val_loss: 367.0096\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 346.0550 - val_loss: 359.9639\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 298.2986 - val_loss: 351.4982\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 307.1282 - val_loss: 344.4060\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 285.8638 - val_loss: 336.2454\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 276.2674 - val_loss: 329.1320\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 285.8678 - val_loss: 320.7213\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 282.5590 - val_loss: 313.2432\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 247.0526 - val_loss: 305.6907\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 245.2327 - val_loss: 297.4509\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 225.5381 - val_loss: 290.3702\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 229.7970 - val_loss: 282.2424\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 221.2722 - val_loss: 275.8245\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 218.8761 - val_loss: 268.7833\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 220.6457 - val_loss: 262.0880\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 227.4091 - val_loss: 255.4121\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 186.3682 - val_loss: 249.7567\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 188.7746 - val_loss: 244.6914\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 223.9096 - val_loss: 238.8785\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 181.7836 - val_loss: 234.4007\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 186.5372 - val_loss: 232.2067\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 177.0927 - val_loss: 226.3252\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 194.0621 - val_loss: 222.7785\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 167.9694 - val_loss: 217.4660\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 193.3384 - val_loss: 214.4739\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 169.8413 - val_loss: 211.1382\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 153.2716 - val_loss: 206.6282\n",
            "\n",
            "\n",
            "Mean Squared Error for Training Model #  13  :  185.92150264234442\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  14 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 14ms/step - loss: 1275.7877 - val_loss: 526.1816\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 463.7088 - val_loss: 352.2349\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 308.0847 - val_loss: 302.8173\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 283.2687 - val_loss: 279.1104\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 254.6171 - val_loss: 248.7203\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 217.4354 - val_loss: 219.5808\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 198.7793 - val_loss: 208.0287\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 183.1236 - val_loss: 182.2459\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 175.1540 - val_loss: 170.3715\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 164.4597 - val_loss: 158.3173\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 153.1625 - val_loss: 149.9276\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 133.3841 - val_loss: 142.0867\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 129.0992 - val_loss: 135.0548\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 119.8588 - val_loss: 133.3526\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 126.3360 - val_loss: 127.7786\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 117.3987 - val_loss: 125.0116\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 108.5236 - val_loss: 123.3284\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 104.8959 - val_loss: 122.5475\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 104.2616 - val_loss: 130.8297\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 107.8869 - val_loss: 117.9760\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 111.2288 - val_loss: 117.1431\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 96.5405 - val_loss: 115.6975\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 95.7984 - val_loss: 115.8139\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 97.5312 - val_loss: 115.1927\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 94.5632 - val_loss: 112.9734\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 104.1928 - val_loss: 115.8468\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 100.5987 - val_loss: 113.6596\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 91.0293 - val_loss: 110.4822\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 99.8107 - val_loss: 110.1179\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 91.5508 - val_loss: 108.9149\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 96.9950 - val_loss: 109.4254\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 96.0509 - val_loss: 106.0351\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 93.4765 - val_loss: 106.7565\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 94.0663 - val_loss: 105.3506\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 86.4220 - val_loss: 104.8050\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 92.0863 - val_loss: 103.1083\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 95.9272 - val_loss: 105.1878\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 85.3295 - val_loss: 102.9988\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 82.0242 - val_loss: 113.1068\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 79.6190 - val_loss: 103.9686\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 81.7686 - val_loss: 101.0499\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 84.4960 - val_loss: 99.6853\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 84.2567 - val_loss: 98.7048\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 83.5351 - val_loss: 99.6805\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 82.4962 - val_loss: 101.8715\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 81.3179 - val_loss: 97.0286\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 80.4734 - val_loss: 97.9279\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 84.7313 - val_loss: 96.1557\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 93.0141 - val_loss: 97.0848\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 87.2882 - val_loss: 94.8436\n",
            "\n",
            "\n",
            "Mean Squared Error for Training Model #  14  :  93.12744789457331\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  15 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 14ms/step - loss: 5179.5275 - val_loss: 2490.4050\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2084.3866 - val_loss: 1291.7893\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1137.0202 - val_loss: 872.2348\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 749.7168 - val_loss: 626.0432\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 538.2340 - val_loss: 494.0154\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 414.3874 - val_loss: 419.3641\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 362.2015 - val_loss: 344.6480\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 369.6758 - val_loss: 289.5854\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 270.1787 - val_loss: 254.8520\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 203.5300 - val_loss: 232.4650\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 222.8043 - val_loss: 217.2830\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 182.2761 - val_loss: 205.4288\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 167.6210 - val_loss: 197.7178\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 147.6262 - val_loss: 190.7324\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 159.0547 - val_loss: 186.0939\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 147.3371 - val_loss: 182.1121\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 139.1557 - val_loss: 178.4164\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 160.4369 - val_loss: 176.4913\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 146.1865 - val_loss: 171.9842\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 145.7547 - val_loss: 169.2694\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 162.5896 - val_loss: 171.1270\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 151.3058 - val_loss: 166.0422\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 150.1028 - val_loss: 167.2523\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 144.3025 - val_loss: 164.5710\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 143.1159 - val_loss: 161.1975\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 137.2312 - val_loss: 163.9224\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 163.6447 - val_loss: 162.3344\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 138.5410 - val_loss: 159.3077\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 129.6059 - val_loss: 162.7800\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 132.6322 - val_loss: 156.3640\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 132.7314 - val_loss: 159.6218\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 136.5178 - val_loss: 156.5275\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 144.4758 - val_loss: 154.0279\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 133.1692 - val_loss: 153.0441\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 134.8343 - val_loss: 156.0446\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 145.8647 - val_loss: 155.2820\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 143.8294 - val_loss: 151.5397\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 128.2282 - val_loss: 153.7282\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 126.5651 - val_loss: 154.0004\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 132.3142 - val_loss: 151.7130\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 127.3925 - val_loss: 149.6240\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 123.3870 - val_loss: 155.7350\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 123.0441 - val_loss: 147.6468\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 129.8813 - val_loss: 149.6781\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 112.9243 - val_loss: 148.0970\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 113.8170 - val_loss: 146.1966\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 122.9197 - val_loss: 148.7190\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 116.7456 - val_loss: 146.6848\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 115.5234 - val_loss: 144.1716\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 132.7414 - val_loss: 146.7116\n",
            "\n",
            "\n",
            "Mean Squared Error for Training Model #  15  :  121.60360296245628\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  16 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 14ms/step - loss: 131938.5887 - val_loss: 82977.7344\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 69700.1383 - val_loss: 41215.8711\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 34752.4663 - val_loss: 19390.0156\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 16494.5466 - val_loss: 8579.9297\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 7183.4014 - val_loss: 3536.8428\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2833.6240 - val_loss: 1402.9993\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1227.5987 - val_loss: 635.1986\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 521.7873 - val_loss: 417.8194\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 400.2933 - val_loss: 348.7350\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 355.8550 - val_loss: 325.0622\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 343.7294 - val_loss: 309.4439\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 333.7711 - val_loss: 297.8080\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 307.0828 - val_loss: 291.5280\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 263.8960 - val_loss: 278.4468\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 261.9241 - val_loss: 269.3830\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 268.7171 - val_loss: 261.6522\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 247.3399 - val_loss: 253.3904\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 231.6805 - val_loss: 247.1684\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 261.6017 - val_loss: 240.2010\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 233.0792 - val_loss: 235.3127\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 214.7910 - val_loss: 231.2754\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 222.8225 - val_loss: 227.3209\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 226.8994 - val_loss: 220.2534\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 210.7907 - val_loss: 218.2581\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 205.7836 - val_loss: 213.9032\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 221.1233 - val_loss: 212.5155\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 187.9752 - val_loss: 209.1318\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 182.9809 - val_loss: 207.2881\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 183.0736 - val_loss: 206.2849\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 184.8938 - val_loss: 202.7773\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 191.9690 - val_loss: 202.0568\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 181.2160 - val_loss: 199.8500\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 173.5304 - val_loss: 198.6047\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 197.8332 - val_loss: 199.0324\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 166.5095 - val_loss: 197.9341\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 168.2947 - val_loss: 196.4847\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 165.2131 - val_loss: 195.0951\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 174.5359 - val_loss: 194.7655\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 185.1423 - val_loss: 193.5477\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 169.6606 - val_loss: 194.7758\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 166.1345 - val_loss: 193.1967\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 174.9033 - val_loss: 192.7748\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 169.9792 - val_loss: 192.0029\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 160.2905 - val_loss: 192.2962\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 161.5491 - val_loss: 190.4242\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 183.7552 - val_loss: 193.2143\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 154.0790 - val_loss: 190.5649\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 149.8225 - val_loss: 190.6887\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 152.2731 - val_loss: 189.7633\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 160.9541 - val_loss: 191.1233\n",
            "\n",
            "\n",
            "Mean Squared Error for Training Model #  16  :  161.08935387416878\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  17 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 13ms/step - loss: 57816.3300 - val_loss: 24675.2402\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 18205.8350 - val_loss: 5591.8516\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 3861.0678 - val_loss: 1100.9689\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 813.0161 - val_loss: 826.8552\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 718.2639 - val_loss: 783.0019\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 648.0189 - val_loss: 752.1747\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 596.0954 - val_loss: 723.8913\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 596.4055 - val_loss: 697.1083\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 558.9503 - val_loss: 672.2210\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 509.1529 - val_loss: 646.9775\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 486.9973 - val_loss: 625.4216\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 420.9881 - val_loss: 604.6769\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 476.5669 - val_loss: 582.7068\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 448.5522 - val_loss: 559.2409\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 425.5854 - val_loss: 540.9774\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 420.0343 - val_loss: 522.6397\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 401.6257 - val_loss: 504.7505\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 402.2243 - val_loss: 492.9055\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 423.8620 - val_loss: 477.6926\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 409.5004 - val_loss: 462.9618\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 373.4862 - val_loss: 449.5051\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 348.7509 - val_loss: 439.5453\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 370.8023 - val_loss: 426.4947\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 326.9097 - val_loss: 415.9601\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 344.6754 - val_loss: 404.9300\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 366.5110 - val_loss: 391.6254\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 321.2246 - val_loss: 381.5351\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 304.4661 - val_loss: 371.3902\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 276.8679 - val_loss: 363.7380\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 271.4425 - val_loss: 353.2324\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 266.8239 - val_loss: 343.2002\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 245.1551 - val_loss: 333.8853\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 252.8284 - val_loss: 325.1083\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 267.7574 - val_loss: 316.1559\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 240.1374 - val_loss: 308.8510\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 231.4208 - val_loss: 301.2242\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 228.3391 - val_loss: 293.2170\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 212.1312 - val_loss: 286.7519\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 237.3015 - val_loss: 280.1332\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 216.1383 - val_loss: 274.0028\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 206.9484 - val_loss: 267.7326\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 195.5813 - val_loss: 260.8824\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 199.9227 - val_loss: 257.9459\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 201.0869 - val_loss: 250.8351\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 177.0469 - val_loss: 246.3669\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 183.7022 - val_loss: 243.6081\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 192.2400 - val_loss: 238.8286\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 198.0339 - val_loss: 234.5338\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 184.3238 - val_loss: 228.9166\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 189.8964 - val_loss: 224.4831\n",
            "\n",
            "\n",
            "Mean Squared Error for Training Model #  17  :  194.6792169832348\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  18 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 14ms/step - loss: 3480.5908 - val_loss: 647.6979\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 469.9864 - val_loss: 421.3299\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 369.6752 - val_loss: 297.6737\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 293.6501 - val_loss: 272.7756\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 263.9747 - val_loss: 259.2973\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 233.3931 - val_loss: 249.2063\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 224.7844 - val_loss: 245.7908\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 222.2742 - val_loss: 236.1966\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 233.1549 - val_loss: 223.5067\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 203.2031 - val_loss: 216.1787\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 202.0228 - val_loss: 202.7340\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 179.7344 - val_loss: 192.6381\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 186.5118 - val_loss: 185.9602\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 159.5690 - val_loss: 174.9766\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 159.7907 - val_loss: 168.2501\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 147.4647 - val_loss: 157.9558\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 142.4957 - val_loss: 152.6741\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 143.7659 - val_loss: 152.9318\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 117.9690 - val_loss: 139.8027\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 116.7994 - val_loss: 134.9401\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 117.0895 - val_loss: 132.1796\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 120.5773 - val_loss: 125.9266\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 104.5236 - val_loss: 132.2273\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 109.4277 - val_loss: 133.5465\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 104.0190 - val_loss: 120.6083\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 101.9936 - val_loss: 138.0551\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 119.3967 - val_loss: 135.4833\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 101.3063 - val_loss: 127.9378\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 109.1013 - val_loss: 114.8060\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 96.6618 - val_loss: 117.6702\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 107.8975 - val_loss: 113.1268\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 83.4571 - val_loss: 114.2520\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 105.9057 - val_loss: 111.7769\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 94.6335 - val_loss: 111.9122\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 91.7042 - val_loss: 112.1854\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 89.7740 - val_loss: 111.6361\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 95.4368 - val_loss: 113.8062\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 82.4840 - val_loss: 109.1972\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 85.5715 - val_loss: 110.1341\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 82.9514 - val_loss: 109.1279\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 92.4711 - val_loss: 108.8255\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 97.2279 - val_loss: 112.1233\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 87.7319 - val_loss: 107.6658\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 88.6170 - val_loss: 107.3756\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 82.0603 - val_loss: 106.9060\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 78.0593 - val_loss: 106.3625\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 93.2090 - val_loss: 107.4014\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 85.8021 - val_loss: 105.8701\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 78.4828 - val_loss: 108.6921\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 82.4668 - val_loss: 112.1374\n",
            "\n",
            "\n",
            "Mean Squared Error for Training Model #  18  :  93.50526055423046\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  19 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 14ms/step - loss: 26232.1852 - val_loss: 14641.4375\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 12020.2486 - val_loss: 5848.2363\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 4608.9437 - val_loss: 2617.5210\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1968.5111 - val_loss: 1276.9645\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 920.3058 - val_loss: 686.7394\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 561.8894 - val_loss: 433.1139\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 358.6736 - val_loss: 329.5803\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 279.8240 - val_loss: 287.9778\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 266.2453 - val_loss: 271.8274\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 227.3724 - val_loss: 266.3797\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 254.4426 - val_loss: 264.2290\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 244.8977 - val_loss: 262.9943\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 256.8074 - val_loss: 262.7897\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 251.4592 - val_loss: 262.6303\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 234.3182 - val_loss: 262.8995\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 224.0271 - val_loss: 262.8911\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 232.1074 - val_loss: 262.8175\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 244.0780 - val_loss: 263.4347\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 246.2899 - val_loss: 262.9237\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 258.1472 - val_loss: 262.6715\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 254.2534 - val_loss: 262.4733\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 218.9097 - val_loss: 262.5170\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 252.3098 - val_loss: 262.4833\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 235.2105 - val_loss: 263.1026\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 247.1326 - val_loss: 262.5205\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 245.1164 - val_loss: 262.8142\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 226.9843 - val_loss: 262.9605\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 262.6801 - val_loss: 262.5174\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 237.0205 - val_loss: 262.5482\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 252.7418 - val_loss: 262.5900\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 232.7430 - val_loss: 262.8164\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 235.1449 - val_loss: 263.0021\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 242.0720 - val_loss: 262.7772\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 236.7620 - val_loss: 262.7408\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 221.8785 - val_loss: 263.3122\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 240.4868 - val_loss: 262.9886\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 259.0240 - val_loss: 262.5850\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 242.9836 - val_loss: 262.5996\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 227.3985 - val_loss: 262.3518\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 257.9235 - val_loss: 261.9773\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 253.3643 - val_loss: 262.2140\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 228.9400 - val_loss: 263.9312\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 242.4931 - val_loss: 261.5144\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 248.5427 - val_loss: 262.3315\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 232.9891 - val_loss: 262.0218\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 235.0520 - val_loss: 261.9285\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 244.8925 - val_loss: 263.2161\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 234.6598 - val_loss: 261.5184\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 242.3122 - val_loss: 261.5469\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 263.3198 - val_loss: 261.6205\n",
            "\n",
            "\n",
            "Mean Squared Error for Training Model #  19  :  245.66236437350943\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  20 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 14ms/step - loss: 15377.7823 - val_loss: 3319.6809\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2220.6990 - val_loss: 603.9836\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 560.2448 - val_loss: 536.0196\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 507.5798 - val_loss: 414.9489\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 403.1642 - val_loss: 369.7091\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 364.0631 - val_loss: 347.9990\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 344.8955 - val_loss: 330.2021\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 332.6454 - val_loss: 310.2816\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 317.7251 - val_loss: 287.8795\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 260.5427 - val_loss: 274.2945\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 258.2759 - val_loss: 256.6334\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 240.0303 - val_loss: 245.9407\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 243.3832 - val_loss: 234.2451\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 221.4188 - val_loss: 227.6591\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 237.3133 - val_loss: 219.0681\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 175.4354 - val_loss: 214.6353\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 195.0587 - val_loss: 204.3291\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 207.4125 - val_loss: 199.6504\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 200.9227 - val_loss: 196.2697\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 179.2510 - val_loss: 187.8818\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 174.7909 - val_loss: 182.4737\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 175.9471 - val_loss: 177.7192\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 165.3732 - val_loss: 172.8888\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 181.1706 - val_loss: 169.1409\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 168.1819 - val_loss: 166.6572\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 157.9922 - val_loss: 165.5471\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 176.9516 - val_loss: 163.0840\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 165.0191 - val_loss: 161.6796\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 171.4092 - val_loss: 159.1608\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 141.1278 - val_loss: 156.7715\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 150.6248 - val_loss: 157.3923\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 161.5959 - val_loss: 152.3450\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 144.6469 - val_loss: 150.1081\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 126.9915 - val_loss: 148.9838\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 145.1059 - val_loss: 150.4254\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 135.8692 - val_loss: 145.4736\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 130.6722 - val_loss: 145.3450\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 147.9484 - val_loss: 142.9884\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 136.7814 - val_loss: 143.3584\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 144.2617 - val_loss: 140.8148\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 140.4105 - val_loss: 140.6085\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 139.5230 - val_loss: 139.6821\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 124.1559 - val_loss: 137.5108\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 135.0304 - val_loss: 136.8189\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 115.6658 - val_loss: 135.8275\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 135.2754 - val_loss: 135.3081\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 125.4222 - val_loss: 132.9550\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 124.6424 - val_loss: 131.6082\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 120.4906 - val_loss: 129.7060\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 111.1348 - val_loss: 129.6862\n",
            "\n",
            "\n",
            "Mean Squared Error for Training Model #  20  :  116.88044895972506\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  21 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 15ms/step - loss: 326585.1489 - val_loss: 239249.9688\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 213527.2445 - val_loss: 150468.8438\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 132567.1236 - val_loss: 88547.1484\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 76352.3176 - val_loss: 46728.9023\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 38098.2569 - val_loss: 20243.9590\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 15053.2588 - val_loss: 5747.4038\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 3884.4312 - val_loss: 1259.9846\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1153.1231 - val_loss: 1024.6782\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1066.4395 - val_loss: 905.6983\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1012.2128 - val_loss: 814.1748\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 823.9986 - val_loss: 744.7579\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 731.5477 - val_loss: 684.4841\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 721.6007 - val_loss: 629.6067\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 663.0049 - val_loss: 584.3506\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 569.3731 - val_loss: 542.9083\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 501.3064 - val_loss: 507.1519\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 533.1967 - val_loss: 473.9738\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 392.7662 - val_loss: 449.2527\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 423.6085 - val_loss: 424.7482\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 398.1062 - val_loss: 403.2703\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 396.5658 - val_loss: 383.1922\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 399.7931 - val_loss: 369.9326\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 351.8678 - val_loss: 352.8389\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 353.2691 - val_loss: 341.2509\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 326.2865 - val_loss: 328.6413\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 290.1672 - val_loss: 320.9325\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 276.0522 - val_loss: 310.4858\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 268.3547 - val_loss: 301.5523\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 268.7528 - val_loss: 293.2917\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 250.9451 - val_loss: 287.5952\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 261.6709 - val_loss: 280.0659\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 259.6525 - val_loss: 275.8232\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 246.8444 - val_loss: 269.8499\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 246.0844 - val_loss: 263.2140\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 227.0776 - val_loss: 259.6641\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 230.1132 - val_loss: 253.6868\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 226.0733 - val_loss: 250.4369\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 232.9843 - val_loss: 245.8039\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 197.9263 - val_loss: 242.7132\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 239.8331 - val_loss: 238.3540\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 209.6231 - val_loss: 234.1283\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 204.9397 - val_loss: 232.6387\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 194.6426 - val_loss: 228.7125\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 192.4700 - val_loss: 224.5129\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 196.1081 - val_loss: 223.0139\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 200.8853 - val_loss: 219.6207\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 199.3354 - val_loss: 216.9211\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 202.0844 - val_loss: 213.2682\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 179.0374 - val_loss: 211.5038\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 185.3403 - val_loss: 209.6629\n",
            "\n",
            "\n",
            "Mean Squared Error for Training Model #  21  :  180.970503626999\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  22 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 15ms/step - loss: 135989.8254 - val_loss: 84786.1875\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 72698.7470 - val_loss: 41572.7422\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 34174.4539 - val_loss: 18519.1055\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 15225.9124 - val_loss: 7242.2437\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 5583.9867 - val_loss: 2520.7759\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1936.6271 - val_loss: 932.9227\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 783.8153 - val_loss: 626.8029\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 570.7966 - val_loss: 567.7662\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 484.2624 - val_loss: 542.7345\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 500.6305 - val_loss: 530.2615\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 455.6712 - val_loss: 522.0666\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 474.7950 - val_loss: 515.4896\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 428.3375 - val_loss: 510.3586\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 458.0287 - val_loss: 504.1188\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 437.4741 - val_loss: 498.8698\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 391.8954 - val_loss: 493.7924\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 475.9053 - val_loss: 488.3465\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 435.8234 - val_loss: 483.9038\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 446.8821 - val_loss: 478.3248\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 400.0225 - val_loss: 473.7576\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 443.2083 - val_loss: 469.2246\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 396.1512 - val_loss: 463.7221\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 385.4981 - val_loss: 459.4455\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 376.2370 - val_loss: 454.2810\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 386.4916 - val_loss: 449.5064\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 366.4078 - val_loss: 445.1093\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 378.0981 - val_loss: 440.3666\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 362.4792 - val_loss: 435.5420\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 360.4157 - val_loss: 431.2901\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 390.2635 - val_loss: 426.5516\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 378.7977 - val_loss: 422.8492\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 342.2132 - val_loss: 418.2949\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 398.6456 - val_loss: 413.6618\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 375.5168 - val_loss: 409.7662\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 353.3495 - val_loss: 405.5893\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 345.7800 - val_loss: 401.7993\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 359.0226 - val_loss: 398.2340\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 336.6999 - val_loss: 393.8662\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 324.7140 - val_loss: 390.2203\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 326.3398 - val_loss: 386.7502\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 323.3193 - val_loss: 382.7307\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 314.7156 - val_loss: 379.6989\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 311.6447 - val_loss: 376.5606\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 341.2632 - val_loss: 372.1905\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 304.2258 - val_loss: 370.1843\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 328.3603 - val_loss: 365.6120\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 315.1245 - val_loss: 362.5486\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 311.2596 - val_loss: 360.6056\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 300.9780 - val_loss: 356.3670\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 319.8529 - val_loss: 353.9800\n",
            "\n",
            "\n",
            "Mean Squared Error for Training Model #  22  :  283.0492091612461\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  23 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 13ms/step - loss: 112397.3474 - val_loss: 37955.0195\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 26981.8089 - val_loss: 4632.5376\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2940.9542 - val_loss: 1051.1119\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 991.0601 - val_loss: 812.7374\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 673.2845 - val_loss: 671.3417\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 640.7719 - val_loss: 609.1222\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 501.8298 - val_loss: 563.3309\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 478.9382 - val_loss: 528.5910\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 432.6539 - val_loss: 499.7021\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 374.9600 - val_loss: 477.7822\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 414.8798 - val_loss: 456.5045\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 370.8688 - val_loss: 436.2598\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 322.2300 - val_loss: 415.9295\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 355.3880 - val_loss: 384.8137\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 312.1683 - val_loss: 352.7643\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 268.3904 - val_loss: 320.2490\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 238.2395 - val_loss: 297.2991\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 225.5155 - val_loss: 279.6794\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 225.6176 - val_loss: 266.0172\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 200.6414 - val_loss: 255.2736\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 188.3126 - val_loss: 250.2908\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 209.9465 - val_loss: 238.5683\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 196.0204 - val_loss: 231.7107\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 179.2066 - val_loss: 228.8237\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 183.8666 - val_loss: 219.1969\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 183.0458 - val_loss: 213.7345\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 178.5871 - val_loss: 214.3368\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 192.1953 - val_loss: 203.6208\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 157.1712 - val_loss: 199.1976\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 157.2752 - val_loss: 194.3994\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 161.2131 - val_loss: 192.9847\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 159.1217 - val_loss: 185.9427\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 167.5039 - val_loss: 182.9049\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 153.0496 - val_loss: 182.5912\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 150.6403 - val_loss: 176.3166\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 139.3772 - val_loss: 174.0682\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 137.9493 - val_loss: 172.4383\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 120.2352 - val_loss: 167.8222\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 141.2123 - val_loss: 167.5366\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 145.7354 - val_loss: 163.1515\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 125.2887 - val_loss: 161.7200\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 156.6261 - val_loss: 162.8679\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 141.7042 - val_loss: 157.9081\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 120.5739 - val_loss: 158.4531\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 135.1334 - val_loss: 155.9361\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 131.3421 - val_loss: 153.2107\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 140.5578 - val_loss: 152.8255\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 131.5942 - val_loss: 153.7940\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 123.1517 - val_loss: 149.7737\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 122.9986 - val_loss: 147.2759\n",
            "\n",
            "\n",
            "Mean Squared Error for Training Model #  23  :  117.64801297538337\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  24 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 15ms/step - loss: 49067.7146 - val_loss: 26982.4512\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 22720.3225 - val_loss: 12706.7441\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 10818.4310 - val_loss: 5648.1196\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 4695.2769 - val_loss: 2388.5198\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 2100.3125 - val_loss: 1002.1868\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 890.1856 - val_loss: 495.6585\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 411.0506 - val_loss: 344.6647\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 334.4352 - val_loss: 309.9377\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 268.6221 - val_loss: 306.7728\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 243.9946 - val_loss: 307.0448\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 268.8000 - val_loss: 306.7056\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 255.1333 - val_loss: 306.2261\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 278.0579 - val_loss: 305.3472\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 273.8273 - val_loss: 305.0313\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 252.4611 - val_loss: 304.6216\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 274.0819 - val_loss: 303.6157\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 247.1248 - val_loss: 303.1584\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 295.8326 - val_loss: 302.6094\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 270.6360 - val_loss: 301.9460\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 280.6377 - val_loss: 301.3510\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 246.7026 - val_loss: 301.3432\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 268.6228 - val_loss: 300.5332\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 261.8494 - val_loss: 299.9258\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 277.4284 - val_loss: 298.7668\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 265.9048 - val_loss: 298.3271\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 273.1749 - val_loss: 298.1592\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 236.6986 - val_loss: 298.2632\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 279.0102 - val_loss: 296.2980\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 237.8080 - val_loss: 296.0342\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 264.7849 - val_loss: 295.5164\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 252.6288 - val_loss: 294.5099\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 271.7132 - val_loss: 294.6270\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 262.2158 - val_loss: 293.5021\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 271.3481 - val_loss: 292.8041\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 252.7123 - val_loss: 292.7041\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 266.2309 - val_loss: 291.0779\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 219.0728 - val_loss: 291.1153\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 260.6010 - val_loss: 290.1578\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 239.9728 - val_loss: 289.0725\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 240.6799 - val_loss: 288.8164\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 266.0494 - val_loss: 287.0519\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 242.1551 - val_loss: 287.2040\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 255.7874 - val_loss: 285.9008\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 242.5884 - val_loss: 285.1814\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 242.4757 - val_loss: 284.2358\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 231.4283 - val_loss: 284.4315\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 245.7397 - val_loss: 282.7233\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 230.4202 - val_loss: 281.6634\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 227.1716 - val_loss: 281.1253\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 224.3254 - val_loss: 279.8011\n",
            "\n",
            "\n",
            "Mean Squared Error for Training Model #  24  :  219.5972716167381\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  25 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 13ms/step - loss: 316692.8419 - val_loss: 156114.9062\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 123586.9407 - val_loss: 51714.2578\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 41028.7826 - val_loss: 19359.2070\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 17775.1901 - val_loss: 11125.1348\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 10735.5798 - val_loss: 7109.6694\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 6982.2035 - val_loss: 4682.9917\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 4360.0248 - val_loss: 3174.1187\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 3083.3449 - val_loss: 2177.7246\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 2263.0707 - val_loss: 1515.9186\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1558.1525 - val_loss: 1083.6754\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1134.5780 - val_loss: 806.6567\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 817.0438 - val_loss: 627.4407\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 700.9860 - val_loss: 511.7531\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 587.2650 - val_loss: 439.7450\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 468.1849 - val_loss: 398.4701\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 437.9325 - val_loss: 371.7089\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 416.6870 - val_loss: 354.3964\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 393.8121 - val_loss: 343.5536\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 373.3508 - val_loss: 336.0667\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 375.1333 - val_loss: 330.2804\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 369.5782 - val_loss: 325.4127\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 334.4987 - val_loss: 322.3186\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 360.3307 - val_loss: 319.5758\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 337.2961 - val_loss: 316.5349\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 371.2831 - val_loss: 313.2670\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 323.2229 - val_loss: 310.7211\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 351.2270 - val_loss: 307.6405\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 290.3552 - val_loss: 306.6229\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 341.1266 - val_loss: 303.1880\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 354.8602 - val_loss: 300.3272\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 320.5839 - val_loss: 298.3165\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 311.3063 - val_loss: 296.3205\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 312.3170 - val_loss: 293.5088\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 324.0777 - val_loss: 291.3700\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 322.4241 - val_loss: 289.0865\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 309.1745 - val_loss: 287.6410\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 308.5332 - val_loss: 285.3022\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 294.1988 - val_loss: 283.0445\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 292.3573 - val_loss: 280.5962\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 319.1884 - val_loss: 278.9473\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 283.0584 - val_loss: 276.6019\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 290.6807 - val_loss: 275.0544\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 276.7973 - val_loss: 273.6029\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 274.4730 - val_loss: 271.3999\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 276.2968 - val_loss: 270.0168\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 275.7630 - val_loss: 267.5612\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 264.7967 - val_loss: 266.1391\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 290.6914 - val_loss: 264.5508\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 297.2369 - val_loss: 262.6945\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 303.1936 - val_loss: 261.2559\n",
            "\n",
            "\n",
            "Mean Squared Error for Training Model #  25  :  263.4846864784761\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  26 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 14ms/step - loss: 1095.1899 - val_loss: 394.1891\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 432.2096 - val_loss: 337.0405\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 338.2622 - val_loss: 302.3541\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 299.1897 - val_loss: 280.4012\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 255.8031 - val_loss: 254.3476\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 231.0624 - val_loss: 219.5293\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 203.7314 - val_loss: 193.8493\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 195.2414 - val_loss: 172.0736\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 157.7060 - val_loss: 160.4200\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 151.0943 - val_loss: 138.4083\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 143.1124 - val_loss: 130.7586\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 114.9273 - val_loss: 129.5464\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 119.4974 - val_loss: 123.0713\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 118.9018 - val_loss: 121.4416\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 117.7806 - val_loss: 120.9063\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 112.3764 - val_loss: 129.1572\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 108.1620 - val_loss: 117.4161\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 98.2909 - val_loss: 115.9901\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 102.5278 - val_loss: 115.2326\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 103.8133 - val_loss: 115.7475\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 107.5393 - val_loss: 114.2330\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 95.5605 - val_loss: 112.0960\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 93.9258 - val_loss: 111.9582\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 93.3527 - val_loss: 115.3992\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 94.5142 - val_loss: 110.6366\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 99.2762 - val_loss: 109.3373\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 92.0410 - val_loss: 109.1824\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 99.3177 - val_loss: 109.0436\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 101.5026 - val_loss: 108.8750\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 99.7143 - val_loss: 107.3907\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 101.4964 - val_loss: 107.7017\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 89.3841 - val_loss: 107.6509\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 95.4389 - val_loss: 104.9935\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 88.5356 - val_loss: 105.6099\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 92.1274 - val_loss: 105.1602\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 93.8336 - val_loss: 103.7309\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 102.9084 - val_loss: 102.4053\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 77.1636 - val_loss: 102.7127\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 95.0540 - val_loss: 101.8239\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 86.5563 - val_loss: 100.7320\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 88.2565 - val_loss: 101.2831\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 91.5877 - val_loss: 101.0438\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 91.4730 - val_loss: 100.0291\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 78.3162 - val_loss: 100.5021\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 81.7361 - val_loss: 102.7679\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 86.4665 - val_loss: 110.8556\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 90.1716 - val_loss: 103.9014\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 98.4981 - val_loss: 98.1944\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 86.2401 - val_loss: 97.3608\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 85.6232 - val_loss: 97.2898\n",
            "\n",
            "\n",
            "Mean Squared Error for Training Model #  26  :  90.9422152456378\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  27 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 14ms/step - loss: 18143.1085 - val_loss: 6710.7700\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 4061.2562 - val_loss: 1654.4792\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1180.3711 - val_loss: 907.3433\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 817.3784 - val_loss: 801.3079\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 730.4770 - val_loss: 715.8050\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 664.0401 - val_loss: 645.1927\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 636.1864 - val_loss: 582.7021\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 501.1647 - val_loss: 531.4841\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 438.2847 - val_loss: 480.8500\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 414.5774 - val_loss: 435.1154\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 367.6804 - val_loss: 400.0180\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 378.0630 - val_loss: 369.7658\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 311.8758 - val_loss: 345.6305\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 288.2577 - val_loss: 325.6860\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 277.4232 - val_loss: 306.8858\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 274.3905 - val_loss: 292.8134\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 247.6946 - val_loss: 283.6147\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 243.0976 - val_loss: 272.5638\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 235.5291 - val_loss: 264.9885\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 226.9956 - val_loss: 256.4117\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 241.5552 - val_loss: 251.4880\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 221.6484 - val_loss: 246.3268\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 228.9310 - val_loss: 240.9669\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 226.0954 - val_loss: 237.0012\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 218.3491 - val_loss: 233.3884\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 222.9467 - val_loss: 229.2668\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 217.2900 - val_loss: 226.2340\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 207.0882 - val_loss: 222.8114\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 194.2400 - val_loss: 219.3639\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 197.5131 - val_loss: 217.7973\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 205.6588 - val_loss: 215.6755\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 218.7532 - val_loss: 212.6345\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 200.4767 - val_loss: 210.2187\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 188.9945 - val_loss: 208.5432\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 189.3783 - val_loss: 206.3722\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 178.6764 - val_loss: 204.2915\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 167.9756 - val_loss: 203.4223\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 189.0788 - val_loss: 201.0385\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 169.0028 - val_loss: 199.1818\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 182.4768 - val_loss: 198.2901\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 176.3188 - val_loss: 196.5423\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 174.2956 - val_loss: 195.5397\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 175.3743 - val_loss: 193.5601\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 170.5406 - val_loss: 192.2245\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 186.3245 - val_loss: 191.5691\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 162.5663 - val_loss: 190.6603\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 169.7839 - val_loss: 188.9268\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 173.8936 - val_loss: 188.1873\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 177.5664 - val_loss: 188.1578\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 175.3596 - val_loss: 185.6717\n",
            "\n",
            "\n",
            "Mean Squared Error for Training Model #  27  :  168.27265282201594\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  28 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 14ms/step - loss: 207131.8713 - val_loss: 83565.7344\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 64554.4724 - val_loss: 21755.9023\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 16119.0967 - val_loss: 4829.1577\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 4340.3640 - val_loss: 2661.4946\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2962.9750 - val_loss: 2642.5173\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2774.5821 - val_loss: 2451.4907\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2587.9934 - val_loss: 2295.3481\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2678.1933 - val_loss: 2166.4294\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2401.0750 - val_loss: 2048.8223\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 2384.2506 - val_loss: 1931.8734\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2106.1340 - val_loss: 1804.9880\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1784.2403 - val_loss: 1613.0419\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1551.5828 - val_loss: 1137.0398\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1247.4239 - val_loss: 774.5068\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 832.4561 - val_loss: 618.0151\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 656.1039 - val_loss: 531.1529\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 578.1093 - val_loss: 483.0306\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 530.4970 - val_loss: 452.8394\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 493.5915 - val_loss: 411.6735\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 407.3937 - val_loss: 392.4675\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 408.2206 - val_loss: 364.3344\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 390.9736 - val_loss: 345.2948\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 358.7438 - val_loss: 327.6624\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 357.3722 - val_loss: 311.9684\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 307.7811 - val_loss: 297.5599\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 298.7336 - val_loss: 287.0483\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 274.6788 - val_loss: 278.5904\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 287.1267 - val_loss: 268.6973\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 289.2154 - val_loss: 263.7376\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 235.6211 - val_loss: 256.7740\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 231.5680 - val_loss: 249.2149\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 237.5778 - val_loss: 244.8103\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 227.5481 - val_loss: 243.8857\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 229.4525 - val_loss: 235.9931\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 195.8218 - val_loss: 232.0383\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 221.3494 - val_loss: 230.1993\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 220.5509 - val_loss: 229.0684\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 197.3061 - val_loss: 221.9759\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 198.3043 - val_loss: 219.5810\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 181.5207 - val_loss: 219.7908\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 200.4318 - val_loss: 215.5814\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 198.3130 - val_loss: 214.5252\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 186.3516 - val_loss: 208.6766\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 198.9626 - val_loss: 207.4014\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 185.3748 - val_loss: 207.2396\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 176.2146 - val_loss: 204.7912\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 165.3281 - val_loss: 204.0935\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 169.5713 - val_loss: 201.2678\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 179.9058 - val_loss: 205.0154\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 184.1258 - val_loss: 198.9207\n",
            "\n",
            "\n",
            "Mean Squared Error for Training Model #  28  :  158.28660708866843\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  29 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 14ms/step - loss: 167716.9982 - val_loss: 104368.1953\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 88895.6806 - val_loss: 46430.7891\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 37878.0826 - val_loss: 13121.5693\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 8462.7662 - val_loss: 2298.2542\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2154.5352 - val_loss: 2247.9648\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2025.5747 - val_loss: 1806.5947\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1650.1512 - val_loss: 1632.7634\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1591.4765 - val_loss: 1496.1886\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 1363.2419 - val_loss: 1375.8284\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1353.9098 - val_loss: 1262.8116\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1084.0451 - val_loss: 1161.2584\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1082.9546 - val_loss: 1070.3218\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1022.8651 - val_loss: 985.9875\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 883.7584 - val_loss: 911.0345\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 812.2912 - val_loss: 843.9611\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 774.7683 - val_loss: 779.8360\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 767.2068 - val_loss: 722.7896\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 691.4360 - val_loss: 672.9591\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 608.6108 - val_loss: 627.3785\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 590.4781 - val_loss: 586.2269\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 613.0608 - val_loss: 549.2593\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 506.4656 - val_loss: 515.3142\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 486.0440 - val_loss: 484.8641\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 486.5054 - val_loss: 456.7207\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 437.0765 - val_loss: 431.2592\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 400.1486 - val_loss: 408.8021\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 400.1405 - val_loss: 386.1086\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 398.9475 - val_loss: 366.2443\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 348.0356 - val_loss: 348.8464\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 356.6592 - val_loss: 332.5670\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 335.3760 - val_loss: 317.9535\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 325.4853 - val_loss: 304.3767\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 313.1683 - val_loss: 291.3351\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 299.7740 - val_loss: 280.2236\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 273.4321 - val_loss: 269.5905\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 302.8247 - val_loss: 259.4554\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 250.5445 - val_loss: 250.7552\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 253.0918 - val_loss: 242.8250\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 244.7448 - val_loss: 235.2764\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 218.5988 - val_loss: 228.5907\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 244.2800 - val_loss: 221.3948\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 228.3787 - val_loss: 215.2565\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 215.3237 - val_loss: 209.6055\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 203.5854 - val_loss: 204.5293\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 223.6012 - val_loss: 199.5143\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 205.0720 - val_loss: 194.8568\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 196.1415 - val_loss: 190.7155\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 217.5100 - val_loss: 186.2254\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 195.0733 - val_loss: 182.7845\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 183.0024 - val_loss: 179.1239\n",
            "\n",
            "\n",
            "Mean Squared Error for Training Model #  29  :  173.16719354546154\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  30 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 15ms/step - loss: 602824.5699 - val_loss: 436903.5938\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 398501.4743 - val_loss: 285206.5000\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 258865.0423 - val_loss: 191567.2031\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 175983.6994 - val_loss: 131998.2188\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 120501.9278 - val_loss: 93656.2422\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 88431.1907 - val_loss: 67682.6406\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 62907.5156 - val_loss: 49774.9844\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 44561.0402 - val_loss: 37069.8789\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 35383.7562 - val_loss: 27648.0117\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 25932.4717 - val_loss: 20749.9141\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 19118.3967 - val_loss: 15521.6426\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 13799.4858 - val_loss: 11589.0791\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 10582.5447 - val_loss: 8642.2764\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 8061.7146 - val_loss: 6458.2925\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 5444.2283 - val_loss: 4886.3823\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 4556.5422 - val_loss: 3728.8140\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 3299.0233 - val_loss: 2924.1377\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 2682.9392 - val_loss: 2361.3682\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1970.5193 - val_loss: 1961.9620\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1841.5385 - val_loss: 1674.1497\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1640.5789 - val_loss: 1465.2185\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1473.2931 - val_loss: 1313.0811\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1255.1043 - val_loss: 1197.9241\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1057.7800 - val_loss: 1105.1814\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1035.2942 - val_loss: 1025.2003\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1054.8372 - val_loss: 957.7265\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 958.2880 - val_loss: 900.1629\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 908.0180 - val_loss: 848.9832\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 786.9618 - val_loss: 805.3357\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 791.6233 - val_loss: 765.1487\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 825.7896 - val_loss: 729.1843\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 664.8973 - val_loss: 696.7901\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 669.0299 - val_loss: 666.5117\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 640.3011 - val_loss: 639.7889\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 562.9402 - val_loss: 615.4781\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 588.4024 - val_loss: 592.5594\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 551.9353 - val_loss: 571.6890\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 583.3957 - val_loss: 551.9771\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 562.6501 - val_loss: 534.5131\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 538.3015 - val_loss: 518.3052\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 542.2079 - val_loss: 502.8877\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 514.2006 - val_loss: 489.6685\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 489.3683 - val_loss: 476.3134\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 468.4433 - val_loss: 463.8327\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 408.3505 - val_loss: 453.6728\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 435.0306 - val_loss: 442.6561\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 445.9715 - val_loss: 432.0207\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 441.0997 - val_loss: 421.5464\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 421.3421 - val_loss: 408.0009\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 398.4443 - val_loss: 390.2724\n",
            "\n",
            "\n",
            "Mean Squared Error for Training Model #  30  :  401.531877544071\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  31 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 15ms/step - loss: 43824.0441 - val_loss: 17234.7734\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 12612.7112 - val_loss: 4193.8906\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 2849.9301 - val_loss: 905.2780\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 614.1613 - val_loss: 591.7933\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 503.8917 - val_loss: 572.8933\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 410.3330 - val_loss: 524.2924\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 422.0661 - val_loss: 502.2610\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 391.3626 - val_loss: 483.7000\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 362.6584 - val_loss: 466.9002\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 380.4360 - val_loss: 451.7568\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 350.4080 - val_loss: 438.0407\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 389.9496 - val_loss: 425.3036\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 325.6250 - val_loss: 414.1563\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 360.9566 - val_loss: 404.2326\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 352.8817 - val_loss: 393.9034\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 338.3936 - val_loss: 384.5322\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 325.3629 - val_loss: 375.8829\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 303.5907 - val_loss: 366.9137\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 315.2859 - val_loss: 359.0404\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 305.3168 - val_loss: 351.5027\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 307.8753 - val_loss: 344.1895\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 278.4965 - val_loss: 337.2994\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 266.8476 - val_loss: 331.4424\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 292.3326 - val_loss: 323.7029\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 272.3821 - val_loss: 317.7172\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 278.8738 - val_loss: 311.3507\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 268.8948 - val_loss: 305.4796\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 254.2695 - val_loss: 300.2144\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 252.6016 - val_loss: 293.9776\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 258.3707 - val_loss: 288.9396\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 245.5155 - val_loss: 284.6270\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 257.0705 - val_loss: 278.9756\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 247.6944 - val_loss: 275.0514\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 227.4660 - val_loss: 269.7550\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 217.5966 - val_loss: 265.7217\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 226.9502 - val_loss: 260.3564\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 217.3855 - val_loss: 256.8156\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 224.0010 - val_loss: 252.5430\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 212.3958 - val_loss: 249.0710\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 219.4120 - val_loss: 243.9457\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 216.8030 - val_loss: 241.3291\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 191.3754 - val_loss: 236.1547\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 201.8097 - val_loss: 234.7797\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 193.3898 - val_loss: 229.5741\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 209.7060 - val_loss: 225.7617\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 197.6116 - val_loss: 222.6781\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 200.7569 - val_loss: 219.4044\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 181.1863 - val_loss: 215.2347\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 181.5085 - val_loss: 213.4385\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 208.6685 - val_loss: 208.9641\n",
            "\n",
            "\n",
            "Mean Squared Error for Training Model #  31  :  174.47289399995552\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  32 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 14ms/step - loss: 101065.8240 - val_loss: 39948.5977\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 31056.7977 - val_loss: 7913.0933\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 5584.8090 - val_loss: 2438.7134\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 2213.3540 - val_loss: 2085.1436\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2039.8268 - val_loss: 1527.1929\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 1525.9953 - val_loss: 1182.3751\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1216.5153 - val_loss: 966.6719\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1003.7272 - val_loss: 822.7095\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 774.6337 - val_loss: 719.5087\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 675.8285 - val_loss: 642.8301\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 601.9257 - val_loss: 579.1820\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 510.9613 - val_loss: 529.0156\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 498.5017 - val_loss: 472.6011\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 452.0993 - val_loss: 431.9328\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 416.4756 - val_loss: 397.6019\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 409.9619 - val_loss: 367.3553\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 363.4860 - val_loss: 348.7993\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 318.8101 - val_loss: 326.1550\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 302.2392 - val_loss: 311.9958\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 312.1007 - val_loss: 297.5062\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 326.4834 - val_loss: 287.5994\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 285.3387 - val_loss: 273.8731\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 285.2649 - val_loss: 265.4540\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 284.5229 - val_loss: 253.9009\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 253.6854 - val_loss: 244.7832\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 244.9828 - val_loss: 235.4268\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 236.1469 - val_loss: 227.3738\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 225.5597 - val_loss: 218.5049\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 217.8554 - val_loss: 213.4805\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 214.2784 - val_loss: 205.1523\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 231.2610 - val_loss: 200.9285\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 214.2068 - val_loss: 191.2788\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 190.0328 - val_loss: 187.0558\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 218.1330 - val_loss: 180.4430\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 204.1680 - val_loss: 175.0367\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 192.9595 - val_loss: 170.6712\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 170.6764 - val_loss: 163.6674\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 151.0095 - val_loss: 159.4559\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 162.0040 - val_loss: 154.9240\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 158.1823 - val_loss: 151.1884\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 151.8340 - val_loss: 147.5497\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 171.4576 - val_loss: 144.4877\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 150.0668 - val_loss: 140.8458\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 142.2165 - val_loss: 137.9758\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 143.4729 - val_loss: 134.9727\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 151.9768 - val_loss: 134.6301\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 139.6651 - val_loss: 130.3346\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 146.0760 - val_loss: 126.8696\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 141.3653 - val_loss: 124.9680\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 145.9706 - val_loss: 123.4299\n",
            "\n",
            "\n",
            "Mean Squared Error for Training Model #  32  :  139.31198273081526\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  33 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 15ms/step - loss: 59563.0666 - val_loss: 38018.1289\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 33292.8735 - val_loss: 21083.3301\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 18716.1682 - val_loss: 12542.9404\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 10729.1973 - val_loss: 7823.8066\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 6762.4677 - val_loss: 5115.0444\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 4567.4703 - val_loss: 3487.6260\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 3189.2202 - val_loss: 2458.0271\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2178.6666 - val_loss: 1782.9064\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1548.4634 - val_loss: 1345.9591\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1230.9006 - val_loss: 1059.3973\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 962.5097 - val_loss: 874.9666\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 747.0764 - val_loss: 763.3621\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 705.6845 - val_loss: 694.1390\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 618.2189 - val_loss: 657.5290\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 526.7928 - val_loss: 634.6163\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 612.9691 - val_loss: 621.0473\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 591.8376 - val_loss: 612.4908\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 568.6826 - val_loss: 607.5099\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 559.2193 - val_loss: 603.4676\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 591.9551 - val_loss: 600.0704\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 537.7458 - val_loss: 597.2574\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 567.9653 - val_loss: 594.8749\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 551.4190 - val_loss: 592.4236\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 608.7870 - val_loss: 589.7766\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 576.6084 - val_loss: 587.5072\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 500.7024 - val_loss: 585.5943\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 582.5168 - val_loss: 583.5088\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 554.8696 - val_loss: 580.9905\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 533.8402 - val_loss: 578.7584\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 564.0108 - val_loss: 576.3686\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 527.9941 - val_loss: 574.2784\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 541.3482 - val_loss: 571.8915\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 563.5755 - val_loss: 569.3368\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 539.6506 - val_loss: 567.2960\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 561.5819 - val_loss: 564.9468\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 536.2742 - val_loss: 562.6434\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 533.5156 - val_loss: 560.2885\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 519.1236 - val_loss: 558.3517\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 512.8612 - val_loss: 556.4538\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 522.3756 - val_loss: 554.4296\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 493.3453 - val_loss: 552.4382\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 463.7172 - val_loss: 550.6162\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 485.7889 - val_loss: 548.4945\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 500.1631 - val_loss: 546.5105\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 507.7373 - val_loss: 544.8021\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 530.2573 - val_loss: 542.8398\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 488.3468 - val_loss: 540.9659\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 461.6669 - val_loss: 538.6815\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 459.1121 - val_loss: 536.6395\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 470.7897 - val_loss: 534.6124\n",
            "\n",
            "\n",
            "Mean Squared Error for Training Model #  33  :  489.7625206756983\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  34 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 15ms/step - loss: 49193.3529 - val_loss: 22284.7930\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 17924.7495 - val_loss: 5986.3687\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 4728.9982 - val_loss: 1654.3131\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 1511.9897 - val_loss: 1625.8381\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1570.2497 - val_loss: 1250.4938\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1127.6557 - val_loss: 1135.7267\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1130.7729 - val_loss: 1050.5033\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1073.3255 - val_loss: 969.6522\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 817.7559 - val_loss: 903.3859\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 903.7789 - val_loss: 838.8975\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 815.9688 - val_loss: 792.7784\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 756.6771 - val_loss: 741.1450\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 735.8518 - val_loss: 698.1951\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 578.0928 - val_loss: 662.0057\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 550.0776 - val_loss: 618.6389\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 522.0988 - val_loss: 582.2897\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 491.5533 - val_loss: 534.4474\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 460.2111 - val_loss: 499.0703\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 447.5643 - val_loss: 452.3409\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 397.7409 - val_loss: 416.2644\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 372.1917 - val_loss: 398.6380\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 357.2076 - val_loss: 380.7887\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 359.2565 - val_loss: 369.3782\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 336.9319 - val_loss: 360.9184\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 320.5090 - val_loss: 344.9194\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 312.3561 - val_loss: 337.7693\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 297.0073 - val_loss: 330.9873\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 287.5786 - val_loss: 321.9503\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 290.8590 - val_loss: 307.3487\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 269.1694 - val_loss: 305.4328\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 271.9218 - val_loss: 289.8679\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 269.0287 - val_loss: 281.0905\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 255.3594 - val_loss: 267.8597\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 242.9181 - val_loss: 259.9244\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 254.3932 - val_loss: 260.1638\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 238.9828 - val_loss: 247.2366\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 208.1178 - val_loss: 241.6977\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 204.8716 - val_loss: 235.5927\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 204.7869 - val_loss: 233.0410\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 193.8293 - val_loss: 225.9765\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 207.5746 - val_loss: 222.3984\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 191.1657 - val_loss: 216.9974\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 188.5326 - val_loss: 215.5096\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 198.5148 - val_loss: 208.0059\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 191.6464 - val_loss: 208.4124\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 174.7880 - val_loss: 200.2389\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 178.5667 - val_loss: 198.6185\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 165.0082 - val_loss: 196.5835\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 172.7531 - val_loss: 190.0087\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 165.9674 - val_loss: 196.8866\n",
            "\n",
            "\n",
            "Mean Squared Error for Training Model #  34  :  192.9873770463558\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  35 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 13ms/step - loss: 6627.3875 - val_loss: 1470.7667\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1268.5528 - val_loss: 1474.8511\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 1130.5256 - val_loss: 1108.4193\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 992.9838 - val_loss: 964.9907\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 766.6197 - val_loss: 859.0421\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 673.0717 - val_loss: 761.5764\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 621.8027 - val_loss: 691.5333\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 596.6922 - val_loss: 627.1649\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 534.8228 - val_loss: 591.1259\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 526.1452 - val_loss: 534.5527\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 449.9511 - val_loss: 497.1908\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 415.0910 - val_loss: 462.1831\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 401.1140 - val_loss: 433.1482\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 381.4370 - val_loss: 405.1825\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 330.3546 - val_loss: 378.6952\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 324.6275 - val_loss: 358.4173\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 314.9050 - val_loss: 338.6657\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 328.6185 - val_loss: 319.2692\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 300.2016 - val_loss: 305.8546\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 255.2196 - val_loss: 292.2166\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 251.9030 - val_loss: 281.4942\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 231.7199 - val_loss: 270.9434\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 255.2220 - val_loss: 259.7910\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 212.9171 - val_loss: 253.1269\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 234.2397 - val_loss: 246.4027\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 201.0849 - val_loss: 237.9593\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 214.1351 - val_loss: 232.3567\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 192.2014 - val_loss: 226.7956\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 214.5455 - val_loss: 219.3189\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 185.5485 - val_loss: 218.5120\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 189.9225 - val_loss: 210.6305\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 196.7640 - val_loss: 207.2702\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 162.2394 - val_loss: 203.3325\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 179.7158 - val_loss: 202.3056\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 170.1938 - val_loss: 196.7790\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 159.9197 - val_loss: 194.7478\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 179.8210 - val_loss: 190.5266\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 160.4025 - val_loss: 193.8495\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 169.7493 - val_loss: 187.6388\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 164.8411 - val_loss: 182.8564\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 158.8803 - val_loss: 180.4040\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 162.6591 - val_loss: 180.0672\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 149.6615 - val_loss: 175.8601\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 151.6075 - val_loss: 176.3244\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 157.2115 - val_loss: 174.5440\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 153.8692 - val_loss: 169.0164\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 148.9566 - val_loss: 174.8221\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 143.7987 - val_loss: 165.2732\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 148.3570 - val_loss: 165.2925\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 137.3242 - val_loss: 161.5610\n",
            "\n",
            "\n",
            "Mean Squared Error for Training Model #  35  :  139.86021808196634\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  36 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 15ms/step - loss: 2263.2264 - val_loss: 906.6413\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 732.6761 - val_loss: 596.5274\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 502.4550 - val_loss: 473.1684\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 384.6922 - val_loss: 421.0014\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 396.1771 - val_loss: 375.5444\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 322.5399 - val_loss: 348.6500\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 301.6671 - val_loss: 323.5740\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 288.3997 - val_loss: 303.5820\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 261.1076 - val_loss: 285.2717\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 261.2193 - val_loss: 269.5058\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 240.4575 - val_loss: 257.3648\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 202.8699 - val_loss: 251.7764\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 229.8298 - val_loss: 237.2572\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 204.5111 - val_loss: 230.4963\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 191.2400 - val_loss: 216.4785\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 201.9950 - val_loss: 207.7154\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 195.1750 - val_loss: 195.9344\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 154.1830 - val_loss: 187.0473\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 170.8156 - val_loss: 180.3944\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 153.4381 - val_loss: 174.2675\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 144.1202 - val_loss: 168.6506\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 129.2140 - val_loss: 164.3468\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 134.5288 - val_loss: 161.1797\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 134.7546 - val_loss: 154.6008\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 151.8371 - val_loss: 152.9349\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 130.1314 - val_loss: 149.7318\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 139.5014 - val_loss: 145.1670\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 124.6559 - val_loss: 143.4804\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 140.7124 - val_loss: 140.9427\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 116.7048 - val_loss: 137.5556\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 114.0269 - val_loss: 137.5512\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 127.3620 - val_loss: 133.0920\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 114.1594 - val_loss: 129.8511\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 108.6258 - val_loss: 129.1708\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 115.7246 - val_loss: 130.0029\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 113.3565 - val_loss: 123.8919\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 111.5602 - val_loss: 123.2363\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 101.1336 - val_loss: 119.6299\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 105.0313 - val_loss: 117.2555\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 99.0303 - val_loss: 115.1556\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 95.8526 - val_loss: 112.8491\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 94.9247 - val_loss: 111.7591\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 101.6412 - val_loss: 108.9424\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 88.9168 - val_loss: 107.6099\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 99.6088 - val_loss: 104.3483\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 89.7843 - val_loss: 109.5976\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 84.7610 - val_loss: 100.2349\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 82.4258 - val_loss: 99.1413\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 96.8699 - val_loss: 94.4361\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 75.5444 - val_loss: 94.3984\n",
            "\n",
            "\n",
            "Mean Squared Error for Training Model #  36  :  74.21225860114039\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  37 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 15ms/step - loss: 72730.6255 - val_loss: 27214.4004\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 16642.2839 - val_loss: 5286.7378\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 4189.5446 - val_loss: 4217.3325\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 4061.9321 - val_loss: 3478.9951\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 2861.4715 - val_loss: 3192.4680\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 2774.7954 - val_loss: 2821.9617\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2627.2693 - val_loss: 2507.3555\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2129.7267 - val_loss: 2222.2483\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1715.5180 - val_loss: 1963.6045\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1611.4050 - val_loss: 1711.8207\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1398.6790 - val_loss: 1492.6273\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1328.7295 - val_loss: 1300.7861\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 999.3120 - val_loss: 1144.3422\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 768.2873 - val_loss: 999.4464\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 865.8521 - val_loss: 878.6451\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 688.7645 - val_loss: 776.5593\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 655.7040 - val_loss: 689.4319\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 568.3933 - val_loss: 623.2505\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 502.3587 - val_loss: 562.6674\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 517.3527 - val_loss: 514.0581\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 450.5804 - val_loss: 472.1066\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 411.6376 - val_loss: 438.2222\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 411.8842 - val_loss: 406.0216\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 370.6497 - val_loss: 383.7458\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 422.0394 - val_loss: 359.2991\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 375.2113 - val_loss: 339.8974\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 345.4290 - val_loss: 323.3062\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 296.2089 - val_loss: 310.2799\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 343.5905 - val_loss: 297.9211\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 296.5465 - val_loss: 285.8403\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 269.6431 - val_loss: 275.5632\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 270.7736 - val_loss: 267.2856\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 280.9960 - val_loss: 259.2558\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 238.9179 - val_loss: 253.5440\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 254.6967 - val_loss: 247.7567\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 267.0122 - val_loss: 242.2967\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 281.4019 - val_loss: 234.8627\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 246.2905 - val_loss: 231.8204\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 228.6393 - val_loss: 226.6868\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 223.9462 - val_loss: 222.9837\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 228.6808 - val_loss: 220.1533\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 221.3973 - val_loss: 215.5387\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 202.3624 - val_loss: 213.2539\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 223.4929 - val_loss: 208.4613\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 208.4688 - val_loss: 205.1657\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 219.5720 - val_loss: 201.4418\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 211.5508 - val_loss: 198.7869\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 206.4351 - val_loss: 194.1950\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 201.7020 - val_loss: 194.6814\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 175.3689 - val_loss: 189.2189\n",
            "\n",
            "\n",
            "Mean Squared Error for Training Model #  37  :  198.2695572013566\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  38 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 14ms/step - loss: 73718.0568 - val_loss: 39524.5703\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 32683.6711 - val_loss: 15690.0322\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 12611.7639 - val_loss: 4667.4399\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 3407.7901 - val_loss: 1160.2460\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1324.6629 - val_loss: 1112.7759\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1102.4063 - val_loss: 940.6995\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 891.1019 - val_loss: 844.7010\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 778.0396 - val_loss: 765.0933\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 758.5525 - val_loss: 685.9239\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 652.0676 - val_loss: 619.3887\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 596.9434 - val_loss: 560.3224\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 517.2495 - val_loss: 501.8694\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 457.6551 - val_loss: 454.7019\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 421.2411 - val_loss: 409.1409\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 370.4630 - val_loss: 370.6812\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 333.5351 - val_loss: 334.0060\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 308.1951 - val_loss: 304.6470\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 285.5938 - val_loss: 280.5687\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 263.4152 - val_loss: 260.4728\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 244.3502 - val_loss: 241.6053\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 220.9562 - val_loss: 227.6567\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 236.9043 - val_loss: 215.6900\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 182.7041 - val_loss: 203.0769\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 176.4766 - val_loss: 194.1258\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 174.5919 - val_loss: 185.6063\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 173.1278 - val_loss: 178.2818\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 161.1173 - val_loss: 171.7354\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 153.8476 - val_loss: 167.3279\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 142.9215 - val_loss: 164.3783\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 143.2535 - val_loss: 158.5387\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 145.4040 - val_loss: 156.1491\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 131.1188 - val_loss: 151.6066\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 131.8637 - val_loss: 149.2445\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 128.9980 - val_loss: 146.2731\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 125.8441 - val_loss: 144.6472\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 127.1626 - val_loss: 144.8101\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 135.5996 - val_loss: 139.8562\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 124.6746 - val_loss: 138.1251\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 131.4087 - val_loss: 137.0769\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 116.8852 - val_loss: 136.6459\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 116.6208 - val_loss: 134.0379\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 117.5252 - val_loss: 133.1663\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 112.5135 - val_loss: 132.1473\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 120.6782 - val_loss: 130.1342\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 117.8737 - val_loss: 129.2689\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 114.6342 - val_loss: 128.5016\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 104.5878 - val_loss: 130.0323\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 100.4426 - val_loss: 127.7470\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 107.4526 - val_loss: 126.3351\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 105.5954 - val_loss: 127.9898\n",
            "\n",
            "\n",
            "Mean Squared Error for Training Model #  38  :  126.61095018236655\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  39 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 13ms/step - loss: 66068.9784 - val_loss: 49451.4844\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 44531.4056 - val_loss: 32592.7559\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 28589.6422 - val_loss: 20526.6621\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 18236.3164 - val_loss: 12764.6660\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 11016.6535 - val_loss: 7771.1230\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 6829.0032 - val_loss: 4397.4946\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 3552.4294 - val_loss: 2263.8201\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1766.3526 - val_loss: 1134.9093\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 839.4322 - val_loss: 688.5613\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 687.4320 - val_loss: 530.7752\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 529.8206 - val_loss: 450.8860\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 430.6588 - val_loss: 393.6945\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 376.7736 - val_loss: 343.2096\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 312.2670 - val_loss: 308.6023\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 287.8521 - val_loss: 278.3356\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 258.4738 - val_loss: 256.5740\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 253.6034 - val_loss: 239.0001\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 254.2882 - val_loss: 226.8336\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 179.5835 - val_loss: 219.8340\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 212.9678 - val_loss: 211.5630\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 179.7364 - val_loss: 208.9256\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 194.4066 - val_loss: 203.6167\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 195.2228 - val_loss: 201.0276\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 184.8433 - val_loss: 201.1362\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 166.0734 - val_loss: 198.7789\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 167.0482 - val_loss: 195.6007\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 172.1415 - val_loss: 195.3530\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 186.8988 - val_loss: 192.8549\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 180.3461 - val_loss: 194.2997\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 184.3629 - val_loss: 192.4248\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 174.5052 - val_loss: 189.8358\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 161.1406 - val_loss: 188.0972\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 166.9012 - val_loss: 187.0716\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 166.2542 - val_loss: 185.4411\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 156.9393 - val_loss: 183.9088\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 179.0037 - val_loss: 183.0977\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 150.7994 - val_loss: 179.5385\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 160.7367 - val_loss: 178.6448\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 152.8732 - val_loss: 177.2828\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 162.5735 - val_loss: 173.4095\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 150.2606 - val_loss: 175.7176\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 150.3143 - val_loss: 173.4539\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 149.0555 - val_loss: 172.0984\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 154.7893 - val_loss: 169.5392\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 148.5391 - val_loss: 169.5509\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 148.3572 - val_loss: 168.6824\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 160.0351 - val_loss: 166.8532\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 145.1415 - val_loss: 164.6154\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 153.8838 - val_loss: 162.9418\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 144.1063 - val_loss: 163.1517\n",
            "\n",
            "\n",
            "Mean Squared Error for Training Model #  39  :  143.72317100137784\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  40 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 15ms/step - loss: 317707.2059 - val_loss: 198550.5312\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 171330.1222 - val_loss: 108103.6016\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 93475.1622 - val_loss: 59035.8789\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 50904.0076 - val_loss: 32546.4727\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 29073.8058 - val_loss: 18195.8750\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 16351.3769 - val_loss: 10349.5332\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 8811.0958 - val_loss: 6093.5732\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 5256.4148 - val_loss: 3706.6086\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 3418.2738 - val_loss: 2356.2529\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 2084.8842 - val_loss: 1594.7815\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1482.8021 - val_loss: 1148.5759\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 992.7395 - val_loss: 890.2321\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 885.3442 - val_loss: 725.4908\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 713.0919 - val_loss: 597.1371\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 528.1931 - val_loss: 520.8082\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 496.8723 - val_loss: 470.4731\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 434.3234 - val_loss: 428.7979\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 408.3941 - val_loss: 392.5112\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 366.4313 - val_loss: 367.1245\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 339.4830 - val_loss: 352.5032\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 325.1487 - val_loss: 337.6211\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 309.7332 - val_loss: 324.4023\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 303.0295 - val_loss: 316.2554\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 301.6055 - val_loss: 304.1627\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 265.3972 - val_loss: 287.1392\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 288.1629 - val_loss: 272.4782\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 246.0725 - val_loss: 263.9171\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 256.9716 - val_loss: 256.6540\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 226.5430 - val_loss: 248.4937\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 230.7325 - val_loss: 244.6262\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 214.1085 - val_loss: 242.4994\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 226.2783 - val_loss: 238.4281\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 234.3920 - val_loss: 232.9973\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 217.2234 - val_loss: 231.9697\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 209.2408 - val_loss: 229.3432\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 198.7825 - val_loss: 228.3254\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 217.4813 - val_loss: 223.2547\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 219.2735 - val_loss: 222.8334\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 195.4735 - val_loss: 221.8521\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 199.1123 - val_loss: 218.5193\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 207.4577 - val_loss: 217.1416\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 220.9819 - val_loss: 217.0779\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 208.8403 - val_loss: 217.9656\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 206.8182 - val_loss: 214.9241\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 205.5381 - val_loss: 212.5254\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 174.9236 - val_loss: 215.7291\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 192.9744 - val_loss: 212.0323\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 188.6262 - val_loss: 213.2023\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 186.1951 - val_loss: 209.8656\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 205.9561 - val_loss: 208.9467\n",
            "\n",
            "\n",
            "Mean Squared Error for Training Model #  40  :  192.00136261056747\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  41 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 23ms/step - loss: 1217.2983 - val_loss: 922.6871\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 926.3984 - val_loss: 727.0001\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 689.5185 - val_loss: 614.6960\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 537.7730 - val_loss: 532.9102\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 491.5589 - val_loss: 471.5144\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 449.4896 - val_loss: 413.0533\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 339.1646 - val_loss: 369.3974\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 321.8337 - val_loss: 327.9881\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 251.5393 - val_loss: 303.5621\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 269.2055 - val_loss: 289.5560\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 260.0316 - val_loss: 285.9981\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 254.2737 - val_loss: 277.3281\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 236.2889 - val_loss: 272.1120\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 225.7548 - val_loss: 268.4654\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 211.7877 - val_loss: 265.7426\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 212.8659 - val_loss: 260.1946\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 225.8622 - val_loss: 257.6154\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 210.6655 - val_loss: 256.6661\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 225.2414 - val_loss: 250.5914\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 218.7800 - val_loss: 247.5196\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 224.3236 - val_loss: 245.8224\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 206.4851 - val_loss: 241.4313\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 201.9583 - val_loss: 236.9433\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 214.9846 - val_loss: 234.3565\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 193.4151 - val_loss: 233.5750\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 210.3239 - val_loss: 230.1180\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 194.9267 - val_loss: 227.1705\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 188.0648 - val_loss: 220.7431\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 207.6802 - val_loss: 219.4605\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 186.3462 - val_loss: 217.5845\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 183.2709 - val_loss: 211.4377\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 206.1311 - val_loss: 208.8981\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 193.0344 - val_loss: 203.2461\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 187.1053 - val_loss: 201.8928\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 189.2601 - val_loss: 197.0227\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 163.1817 - val_loss: 196.4603\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 165.6237 - val_loss: 192.0970\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 184.5162 - val_loss: 190.1096\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 153.1724 - val_loss: 187.1188\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 170.8353 - val_loss: 183.9829\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 168.9911 - val_loss: 181.4131\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 150.4690 - val_loss: 177.9016\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 169.7690 - val_loss: 177.3501\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 144.5258 - val_loss: 175.6660\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 140.6961 - val_loss: 172.2182\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 157.6869 - val_loss: 173.7409\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 141.9970 - val_loss: 166.9476\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 147.7943 - val_loss: 165.3342\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 152.0975 - val_loss: 166.5491\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 147.8505 - val_loss: 161.8461\n",
            "\n",
            "\n",
            "Mean Squared Error for Training Model #  41  :  141.7089565717963\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  42 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 15ms/step - loss: 4591.8253 - val_loss: 1953.0267\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 2124.9297 - val_loss: 1287.5780\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1365.4874 - val_loss: 823.9124\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 974.0556 - val_loss: 657.8151\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 637.5589 - val_loss: 561.8930\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 564.9617 - val_loss: 503.3453\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 486.7988 - val_loss: 440.1761\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 423.3481 - val_loss: 405.4159\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 385.5037 - val_loss: 357.9860\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 337.6125 - val_loss: 327.3753\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 315.8880 - val_loss: 297.4469\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 288.9915 - val_loss: 281.2903\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 275.9546 - val_loss: 264.6839\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 246.8416 - val_loss: 244.5876\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 258.4451 - val_loss: 227.0224\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 217.9142 - val_loss: 227.8486\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 219.2160 - val_loss: 219.6645\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 238.7315 - val_loss: 198.6802\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 190.1834 - val_loss: 190.7179\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 178.7840 - val_loss: 183.0857\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 187.7367 - val_loss: 195.0764\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 186.4752 - val_loss: 188.0913\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 177.8411 - val_loss: 169.0292\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 158.7392 - val_loss: 161.2293\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 169.9328 - val_loss: 157.8222\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 166.2717 - val_loss: 154.1643\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 137.0233 - val_loss: 151.3687\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 142.3814 - val_loss: 147.8829\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 136.7029 - val_loss: 146.8315\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 153.3771 - val_loss: 142.8183\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 137.0266 - val_loss: 139.0750\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 139.6173 - val_loss: 138.0506\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 119.1459 - val_loss: 135.6025\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 120.8984 - val_loss: 133.7400\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 119.3502 - val_loss: 131.1139\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 127.6735 - val_loss: 131.7907\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 137.4421 - val_loss: 141.0997\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 123.1038 - val_loss: 129.5517\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 128.2394 - val_loss: 128.3290\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 126.8863 - val_loss: 129.3398\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 113.7984 - val_loss: 132.5538\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 106.7069 - val_loss: 123.7393\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 116.8457 - val_loss: 123.6906\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 108.6240 - val_loss: 129.0709\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 107.9692 - val_loss: 121.4755\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 119.8524 - val_loss: 118.3165\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 111.6587 - val_loss: 118.8953\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 109.1156 - val_loss: 118.9116\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 101.2030 - val_loss: 119.0056\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 104.3527 - val_loss: 117.7907\n",
            "\n",
            "\n",
            "Mean Squared Error for Training Model #  42  :  108.76805494532236\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  43 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 14ms/step - loss: 2675.0970 - val_loss: 1815.3206\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1446.6078 - val_loss: 1318.8748\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1046.7359 - val_loss: 1122.8328\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 930.8922 - val_loss: 979.9720\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 835.5726 - val_loss: 860.8522\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 757.9800 - val_loss: 772.0708\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 642.1718 - val_loss: 707.0389\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 606.1260 - val_loss: 645.3815\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 533.6514 - val_loss: 591.9200\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 573.1998 - val_loss: 566.0503\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 469.7768 - val_loss: 512.1165\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 384.4170 - val_loss: 478.6713\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 425.8855 - val_loss: 449.7430\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 378.0274 - val_loss: 426.2703\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 347.3222 - val_loss: 404.7144\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 349.5992 - val_loss: 384.8085\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 307.2345 - val_loss: 367.9527\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 287.4865 - val_loss: 350.4880\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 311.7331 - val_loss: 357.7318\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 299.5329 - val_loss: 323.7570\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 275.2704 - val_loss: 310.1199\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 245.8009 - val_loss: 294.2950\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 241.1085 - val_loss: 284.2758\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 251.8732 - val_loss: 272.6601\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 217.8743 - val_loss: 262.0240\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 231.0164 - val_loss: 252.0655\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 205.3623 - val_loss: 243.7892\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 195.9121 - val_loss: 234.5927\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 181.4985 - val_loss: 226.3369\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 183.8368 - val_loss: 218.5196\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 183.1919 - val_loss: 213.6730\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 175.3387 - val_loss: 207.9242\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 175.2475 - val_loss: 212.1736\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 166.3869 - val_loss: 197.2399\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 162.8401 - val_loss: 187.7361\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 161.3931 - val_loss: 179.8448\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 143.0676 - val_loss: 177.7204\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 144.4890 - val_loss: 172.5388\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 144.1336 - val_loss: 165.3247\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 146.3096 - val_loss: 165.0167\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 132.9774 - val_loss: 159.3068\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 142.0938 - val_loss: 161.5800\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 126.8203 - val_loss: 165.3730\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 131.8479 - val_loss: 149.7059\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 124.3308 - val_loss: 151.3604\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 128.1688 - val_loss: 145.1625\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 122.3537 - val_loss: 139.7355\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 122.1960 - val_loss: 137.4446\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 118.9123 - val_loss: 135.7875\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 121.4864 - val_loss: 139.7112\n",
            "\n",
            "\n",
            "Mean Squared Error for Training Model #  43  :  131.39006477195423\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  44 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 15ms/step - loss: 1610.0235 - val_loss: 770.0112\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 759.3874 - val_loss: 584.6971\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 558.8536 - val_loss: 522.7606\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 477.7201 - val_loss: 460.1450\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 388.5249 - val_loss: 419.6992\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 357.2202 - val_loss: 391.1543\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 346.9915 - val_loss: 366.9984\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 293.0519 - val_loss: 350.1643\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 297.9786 - val_loss: 335.4966\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 278.6522 - val_loss: 324.7544\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 291.7218 - val_loss: 314.0600\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 251.4826 - val_loss: 305.1656\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 236.4541 - val_loss: 297.7715\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 212.5356 - val_loss: 287.3008\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 236.8077 - val_loss: 279.5252\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 234.2906 - val_loss: 268.8973\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 241.1547 - val_loss: 259.7419\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 199.3691 - val_loss: 251.2932\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 199.4626 - val_loss: 245.4396\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 232.7650 - val_loss: 232.6808\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 189.8069 - val_loss: 222.3563\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 179.8869 - val_loss: 213.7983\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 171.1244 - val_loss: 205.2157\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 155.0121 - val_loss: 191.7638\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 162.1387 - val_loss: 181.8594\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 143.8578 - val_loss: 171.4367\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 128.9441 - val_loss: 160.7531\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 127.0174 - val_loss: 152.4900\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 125.3643 - val_loss: 145.3732\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 117.1489 - val_loss: 139.4427\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 113.0805 - val_loss: 134.4326\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 118.2679 - val_loss: 130.6333\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 109.3501 - val_loss: 128.6194\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 98.8211 - val_loss: 130.0715\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 110.1803 - val_loss: 125.8824\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 103.2434 - val_loss: 122.6478\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 96.2708 - val_loss: 122.3419\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 112.3876 - val_loss: 120.7924\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 103.8041 - val_loss: 120.0753\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 113.0878 - val_loss: 119.4089\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 108.8678 - val_loss: 122.0306\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 108.8486 - val_loss: 121.4550\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 108.0395 - val_loss: 123.5923\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 109.8465 - val_loss: 131.4811\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 107.5755 - val_loss: 121.1893\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 101.9771 - val_loss: 117.6676\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 99.1561 - val_loss: 117.6058\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 105.2146 - val_loss: 117.6176\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 97.6432 - val_loss: 119.0663\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 104.3759 - val_loss: 116.6125\n",
            "\n",
            "\n",
            "Mean Squared Error for Training Model #  44  :  111.02062456668145\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  45 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 14ms/step - loss: 1059.2374 - val_loss: 455.7621\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 431.1036 - val_loss: 302.1428\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 288.1771 - val_loss: 232.6491\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 236.4255 - val_loss: 206.1550\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 205.7087 - val_loss: 195.8037\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 197.0404 - val_loss: 179.1690\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 182.5824 - val_loss: 174.5442\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 210.4884 - val_loss: 165.4137\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 175.8091 - val_loss: 160.2803\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 174.7555 - val_loss: 159.3714\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 152.7198 - val_loss: 151.9100\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 142.2341 - val_loss: 148.0345\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 147.1682 - val_loss: 145.0614\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 150.5126 - val_loss: 146.8062\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 149.1927 - val_loss: 142.2436\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 145.7112 - val_loss: 154.5659\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 132.3332 - val_loss: 136.0551\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 133.6798 - val_loss: 134.4594\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 142.2393 - val_loss: 134.6675\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 127.5254 - val_loss: 132.7490\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 137.6292 - val_loss: 132.1396\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 124.9954 - val_loss: 133.0305\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 117.5092 - val_loss: 133.4336\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 138.1616 - val_loss: 133.3493\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 113.3733 - val_loss: 132.9609\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 129.0847 - val_loss: 132.0585\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 121.4479 - val_loss: 142.5197\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 137.3846 - val_loss: 131.1791\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 117.3290 - val_loss: 131.5288\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 119.7829 - val_loss: 137.0349\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 123.9529 - val_loss: 139.8784\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 120.6120 - val_loss: 134.5551\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 128.5344 - val_loss: 129.7771\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 119.5019 - val_loss: 133.2997\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 113.8961 - val_loss: 145.3450\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 129.6977 - val_loss: 132.5110\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 113.4094 - val_loss: 131.5319\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 109.1103 - val_loss: 129.3493\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 117.7438 - val_loss: 129.3830\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 113.6515 - val_loss: 129.2454\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 117.7542 - val_loss: 129.6314\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 111.4996 - val_loss: 128.2485\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 120.8252 - val_loss: 127.4958\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 114.7714 - val_loss: 127.3027\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 112.6527 - val_loss: 126.4460\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 108.2352 - val_loss: 129.8738\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 127.2096 - val_loss: 125.3513\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 116.3532 - val_loss: 130.6736\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 107.3390 - val_loss: 127.4753\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 110.3427 - val_loss: 127.0276\n",
            "\n",
            "\n",
            "Mean Squared Error for Training Model #  45  :  115.27817418821324\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  46 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 15ms/step - loss: 60913.2121 - val_loss: 13960.4307\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 7456.0396 - val_loss: 2709.1121\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2741.1836 - val_loss: 2968.9531\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2891.3680 - val_loss: 2403.8057\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2254.5935 - val_loss: 2314.6619\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2172.1835 - val_loss: 2150.1445\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2078.9702 - val_loss: 2029.5951\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2075.8910 - val_loss: 1916.8138\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1943.9854 - val_loss: 1814.6637\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1796.0449 - val_loss: 1714.9943\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1694.6980 - val_loss: 1630.1934\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1512.5771 - val_loss: 1540.2749\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1545.9748 - val_loss: 1464.3311\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1383.6761 - val_loss: 1391.4445\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1396.4204 - val_loss: 1314.4520\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1288.2211 - val_loss: 1249.2932\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1236.0285 - val_loss: 1201.5884\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1218.3705 - val_loss: 1134.8724\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1107.8396 - val_loss: 1081.9950\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1001.8404 - val_loss: 1032.7804\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 968.9484 - val_loss: 990.4555\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 991.2609 - val_loss: 941.8615\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 934.5650 - val_loss: 907.4213\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 904.8176 - val_loss: 859.0209\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 853.7682 - val_loss: 835.8911\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 803.0753 - val_loss: 791.3129\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 743.6163 - val_loss: 761.0673\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 728.4120 - val_loss: 729.9241\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 706.7833 - val_loss: 704.9998\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 690.0154 - val_loss: 672.3828\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 670.1172 - val_loss: 651.2217\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 608.0745 - val_loss: 621.9670\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 577.5890 - val_loss: 601.3247\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 599.3608 - val_loss: 580.5160\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 586.0961 - val_loss: 559.3860\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 521.6422 - val_loss: 539.3651\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 524.4839 - val_loss: 523.7938\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 518.9092 - val_loss: 502.6376\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 478.8349 - val_loss: 489.0535\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 485.9799 - val_loss: 471.9384\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 455.4785 - val_loss: 457.0749\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 424.6958 - val_loss: 444.8379\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 415.9634 - val_loss: 426.1348\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 420.3857 - val_loss: 415.1748\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 404.8809 - val_loss: 405.8080\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 345.7106 - val_loss: 389.5093\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 373.7708 - val_loss: 380.9535\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 341.5092 - val_loss: 369.1563\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 336.5117 - val_loss: 360.1402\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 353.2706 - val_loss: 350.0814\n",
            "\n",
            "\n",
            "Mean Squared Error for Training Model #  46  :  330.562750231138\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  47 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 15ms/step - loss: 20476.0978 - val_loss: 13091.0889\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 11065.4330 - val_loss: 6258.0610\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 5176.5268 - val_loss: 3111.1975\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2570.5625 - val_loss: 1722.0911\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1218.1782 - val_loss: 1037.0991\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 817.0246 - val_loss: 678.1989\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 527.3142 - val_loss: 522.7975\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 389.7737 - val_loss: 465.7682\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 357.3798 - val_loss: 447.0129\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 385.2303 - val_loss: 435.9686\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 374.8458 - val_loss: 426.3711\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 338.7181 - val_loss: 417.3456\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 327.3870 - val_loss: 407.7739\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 364.1846 - val_loss: 397.9494\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 307.7744 - val_loss: 387.6058\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 350.1914 - val_loss: 376.7570\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 315.2092 - val_loss: 366.1729\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 301.4415 - val_loss: 357.1420\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 283.6377 - val_loss: 351.3754\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 283.5057 - val_loss: 345.8405\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 277.5207 - val_loss: 340.5501\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 255.1487 - val_loss: 336.2688\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 269.8366 - val_loss: 330.2364\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 276.1429 - val_loss: 326.3803\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 302.1648 - val_loss: 322.0436\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 265.3908 - val_loss: 318.5690\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 245.4409 - val_loss: 315.3825\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 255.2251 - val_loss: 312.1917\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 266.0473 - val_loss: 308.8140\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 262.2724 - val_loss: 305.7961\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 232.7388 - val_loss: 302.2222\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 260.4066 - val_loss: 299.7550\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 276.3716 - val_loss: 297.2617\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 267.9869 - val_loss: 294.1464\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 248.8561 - val_loss: 291.8619\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 256.9961 - val_loss: 289.7596\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 239.4640 - val_loss: 287.1565\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 242.8846 - val_loss: 282.8752\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 220.6172 - val_loss: 280.4756\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 246.5189 - val_loss: 277.6767\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 211.3140 - val_loss: 275.2383\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 241.7267 - val_loss: 272.3389\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 226.3024 - val_loss: 270.2694\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 215.3669 - val_loss: 267.0888\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 214.8603 - val_loss: 265.1746\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 216.3630 - val_loss: 261.7749\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 207.0288 - val_loss: 259.4872\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 217.7944 - val_loss: 256.9308\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 207.7436 - val_loss: 253.5911\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 201.6813 - val_loss: 250.4299\n",
            "\n",
            "\n",
            "Mean Squared Error for Training Model #  47  :  212.31489024352987\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  48 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 14ms/step - loss: 77999.6043 - val_loss: 45151.8516\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 38263.2631 - val_loss: 20839.5371\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 18244.1159 - val_loss: 9083.3984\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 7498.1907 - val_loss: 3966.4885\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 3386.7921 - val_loss: 2399.4194\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 2239.6590 - val_loss: 1718.1852\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 1610.4884 - val_loss: 1299.5647\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1286.4542 - val_loss: 1068.5692\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1092.5201 - val_loss: 922.4410\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 904.3217 - val_loss: 808.0620\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 730.0388 - val_loss: 718.3350\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 713.5768 - val_loss: 641.0842\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 596.9452 - val_loss: 575.1448\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 530.1874 - val_loss: 519.3705\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 524.3468 - val_loss: 473.7205\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 458.3199 - val_loss: 438.3804\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 403.3751 - val_loss: 407.1735\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 397.7310 - val_loss: 384.0641\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 431.9118 - val_loss: 362.9050\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 378.9801 - val_loss: 350.6662\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 316.5848 - val_loss: 335.6620\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 303.8677 - val_loss: 327.1146\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 297.7705 - val_loss: 318.6149\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 305.1012 - val_loss: 312.2003\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 294.6260 - val_loss: 305.2995\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 263.6636 - val_loss: 301.5602\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 255.5585 - val_loss: 298.8701\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 280.8054 - val_loss: 294.0761\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 272.3277 - val_loss: 291.8138\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 313.4146 - val_loss: 291.3255\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 277.9802 - val_loss: 286.8702\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 261.2305 - val_loss: 287.6790\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 278.5017 - val_loss: 283.6395\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 286.4722 - val_loss: 282.6220\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 271.2206 - val_loss: 279.8806\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 272.1634 - val_loss: 278.4872\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 270.6518 - val_loss: 276.9250\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 266.9785 - val_loss: 275.5386\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 262.2995 - val_loss: 274.4231\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 244.4246 - val_loss: 273.0831\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 267.0920 - val_loss: 273.9195\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 252.8025 - val_loss: 270.8030\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 238.3720 - val_loss: 271.6954\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 249.6369 - val_loss: 268.8975\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 250.5157 - val_loss: 267.8348\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 256.2986 - val_loss: 266.8553\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 231.7409 - val_loss: 265.5446\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 255.0892 - val_loss: 265.2461\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 253.7896 - val_loss: 263.7163\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 242.5405 - val_loss: 262.6763\n",
            "\n",
            "\n",
            "Mean Squared Error for Training Model #  48  :  250.85885720679167\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  49 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 15ms/step - loss: 2915.1408 - val_loss: 1158.9917\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 853.0024 - val_loss: 357.2648\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 296.3487 - val_loss: 296.5738\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 289.8324 - val_loss: 293.3341\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 288.5756 - val_loss: 288.4783\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 278.6468 - val_loss: 287.1943\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 259.3597 - val_loss: 282.3217\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 256.5435 - val_loss: 279.2155\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 271.6072 - val_loss: 275.0682\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 282.2856 - val_loss: 270.5819\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 241.6694 - val_loss: 264.8123\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 250.0404 - val_loss: 252.3695\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 224.4110 - val_loss: 221.6709\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 206.1355 - val_loss: 199.9708\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 179.5169 - val_loss: 189.2228\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 186.6436 - val_loss: 181.5419\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 173.2323 - val_loss: 175.9316\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 163.5075 - val_loss: 171.4503\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 158.1021 - val_loss: 168.5216\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 145.6931 - val_loss: 162.8800\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 144.6878 - val_loss: 160.5106\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 135.5230 - val_loss: 157.8174\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 148.2160 - val_loss: 157.2144\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 133.3272 - val_loss: 153.5957\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 132.9792 - val_loss: 150.9804\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 135.6385 - val_loss: 148.6650\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 128.1061 - val_loss: 147.2586\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 141.8719 - val_loss: 146.1841\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 132.6706 - val_loss: 143.7605\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 128.4870 - val_loss: 141.6888\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 127.4202 - val_loss: 141.2069\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 129.0733 - val_loss: 139.5208\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 127.6410 - val_loss: 136.9982\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 119.8797 - val_loss: 137.9534\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 122.2734 - val_loss: 134.9935\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 121.3752 - val_loss: 134.8560\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 121.4430 - val_loss: 131.3057\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 127.5330 - val_loss: 130.8860\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 125.3431 - val_loss: 129.4851\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 110.2907 - val_loss: 127.7728\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 117.7172 - val_loss: 126.5088\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 114.2390 - val_loss: 125.4204\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 114.6074 - val_loss: 124.4169\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 108.0314 - val_loss: 123.0813\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 119.6182 - val_loss: 124.8660\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 114.1642 - val_loss: 121.3369\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 121.1672 - val_loss: 120.2827\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 101.0956 - val_loss: 120.6672\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 118.3578 - val_loss: 119.0122\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 100.8074 - val_loss: 117.2942\n",
            "\n",
            "\n",
            "Mean Squared Error for Training Model #  49  :  107.14272687498546\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  50 \n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 14ms/step - loss: 37869.5489 - val_loss: 23806.2793\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 21194.1999 - val_loss: 13528.9170\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 11868.3564 - val_loss: 7525.7627\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 6950.0159 - val_loss: 4260.2412\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 3634.8767 - val_loss: 2682.1719\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2284.1066 - val_loss: 1869.0758\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1665.8779 - val_loss: 1411.4182\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1163.9388 - val_loss: 1072.9399\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1022.8822 - val_loss: 804.2805\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 746.8166 - val_loss: 637.6266\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 676.4083 - val_loss: 542.6780\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 578.8539 - val_loss: 485.9022\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 422.9379 - val_loss: 448.7259\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 442.1233 - val_loss: 421.7000\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 431.2807 - val_loss: 403.7814\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 374.8123 - val_loss: 391.3197\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 369.4154 - val_loss: 380.3000\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 377.4938 - val_loss: 371.3359\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 363.8434 - val_loss: 363.1874\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 325.6783 - val_loss: 357.6934\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 338.0442 - val_loss: 348.5695\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 310.8727 - val_loss: 342.2400\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 299.2826 - val_loss: 337.1751\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 311.4457 - val_loss: 329.9307\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 289.3691 - val_loss: 325.0489\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 289.6258 - val_loss: 319.1982\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 286.4285 - val_loss: 313.1114\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 284.0220 - val_loss: 307.6676\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 293.8820 - val_loss: 303.6644\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 294.1923 - val_loss: 297.8835\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 269.9755 - val_loss: 293.5226\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 274.0329 - val_loss: 287.6559\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 266.7633 - val_loss: 279.5091\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 251.8586 - val_loss: 270.5700\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 261.2466 - val_loss: 262.4392\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 243.4243 - val_loss: 259.0542\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 251.3539 - val_loss: 254.6029\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 259.2213 - val_loss: 251.7800\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 247.2365 - val_loss: 252.5951\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 249.9736 - val_loss: 249.6078\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 226.0412 - val_loss: 248.2289\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 239.2612 - val_loss: 240.2281\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 251.5286 - val_loss: 235.1664\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 251.0938 - val_loss: 234.5849\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 225.4937 - val_loss: 232.4801\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 218.6906 - val_loss: 229.2654\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 208.4993 - val_loss: 225.4037\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 217.2597 - val_loss: 221.6425\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 188.6096 - val_loss: 217.3621\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 190.5973 - val_loss: 211.4825\n",
            "\n",
            "\n",
            "Mean Squared Error for Training Model #  50  :  207.19901198928324\n",
            "\n",
            "\n",
            "Total Execution Time :  0:04:17.758835\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlva1wS-S_cv",
        "outputId": "f4ff30b2-c2f6-412d-a0ce-0727e94b9be8"
      },
      "source": [
        "# Calculate the Mean of the MSE of 50 models\r\n",
        "mean_of_mse = stats.mean(list_of_mse)\r\n",
        "\r\n",
        "# Calculate the Standard Deviation of the MSE of 50 models\r\n",
        "std_of_mse = stats.stdev(list_of_mse)\r\n",
        "\r\n",
        "# Print the Mean and Standard Deviation of MSE of 50 models\r\n",
        "print('\\n\\nMean of the MSE of 50 Models : ' , str(mean_of_mse))\r\n",
        "print('Standard Deviation of MSE of 50 Models : ' , str(std_of_mse))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Mean of the MSE of 50 Models :  177.2722700901618\n",
            "Standard Deviation of MSE of 50 Models :  81.36343951255373\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eE91rF21TI__"
      },
      "source": [
        "# <font color = blue> END OF PART A </font>"
      ]
    }
  ]
}