{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PartC.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ghn3WIOzKELl"
      },
      "source": [
        "# Importing the neccessary libraries\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import statistics as stats\r\n",
        "import os\r\n",
        "import keras\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from sklearn import preprocessing\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import mean_squared_error\r\n",
        "from datetime import datetime\r\n",
        "\r\n",
        "# Forcing keras to use CPU.\r\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "id": "HzGihXDrKInQ",
        "outputId": "c214ed14-1512-43a9-eb17-e5dcf5ab3181"
      },
      "source": [
        "# Reading the Data and storing it in a dataframe\r\n",
        "\r\n",
        "df = pd.read_csv('https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0101EN/labs/data/concrete_data.csv')\r\n",
        "df.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Cement</th>\n",
              "      <th>Blast Furnace Slag</th>\n",
              "      <th>Fly Ash</th>\n",
              "      <th>Water</th>\n",
              "      <th>Superplasticizer</th>\n",
              "      <th>Coarse Aggregate</th>\n",
              "      <th>Fine Aggregate</th>\n",
              "      <th>Age</th>\n",
              "      <th>Strength</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>540.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>162.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1040.0</td>\n",
              "      <td>676.0</td>\n",
              "      <td>28</td>\n",
              "      <td>79.99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>540.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>162.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1055.0</td>\n",
              "      <td>676.0</td>\n",
              "      <td>28</td>\n",
              "      <td>61.89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>332.5</td>\n",
              "      <td>142.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>228.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>932.0</td>\n",
              "      <td>594.0</td>\n",
              "      <td>270</td>\n",
              "      <td>40.27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>332.5</td>\n",
              "      <td>142.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>228.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>932.0</td>\n",
              "      <td>594.0</td>\n",
              "      <td>365</td>\n",
              "      <td>41.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>198.6</td>\n",
              "      <td>132.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>192.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>978.4</td>\n",
              "      <td>825.5</td>\n",
              "      <td>360</td>\n",
              "      <td>44.30</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Cement  Blast Furnace Slag  Fly Ash  ...  Fine Aggregate  Age  Strength\n",
              "0   540.0                 0.0      0.0  ...           676.0   28     79.99\n",
              "1   540.0                 0.0      0.0  ...           676.0   28     61.89\n",
              "2   332.5               142.5      0.0  ...           594.0  270     40.27\n",
              "3   332.5               142.5      0.0  ...           594.0  365     41.05\n",
              "4   198.6               132.4      0.0  ...           825.5  360     44.30\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqTYkPvzKLTm",
        "outputId": "d3a46998-52d6-4764-eff0-9c239e90df41"
      },
      "source": [
        "print('\\nShape of dataframe : ',df.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Shape of dataframe :  (1030, 9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "r8niVpLzKMhh",
        "outputId": "d39d71d1-0767-436f-ae16-20d76fe3b475"
      },
      "source": [
        "# Summary of the dataset\r\n",
        "df.describe()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Cement</th>\n",
              "      <th>Blast Furnace Slag</th>\n",
              "      <th>Fly Ash</th>\n",
              "      <th>Water</th>\n",
              "      <th>Superplasticizer</th>\n",
              "      <th>Coarse Aggregate</th>\n",
              "      <th>Fine Aggregate</th>\n",
              "      <th>Age</th>\n",
              "      <th>Strength</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>281.167864</td>\n",
              "      <td>73.895825</td>\n",
              "      <td>54.188350</td>\n",
              "      <td>181.567282</td>\n",
              "      <td>6.204660</td>\n",
              "      <td>972.918932</td>\n",
              "      <td>773.580485</td>\n",
              "      <td>45.662136</td>\n",
              "      <td>35.817961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>104.506364</td>\n",
              "      <td>86.279342</td>\n",
              "      <td>63.997004</td>\n",
              "      <td>21.354219</td>\n",
              "      <td>5.973841</td>\n",
              "      <td>77.753954</td>\n",
              "      <td>80.175980</td>\n",
              "      <td>63.169912</td>\n",
              "      <td>16.705742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>102.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>121.800000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>801.000000</td>\n",
              "      <td>594.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.330000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>192.375000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>164.900000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>932.000000</td>\n",
              "      <td>730.950000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>23.710000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>272.900000</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>185.000000</td>\n",
              "      <td>6.400000</td>\n",
              "      <td>968.000000</td>\n",
              "      <td>779.500000</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>34.445000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>350.000000</td>\n",
              "      <td>142.950000</td>\n",
              "      <td>118.300000</td>\n",
              "      <td>192.000000</td>\n",
              "      <td>10.200000</td>\n",
              "      <td>1029.400000</td>\n",
              "      <td>824.000000</td>\n",
              "      <td>56.000000</td>\n",
              "      <td>46.135000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>540.000000</td>\n",
              "      <td>359.400000</td>\n",
              "      <td>200.100000</td>\n",
              "      <td>247.000000</td>\n",
              "      <td>32.200000</td>\n",
              "      <td>1145.000000</td>\n",
              "      <td>992.600000</td>\n",
              "      <td>365.000000</td>\n",
              "      <td>82.600000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Cement  Blast Furnace Slag  ...          Age     Strength\n",
              "count  1030.000000         1030.000000  ...  1030.000000  1030.000000\n",
              "mean    281.167864           73.895825  ...    45.662136    35.817961\n",
              "std     104.506364           86.279342  ...    63.169912    16.705742\n",
              "min     102.000000            0.000000  ...     1.000000     2.330000\n",
              "25%     192.375000            0.000000  ...     7.000000    23.710000\n",
              "50%     272.900000           22.000000  ...    28.000000    34.445000\n",
              "75%     350.000000          142.950000  ...    56.000000    46.135000\n",
              "max     540.000000          359.400000  ...   365.000000    82.600000\n",
              "\n",
              "[8 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reQaLIoHKOXl",
        "outputId": "74c2da90-bb37-414d-f393-cf29b84f1287"
      },
      "source": [
        "# Creating dataframes of features (X) and target (Y)\r\n",
        "X = df.iloc[:, 0:8]\r\n",
        "Y = df.iloc[:,8]\r\n",
        "\r\n",
        "# Printing the dataframes X and Y to ensure we have created the dataframes with the correct columns\r\n",
        "print('The features or the predictors (X) are : ', X, '\\n\\n') \r\n",
        "print('The target (Y) is : ', Y, '\\n\\n')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The features or the predictors (X) are :        Cement  Blast Furnace Slag  ...  Fine Aggregate  Age\n",
            "0      540.0                 0.0  ...           676.0   28\n",
            "1      540.0                 0.0  ...           676.0   28\n",
            "2      332.5               142.5  ...           594.0  270\n",
            "3      332.5               142.5  ...           594.0  365\n",
            "4      198.6               132.4  ...           825.5  360\n",
            "...      ...                 ...  ...             ...  ...\n",
            "1025   276.4               116.0  ...           768.3   28\n",
            "1026   322.2                 0.0  ...           813.4   28\n",
            "1027   148.5               139.4  ...           780.0   28\n",
            "1028   159.1               186.7  ...           788.9   28\n",
            "1029   260.9               100.5  ...           761.5   28\n",
            "\n",
            "[1030 rows x 8 columns] \n",
            "\n",
            "\n",
            "The target (Y) is :  0       79.99\n",
            "1       61.89\n",
            "2       40.27\n",
            "3       41.05\n",
            "4       44.30\n",
            "        ...  \n",
            "1025    44.28\n",
            "1026    31.18\n",
            "1027    23.70\n",
            "1028    32.77\n",
            "1029    32.40\n",
            "Name: Strength, Length: 1030, dtype: float64 \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpeIxkgTKSwn"
      },
      "source": [
        "<b>Note 1</b> : Unlike the method in this course, the splitting is done using indexing instead of using the names of the columns. Additionally, a different notation is used. The word <i>features</i> is used instead of <i>predictors</i>.\r\n",
        "\r\n",
        "\r\n",
        "<b>Note 2</b> : Pandas indexes columns starting from 0. Note in the code below for the features (X) indexing is used as `[:, 0:8]`. The first part preceding the coma `(:)` tells pandas to include ALL rows of the original dataframe (df) in the new dataframe called X while the part succedding the comma `(0:8)` tells pandas to include all columns of the original dataframe (df) starting from column with index = 0 and ending with column with index = 7, <b> but not to include the column with index = 8 </b>  \r\n",
        "\r\n",
        "<b>Note 3</b> : In order to split the data into train and test sets, the train_test_split function of the sklearn library is used. `The random_state` is used to ensure that the train and test split is the same each time, i.e. the train set and the test set have the same samples each time the code is run which is good for reproducing the results. If left empty, the random state is used by `np.random`. Since the Project requires splitting data the into <b>random</b> sets, hence `random_state` is not used, i.e. no value is set for random state. As the data hase to be split randomly  into train and test sets <b>50</b> times, a for loop will be used to to split the data in train test sets for <b>each model</b> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GhHM1EdKVKn"
      },
      "source": [
        "def regression_model() :\r\n",
        "    \r\n",
        "    # Create the model\r\n",
        "    model = Sequential()\r\n",
        "    model.add(Dense(10, activation='relu', input_shape=(X.shape[1],)))\r\n",
        "    model.add(Dense(10, activation='relu'))\r\n",
        "    model.add(Dense(1))\r\n",
        "\r\n",
        "    # Compile the model\r\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\r\n",
        "    \r\n",
        "    return model"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1_0sjSwKXxO"
      },
      "source": [
        "def data_split() :\r\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)\r\n",
        "    \r\n",
        "    # Create a list containing X_train, X_test, Y_train, Y_test and return the list\r\n",
        "    splits = [X_train, X_test, Y_train, Y_test] \r\n",
        "    return splits"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXcPvjk6KaJx"
      },
      "source": [
        "<b>Note </b> : In the above function `data_split()` the <i>X_train, X_test, Y_train, Y_test</i> sets are stored in a list and list is returned. This is to ensure that the <i>X_train, X_test, Y_train, Y_test</i> sets are not printed when the function is called"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2zoMyifKZcU"
      },
      "source": [
        "def predict() :\r\n",
        "    return model.predict(X_test)\r\n",
        "\r\n",
        "def calculate_mse() :\r\n",
        "    return mean_squared_error(Y_test,Y_predicted)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ0h9iIhKfhp"
      },
      "source": [
        "<b>Note 1</b> : The function `regression_model` as defined above only **compiles** the model and doesn't fit the model to training set. This is because in PART C, the number of epochs are changed and this would allow to fit the model with new number of epochs. This function will be used for **PART A**, **PART B** and **PART C**.\r\n",
        "\r\n",
        "<b>Note 2</b> : Since the splitting data, prediction and mean squared error calculations are all being used repeatly as well, hence there are separate functions created for them to remove redundancy of typing the same line of code while also making the code more neat. However,  as we the features (X) are to be normalized only **once**, hence there is no need to create a function for it\r\n",
        "\r\n",
        "<b>Note 3</b> : As the split function is executed first, the training and test sets are obtained and hence there is no need to explicitly pass any arguments to `predict()` and `calculate_mse()` functions. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCBHre6g0B4W"
      },
      "source": [
        "# <font color = blue> PART C : BASELINE MODEL WITH 100 EPOCHS </font>\r\n",
        "\r\n",
        "\r\n",
        "In this part, all the tasks from <b>PART B</b> are performed, but this time the number of epochs are increased to 100\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4U7W-A90FBj"
      },
      "source": [
        "## <font color = darkorange>Task 1 : Train and Test the Baseline Model with Normalized Features and 100 epochs</font>\r\n",
        "\r\n",
        "In order to train and test the the baseline model with normalized features and 100 epochs, the following steps are performed :\r\n",
        "<ol>\r\n",
        "    <li>Normalize the features (X)</li>\r\n",
        "    <li>Randomly split the data into <i>X_train, X_test, Y_train, Y_test</i> sets</li>\r\n",
        "    <li>Create a new model with 100 epochs</li>\r\n",
        "    <li>Train the model using <i>X_train, Y_train</i> using <b>50</b> epochs</li>\r\n",
        "    <li>Evaluate the model using <i>X_test, Y_test</i></li>\r\n",
        "    <li>Get the predictions on the <i>X_test</i> set </li>\r\n",
        "    <li>Compute the <b>mean squared error</b> on the test set using sklearn</li>\r\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bo23R89D0HdX"
      },
      "source": [
        "### <font color = #2980B9> Step 1 : Normalize the features (X) </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kW1jriCR0K3w"
      },
      "source": [
        "X = (X - X.mean()) / X.std()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heqgrpoA0dVm"
      },
      "source": [
        "### <font color = #2980B9> Step 2 : Randomly split the data into <i>X_train, X_test, Y_train, Y_test </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dv-xsLTl0ec5"
      },
      "source": [
        "# Creating X_train, X_test, Y_train and Y_test sets\r\n",
        "X_train, X_test, Y_train, Y_test = data_split()"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvBg0MNd0hSs"
      },
      "source": [
        "### <font color = #2980B9> Step 3 : Train the model using <i>X_train, Y_train</i> using 50 epochs </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_ZXGm-C0iYW",
        "outputId": "377d3ff4-6935-40e5-f7ba-eb3a0195d207"
      },
      "source": [
        "model = regression_model()\r\n",
        "\r\n",
        "# Fit the model on the train set\r\n",
        "model.fit(X_train, Y_train, validation_split=0.3, epochs=100)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 15ms/step - loss: 1623.1631 - val_loss: 1444.2233\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1493.2416 - val_loss: 1434.3829\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1569.5456 - val_loss: 1422.6169\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1452.1183 - val_loss: 1407.6609\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1426.8434 - val_loss: 1388.9877\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1544.5716 - val_loss: 1365.4937\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1437.1526 - val_loss: 1336.8038\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1420.7290 - val_loss: 1300.9943\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1437.3666 - val_loss: 1258.6362\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1322.0356 - val_loss: 1208.9742\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1289.5445 - val_loss: 1151.5427\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1281.4663 - val_loss: 1086.3241\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1186.0531 - val_loss: 1014.1712\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1094.0543 - val_loss: 936.5606\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1048.1534 - val_loss: 853.1929\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 937.4268 - val_loss: 767.2022\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 924.4449 - val_loss: 680.8663\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 784.0804 - val_loss: 598.0155\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 651.1986 - val_loss: 521.3373\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 604.3137 - val_loss: 452.7625\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 509.6458 - val_loss: 395.7915\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 459.6313 - val_loss: 349.4627\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 383.8764 - val_loss: 316.6646\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 377.1167 - val_loss: 293.7326\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 336.8280 - val_loss: 278.7349\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 308.3201 - val_loss: 269.2638\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 316.8327 - val_loss: 263.7116\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 282.5850 - val_loss: 259.6456\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 282.4398 - val_loss: 256.8712\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 268.3044 - val_loss: 255.3083\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 272.5977 - val_loss: 252.1276\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 251.5697 - val_loss: 251.3595\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 233.9694 - val_loss: 248.3467\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 260.7783 - val_loss: 245.3366\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 239.5671 - val_loss: 244.7338\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 242.0345 - val_loss: 241.0807\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 244.3399 - val_loss: 238.8982\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 246.5737 - val_loss: 236.6430\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 231.1253 - val_loss: 234.3256\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 210.2710 - val_loss: 233.3471\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 218.5134 - val_loss: 229.8647\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 215.8901 - val_loss: 227.1842\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 218.5728 - val_loss: 226.3200\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 203.0299 - val_loss: 222.9875\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 220.4735 - val_loss: 219.6837\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 195.7534 - val_loss: 217.5571\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 193.0260 - val_loss: 214.8692\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 206.7626 - val_loss: 211.8201\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 202.6180 - val_loss: 209.4905\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 207.1006 - val_loss: 207.9972\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 184.9501 - val_loss: 204.9801\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 197.7234 - val_loss: 202.5787\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 189.5144 - val_loss: 200.5799\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 198.0110 - val_loss: 198.2265\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 189.6270 - val_loss: 195.1940\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 171.3028 - val_loss: 193.5054\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 178.3626 - val_loss: 191.5142\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 180.9770 - val_loss: 188.2018\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 164.6778 - val_loss: 186.5332\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 174.7660 - val_loss: 184.6041\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 179.2731 - val_loss: 182.0331\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 154.9517 - val_loss: 180.0341\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 153.3264 - val_loss: 178.8537\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 163.9591 - val_loss: 176.7883\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 155.9770 - val_loss: 174.6273\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 176.4897 - val_loss: 172.5212\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 142.1291 - val_loss: 170.6669\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 161.8966 - val_loss: 169.5919\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 167.7540 - val_loss: 167.3248\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 136.3354 - val_loss: 165.4178\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 140.0077 - val_loss: 163.8313\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 146.2553 - val_loss: 161.2972\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 136.7370 - val_loss: 158.5639\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 144.4445 - val_loss: 156.9202\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 129.3228 - val_loss: 155.2921\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 147.5245 - val_loss: 153.9255\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 137.1614 - val_loss: 152.2775\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 122.1174 - val_loss: 151.2019\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 133.2462 - val_loss: 148.6771\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 142.7623 - val_loss: 147.4043\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 127.4133 - val_loss: 145.3911\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 121.8102 - val_loss: 143.8320\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 124.0571 - val_loss: 141.8741\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 122.6501 - val_loss: 140.9782\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 106.9976 - val_loss: 138.9703\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 123.0507 - val_loss: 137.9887\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 121.4140 - val_loss: 135.6721\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 109.8786 - val_loss: 134.0274\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 120.0061 - val_loss: 132.5772\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 106.4526 - val_loss: 130.8435\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 113.9593 - val_loss: 130.0283\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 104.8876 - val_loss: 127.9663\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 99.6577 - val_loss: 126.9474\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 110.7285 - val_loss: 124.6137\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 104.8423 - val_loss: 123.1001\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 105.1416 - val_loss: 121.9661\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 110.2958 - val_loss: 120.2453\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 101.5808 - val_loss: 119.1671\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 99.6259 - val_loss: 117.7209\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 101.1142 - val_loss: 116.5872\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe5f28c1b00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8B2-Ql_V0kFa"
      },
      "source": [
        "### <font color = #2980B9> Step 4 : Get the predictions on the X_test </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zs1jI5-A0mP_"
      },
      "source": [
        "# Store the predictions in a variable Y_Predicted\r\n",
        "Y_predicted = predict()"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLSMwiD70nV5"
      },
      "source": [
        "### <font color = #2980B9> Step 5 : Compute the <i>mean squared error</i> on the test set using sklearn </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEOD_eki0rjB",
        "outputId": "8deeb2ec-844d-4ba4-fe1d-a6a9d72036cb"
      },
      "source": [
        "# Calculate the mean square error\r\n",
        "\r\n",
        "mse = calculate_mse()\r\n",
        "print('Mean Square Error (MSE) of the Baseline Model with Normalized Features is : ' , str(mse))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean Square Error (MSE) of the Baseline Model with Normalized Features is :  94.96626148742997\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFuo41de01l2"
      },
      "source": [
        "## <font color = darkorange>Task 2 : Create 50 Models and Calculate the <i>µ</i> & <i>σ</i> of their MSE Errors with 100 Epochs</font>\r\n",
        "\r\n",
        "In order to train 50 models with new features (normalized features) and calulate the mean (µ) and standard deviation (σ) of their mean square errors (MSE) the following steps are performed :\r\n",
        "<ol>\r\n",
        "    <li>Create an empty list <code>list_of_mse</code> to store the mean square error of each of the models</li>\r\n",
        "    <li>Define a for loop and perform each of the following steps in the loop</li>\r\n",
        "        <ol>\r\n",
        "        <li>Randomly split the data into <i>X_train, X_test, Y_train, Y_test</i> sets</li>\r\n",
        "        <li>Train the model using <i>X_train, Y_train</i> using <b>50</b> epochs</li>\r\n",
        "        <li>Evaluate the model using <i>X_test, Y_test</i></li>\r\n",
        "        <li>Get the predictions on the <i>X_test</i> set </li>\r\n",
        "        <li>Compute the <b>mean squared error</b> on the test set using sklearn</li>\r\n",
        "    </ol>\r\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hELx9hPF0337"
      },
      "source": [
        "### <font color = #2980B9> Step 1 : Creating the <i>list_of_mse</i> list</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KALRM0DU07sV"
      },
      "source": [
        "# Create the empty lists\r\n",
        "list_of_mse = []"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZAD7OtG09KM"
      },
      "source": [
        "### <font color = #2980B9> Step 2 : Creating 50 Models, Calculating The MSE for Each and <i>µ</i> & <i>σ</i> of the 50 MSE Values in <i>list_of_mse</i> </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W57DOFi30-Ps",
        "outputId": "d67672ae-ccd0-409d-d9ec-ed15afbcacbe"
      },
      "source": [
        "# Create the for loop to split the data, create, compile & fit model, evaluate & nake predictions, caluclate mse and store\r\n",
        "# in list_of_mse\r\n",
        "\r\n",
        "start_time = datetime.now() # Starting time of the for loop execution\r\n",
        "\r\n",
        "for i in range(50) :\r\n",
        "    # Split the data into train and test set\r\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)\r\n",
        "    \r\n",
        "    # Create and compile the regression model using the function regression_model as defined in TASK 1\r\n",
        "    model = regression_model()\r\n",
        "\r\n",
        "    # Fit the model on the train set\r\n",
        "    print('\\n\\n\\nTraining Model # ' , i+1 , '\\n\\n') # Print the Model Number that is being trained\r\n",
        "    model.fit(X_train, Y_train, validation_split=0.3, epochs=100)\r\n",
        "    print('\\n')\r\n",
        "    \r\n",
        "    # Make prediction on the test set\r\n",
        "    Y_predicted = model.predict(X_test)\r\n",
        "    \r\n",
        "    # Calculate the mean square error\r\n",
        "    mse = mean_squared_error(Y_test, Y_predicted)\r\n",
        "    \r\n",
        "    # Add the mse to the list_of_mse list\r\n",
        "    list_of_mse.append(mse)\r\n",
        "\r\n",
        "end_time = datetime.now() # Ending time of the for loop execution\r\n",
        "\r\n",
        "# Print time taken for fitting 50 models and calucating the Mean and Standard Deviation of MSE of 50 models\r\n",
        "print('\\n\\nTotal Execution Time : ' , format(end_time - start_time))\r\n",
        "    "
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 114.7279 - val_loss: 136.2938\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 118.8525 - val_loss: 135.8019\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  27 \n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 15ms/step - loss: 1631.5606 - val_loss: 1651.0684\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1460.8536 - val_loss: 1632.7080\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1446.9505 - val_loss: 1613.0117\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1413.3902 - val_loss: 1590.4089\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1373.8037 - val_loss: 1564.5708\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1304.4678 - val_loss: 1533.9893\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1369.4536 - val_loss: 1497.6176\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1317.5336 - val_loss: 1455.1469\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1342.9251 - val_loss: 1406.2233\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1235.1460 - val_loss: 1349.7046\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1153.5454 - val_loss: 1285.8584\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1186.8496 - val_loss: 1213.9021\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1065.9489 - val_loss: 1136.6848\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 901.9511 - val_loss: 1052.1906\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 923.6661 - val_loss: 962.7366\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 800.0346 - val_loss: 871.5922\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 710.4885 - val_loss: 779.8110\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 672.0776 - val_loss: 690.0274\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 543.7247 - val_loss: 606.1318\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 516.4282 - val_loss: 526.9333\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 411.3452 - val_loss: 459.4221\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 393.8979 - val_loss: 402.3499\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 332.2780 - val_loss: 357.4690\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 317.1057 - val_loss: 320.4767\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 263.2666 - val_loss: 294.3885\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 272.1572 - val_loss: 273.8337\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 260.7632 - val_loss: 257.3760\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 255.7886 - val_loss: 245.6107\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 233.1197 - val_loss: 236.0198\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 222.3032 - val_loss: 228.4650\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 216.6401 - val_loss: 222.0461\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 206.8015 - val_loss: 216.6527\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 217.0255 - val_loss: 211.8720\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 204.1022 - val_loss: 206.6740\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 199.4705 - val_loss: 202.4710\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 206.9259 - val_loss: 199.0971\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 203.6141 - val_loss: 194.8466\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 197.7426 - val_loss: 191.5403\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 182.6815 - val_loss: 189.0219\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 194.0249 - val_loss: 186.1420\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 187.1395 - val_loss: 182.4944\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 200.7251 - val_loss: 179.5963\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 183.6350 - val_loss: 177.4595\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 189.6045 - val_loss: 174.7036\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 170.9986 - val_loss: 172.8617\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 170.5842 - val_loss: 171.6351\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 174.1619 - val_loss: 169.2743\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 160.9563 - val_loss: 167.1510\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 156.7263 - val_loss: 165.7628\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 159.8925 - val_loss: 163.8933\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 169.3838 - val_loss: 161.7310\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 154.9523 - val_loss: 160.2680\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 155.5806 - val_loss: 158.8413\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 168.0523 - val_loss: 157.5681\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 167.0231 - val_loss: 155.9642\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 163.7238 - val_loss: 154.2646\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 151.8825 - val_loss: 153.1940\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 147.9476 - val_loss: 152.0002\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 151.5421 - val_loss: 150.7257\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 152.7832 - val_loss: 149.4622\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 149.2384 - val_loss: 148.1234\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 152.4485 - val_loss: 147.0210\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 153.6981 - val_loss: 146.5110\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 146.3539 - val_loss: 145.2041\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 144.4628 - val_loss: 144.4834\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 146.8060 - val_loss: 143.5973\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 132.7218 - val_loss: 142.7706\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 141.9832 - val_loss: 141.9283\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 144.3828 - val_loss: 140.7564\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 139.4245 - val_loss: 140.0418\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 132.9493 - val_loss: 139.3983\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 135.8177 - val_loss: 138.2220\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 131.3495 - val_loss: 137.6589\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 142.9333 - val_loss: 137.3772\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 145.6360 - val_loss: 136.5768\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 134.1842 - val_loss: 135.6726\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 139.5810 - val_loss: 134.8673\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 141.5523 - val_loss: 134.5931\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 134.5389 - val_loss: 133.6272\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 147.6641 - val_loss: 133.0116\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 125.2384 - val_loss: 132.6673\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 128.5751 - val_loss: 132.0845\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 129.4414 - val_loss: 131.4637\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 128.6292 - val_loss: 130.7361\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 134.0874 - val_loss: 130.6484\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 121.7910 - val_loss: 129.8418\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 138.0130 - val_loss: 129.3610\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 134.0822 - val_loss: 128.7643\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 130.2961 - val_loss: 128.3014\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 135.5096 - val_loss: 127.8127\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 133.4782 - val_loss: 127.7092\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 125.7393 - val_loss: 126.8925\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 131.4069 - val_loss: 126.8972\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 118.0528 - val_loss: 126.2690\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 131.7942 - val_loss: 126.0103\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 127.2903 - val_loss: 125.5667\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 133.0759 - val_loss: 125.2612\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 126.1065 - val_loss: 124.6640\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 125.1324 - val_loss: 124.6901\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 122.7029 - val_loss: 124.1733\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  28 \n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 17ms/step - loss: 1612.2456 - val_loss: 1644.5074\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1452.5013 - val_loss: 1632.4016\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1525.6368 - val_loss: 1618.5712\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1537.8583 - val_loss: 1603.0785\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1476.5875 - val_loss: 1584.9453\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1374.5409 - val_loss: 1563.6619\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1469.9417 - val_loss: 1538.4568\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1403.5469 - val_loss: 1508.8489\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1347.8942 - val_loss: 1473.4436\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1357.9968 - val_loss: 1431.3140\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1301.1740 - val_loss: 1382.8099\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1277.3578 - val_loss: 1323.9520\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1238.1898 - val_loss: 1256.3596\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1045.1972 - val_loss: 1182.8715\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1186.3641 - val_loss: 1099.0005\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1013.2425 - val_loss: 1010.1852\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 947.4539 - val_loss: 917.2645\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 847.1138 - val_loss: 826.8204\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 701.4329 - val_loss: 738.8629\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 698.4888 - val_loss: 657.7808\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 587.3303 - val_loss: 585.9610\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 524.3971 - val_loss: 522.4712\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 451.3062 - val_loss: 467.3845\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 431.6641 - val_loss: 420.0908\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 368.8803 - val_loss: 381.4583\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 334.5518 - val_loss: 349.2144\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 264.6099 - val_loss: 322.6432\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 248.7068 - val_loss: 302.2401\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 219.8235 - val_loss: 285.6564\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 233.5338 - val_loss: 271.8289\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 206.6599 - val_loss: 261.4932\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 209.7216 - val_loss: 253.6849\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 185.2404 - val_loss: 247.0114\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 166.3114 - val_loss: 241.5517\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 172.0724 - val_loss: 236.7034\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 166.4317 - val_loss: 231.9769\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 142.1843 - val_loss: 227.9072\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 172.6554 - val_loss: 223.7612\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 159.3259 - val_loss: 220.3787\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 146.2808 - val_loss: 217.8505\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 151.1653 - val_loss: 214.7561\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 151.6345 - val_loss: 211.8067\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 142.3798 - val_loss: 209.9080\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 153.7455 - val_loss: 207.7978\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 149.6789 - val_loss: 205.3040\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 140.9877 - val_loss: 203.7940\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 135.2920 - val_loss: 201.1158\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 141.7356 - val_loss: 199.5246\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 145.3491 - val_loss: 198.7723\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 132.2273 - val_loss: 197.4825\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 135.0037 - val_loss: 195.7659\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 129.3214 - val_loss: 194.8033\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 138.0942 - val_loss: 193.4894\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 137.3307 - val_loss: 192.6602\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 132.2765 - val_loss: 191.4486\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 132.2306 - val_loss: 190.8218\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 133.5316 - val_loss: 189.3613\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 145.3325 - val_loss: 187.8561\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 129.0753 - val_loss: 186.9240\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 127.5562 - val_loss: 186.1460\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 132.9404 - val_loss: 184.9699\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 128.0369 - val_loss: 184.6084\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 123.2286 - val_loss: 184.0653\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 128.2227 - val_loss: 182.6262\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 126.9707 - val_loss: 181.9479\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 127.1050 - val_loss: 180.7773\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 126.7300 - val_loss: 180.4352\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 116.2836 - val_loss: 180.1497\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 126.9704 - val_loss: 178.8115\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 120.4537 - val_loss: 177.5269\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 128.2461 - val_loss: 177.0410\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 123.2048 - val_loss: 176.4331\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 125.0036 - val_loss: 176.1664\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 123.9175 - val_loss: 175.6431\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 123.7310 - val_loss: 174.8530\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 117.2356 - val_loss: 174.4498\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 124.9395 - val_loss: 174.1657\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 118.4488 - val_loss: 173.4950\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 122.0277 - val_loss: 173.2531\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 118.6784 - val_loss: 172.5529\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 121.0434 - val_loss: 171.7767\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 117.2731 - val_loss: 171.4464\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 113.3266 - val_loss: 172.0418\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 115.0981 - val_loss: 171.1315\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 105.8334 - val_loss: 170.4846\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 125.0693 - val_loss: 170.3335\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 119.0568 - val_loss: 170.3579\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 120.7000 - val_loss: 168.8124\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 115.6319 - val_loss: 168.7638\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 113.3829 - val_loss: 167.8985\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 117.4786 - val_loss: 167.6587\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 108.7205 - val_loss: 168.4491\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 109.1912 - val_loss: 167.4728\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 116.2344 - val_loss: 166.6537\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 100.0655 - val_loss: 166.7768\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 120.1442 - val_loss: 166.1856\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 118.3316 - val_loss: 165.5755\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 115.4146 - val_loss: 165.2428\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 122.2755 - val_loss: 164.7415\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 110.6913 - val_loss: 164.7449\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  29 \n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 16ms/step - loss: 1485.7384 - val_loss: 1390.7241\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1393.0219 - val_loss: 1367.1749\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1468.9244 - val_loss: 1341.5679\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1462.5427 - val_loss: 1313.2864\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1444.1650 - val_loss: 1281.5568\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1278.2486 - val_loss: 1245.6024\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1345.7326 - val_loss: 1204.4713\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 1260.7712 - val_loss: 1158.9523\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1158.7758 - val_loss: 1108.2992\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1101.6240 - val_loss: 1052.9016\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1032.6832 - val_loss: 992.4310\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 984.1199 - val_loss: 928.5120\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 903.5141 - val_loss: 862.4248\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 888.9777 - val_loss: 792.8978\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 773.6283 - val_loss: 725.1122\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 668.2144 - val_loss: 655.9861\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 647.4899 - val_loss: 591.3336\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 522.0360 - val_loss: 531.6910\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 469.4529 - val_loss: 478.1971\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 429.9096 - val_loss: 432.0322\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 401.6911 - val_loss: 392.4127\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 325.1605 - val_loss: 359.1425\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 340.2891 - val_loss: 330.4134\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 315.0598 - val_loss: 306.3232\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 274.3574 - val_loss: 286.3833\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 260.2023 - val_loss: 270.4673\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 240.3988 - val_loss: 255.7584\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 250.0652 - val_loss: 243.7580\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 240.0901 - val_loss: 232.9792\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 211.6948 - val_loss: 224.8434\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 199.2312 - val_loss: 217.3170\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 214.9384 - val_loss: 210.5670\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 216.2347 - val_loss: 204.8010\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 192.1060 - val_loss: 200.4544\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 200.6045 - val_loss: 196.2397\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 194.3773 - val_loss: 192.6548\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 208.4978 - val_loss: 189.0887\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 194.9470 - val_loss: 186.1827\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 175.3493 - val_loss: 183.0847\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 173.7915 - val_loss: 180.3270\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 179.3606 - val_loss: 177.3206\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 178.3679 - val_loss: 175.4991\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 170.8815 - val_loss: 173.0453\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 168.1560 - val_loss: 171.1748\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 169.8036 - val_loss: 169.0435\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 185.2870 - val_loss: 167.2891\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 171.0811 - val_loss: 165.7386\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 164.1062 - val_loss: 163.8696\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 173.4844 - val_loss: 162.1635\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 156.8603 - val_loss: 160.7334\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 151.3101 - val_loss: 159.5250\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 147.7736 - val_loss: 158.0922\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 159.9437 - val_loss: 157.1320\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 152.3833 - val_loss: 155.9691\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 149.7494 - val_loss: 154.8665\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 166.5479 - val_loss: 153.3233\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 148.3540 - val_loss: 152.8152\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 145.6685 - val_loss: 151.8524\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 147.8419 - val_loss: 151.2727\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 149.7944 - val_loss: 150.6641\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 149.3389 - val_loss: 149.8480\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 143.8856 - val_loss: 148.6157\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 146.2599 - val_loss: 148.0194\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 140.2187 - val_loss: 147.4532\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 143.8653 - val_loss: 146.8356\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 134.9186 - val_loss: 146.1218\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 137.2346 - val_loss: 145.3725\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 149.7065 - val_loss: 145.0312\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 141.6418 - val_loss: 144.0864\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 135.1882 - val_loss: 144.0318\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 155.6925 - val_loss: 143.5766\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 126.9570 - val_loss: 142.8221\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 135.5223 - val_loss: 142.6470\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 137.5345 - val_loss: 142.0186\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 154.6552 - val_loss: 142.0499\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 136.6981 - val_loss: 141.5934\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 137.5400 - val_loss: 141.2804\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 144.1887 - val_loss: 140.6115\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 141.2494 - val_loss: 140.0923\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 143.6423 - val_loss: 139.9442\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 145.8858 - val_loss: 139.5022\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 132.8622 - val_loss: 138.9314\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 139.8021 - val_loss: 138.8650\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 134.5460 - val_loss: 138.3470\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 137.7428 - val_loss: 138.2475\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 130.7651 - val_loss: 137.7448\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 137.2362 - val_loss: 137.5027\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 137.9311 - val_loss: 137.6007\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 127.9181 - val_loss: 137.4418\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 124.7947 - val_loss: 137.0412\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 121.0451 - val_loss: 136.8655\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 124.1708 - val_loss: 136.7759\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 131.8665 - val_loss: 136.4322\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 127.7152 - val_loss: 136.3405\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 124.5395 - val_loss: 136.2016\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 128.8753 - val_loss: 135.9094\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 124.7034 - val_loss: 135.9790\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 129.6336 - val_loss: 135.5905\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 118.1920 - val_loss: 135.3263\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 118.2876 - val_loss: 135.1634\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  30 \n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 17ms/step - loss: 1495.0061 - val_loss: 1543.8723\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1541.9747 - val_loss: 1519.9746\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1497.7623 - val_loss: 1495.4192\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1580.4793 - val_loss: 1468.7942\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1503.7533 - val_loss: 1438.8854\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1516.3900 - val_loss: 1405.7351\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1338.8420 - val_loss: 1368.0464\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1372.9048 - val_loss: 1324.5505\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1331.1553 - val_loss: 1276.0023\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1248.5698 - val_loss: 1221.6483\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1222.7065 - val_loss: 1161.9342\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1130.4847 - val_loss: 1098.1329\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1081.2683 - val_loss: 1027.9878\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 990.3901 - val_loss: 954.9883\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 949.7670 - val_loss: 881.1577\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 865.3353 - val_loss: 806.5115\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 763.8813 - val_loss: 733.7620\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 668.3202 - val_loss: 663.0151\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 606.6808 - val_loss: 593.4404\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 579.7327 - val_loss: 529.9962\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 489.4699 - val_loss: 474.2941\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 431.5765 - val_loss: 426.8229\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 382.9607 - val_loss: 387.1262\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 344.5852 - val_loss: 354.2082\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 301.8936 - val_loss: 330.1761\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 299.5474 - val_loss: 310.6007\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 267.3043 - val_loss: 295.9673\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 254.9043 - val_loss: 284.1035\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 245.3519 - val_loss: 275.0696\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 259.7765 - val_loss: 267.3651\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 211.8426 - val_loss: 261.4128\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 240.9693 - val_loss: 255.6558\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 226.8055 - val_loss: 250.6696\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 228.2715 - val_loss: 246.1196\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 216.3766 - val_loss: 242.1194\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 219.4870 - val_loss: 238.5026\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 209.4133 - val_loss: 234.9672\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 218.5755 - val_loss: 231.4636\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 200.0054 - val_loss: 228.2519\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 201.4483 - val_loss: 225.4162\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 214.2071 - val_loss: 222.7361\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 197.8656 - val_loss: 220.3412\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 186.7307 - val_loss: 217.7401\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 189.3881 - val_loss: 215.2992\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 183.7577 - val_loss: 213.1936\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 189.8804 - val_loss: 211.0368\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 181.6802 - val_loss: 209.0700\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 180.8590 - val_loss: 207.0621\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 160.0640 - val_loss: 205.3766\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 184.1252 - val_loss: 203.3424\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 179.7333 - val_loss: 201.6268\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 181.3848 - val_loss: 199.8088\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 185.3561 - val_loss: 198.4326\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 164.8236 - val_loss: 197.0173\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 158.5522 - val_loss: 195.2669\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 185.3124 - val_loss: 193.8008\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 151.5389 - val_loss: 192.4124\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 169.6407 - val_loss: 191.3754\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 164.1225 - val_loss: 190.0479\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 152.7326 - val_loss: 188.6365\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 171.9780 - val_loss: 187.2763\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 163.1943 - val_loss: 186.2245\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 149.3666 - val_loss: 185.0206\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 159.0307 - val_loss: 183.8539\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 159.6370 - val_loss: 182.5352\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 155.7184 - val_loss: 181.5054\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 164.4151 - val_loss: 180.4399\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 173.7438 - val_loss: 179.3527\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 156.2019 - val_loss: 178.5049\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 151.0063 - val_loss: 177.4212\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 146.2458 - val_loss: 176.3232\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 151.3992 - val_loss: 175.3096\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 163.0378 - val_loss: 174.5716\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 137.8152 - val_loss: 173.8938\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 149.5466 - val_loss: 173.0102\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 152.8878 - val_loss: 172.0327\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 143.2755 - val_loss: 171.1072\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 160.9054 - val_loss: 170.2842\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 135.3689 - val_loss: 169.5983\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 136.1934 - val_loss: 168.8266\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 143.1958 - val_loss: 167.8460\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 145.9741 - val_loss: 167.2250\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 135.2304 - val_loss: 166.3018\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 148.7140 - val_loss: 165.7631\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 145.6606 - val_loss: 164.7866\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 127.8154 - val_loss: 164.2203\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 137.3424 - val_loss: 163.4925\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 138.9320 - val_loss: 163.1701\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 134.7356 - val_loss: 162.6292\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 140.5399 - val_loss: 161.7568\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 144.6848 - val_loss: 161.1485\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 140.2366 - val_loss: 160.7921\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 136.4636 - val_loss: 160.2322\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 141.9153 - val_loss: 159.6742\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 127.9547 - val_loss: 159.1756\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 134.0840 - val_loss: 158.4044\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 139.2328 - val_loss: 157.9132\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 141.1216 - val_loss: 157.3275\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 130.3476 - val_loss: 156.7751\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 138.9296 - val_loss: 156.3149\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  31 \n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 15ms/step - loss: 1529.6787 - val_loss: 1503.6327\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1398.1692 - val_loss: 1481.3632\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1552.0989 - val_loss: 1456.1202\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1512.5010 - val_loss: 1427.5776\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1490.2304 - val_loss: 1395.0021\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1428.7308 - val_loss: 1357.0784\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1467.9125 - val_loss: 1313.9196\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1388.1516 - val_loss: 1264.4633\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1313.6250 - val_loss: 1208.5551\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1246.6540 - val_loss: 1146.8226\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1246.5353 - val_loss: 1078.5131\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1129.0056 - val_loss: 1005.6042\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1016.3800 - val_loss: 929.0482\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 928.8934 - val_loss: 850.7628\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 872.9163 - val_loss: 770.7162\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 765.2739 - val_loss: 694.6017\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 679.5533 - val_loss: 622.1893\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 578.5076 - val_loss: 555.6270\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 547.8043 - val_loss: 496.3862\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 488.1183 - val_loss: 446.6463\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 482.7559 - val_loss: 405.2954\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 395.4801 - val_loss: 372.1938\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 350.9294 - val_loss: 345.9074\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 353.0058 - val_loss: 323.1066\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 319.8525 - val_loss: 305.0060\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 295.4956 - val_loss: 288.9959\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 243.3492 - val_loss: 275.8283\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 242.4147 - val_loss: 263.9840\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 237.4447 - val_loss: 254.1138\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 226.4637 - val_loss: 244.9199\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 213.5206 - val_loss: 237.7486\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 221.8520 - val_loss: 230.6447\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 200.5391 - val_loss: 224.9529\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 205.1857 - val_loss: 219.3692\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 202.2164 - val_loss: 214.0694\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 194.8910 - val_loss: 210.5118\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 186.1568 - val_loss: 206.6224\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 178.6657 - val_loss: 203.5872\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 194.0226 - val_loss: 199.8798\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 171.8089 - val_loss: 197.0766\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 182.2548 - val_loss: 193.6947\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 175.2055 - val_loss: 191.0767\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 176.7113 - val_loss: 188.9196\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 159.7745 - val_loss: 186.3867\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 178.7849 - val_loss: 184.0013\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 166.9727 - val_loss: 181.9014\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 164.2161 - val_loss: 179.7363\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 153.7132 - val_loss: 177.8351\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 151.2993 - val_loss: 175.9731\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 152.0121 - val_loss: 174.1666\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 152.0204 - val_loss: 172.0822\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 149.2125 - val_loss: 170.3948\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 148.8760 - val_loss: 168.5433\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 143.9690 - val_loss: 166.9892\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 142.5182 - val_loss: 165.2688\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 145.0236 - val_loss: 163.4383\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 149.6152 - val_loss: 161.9733\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 135.0125 - val_loss: 160.6233\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 142.1623 - val_loss: 158.9178\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 137.2311 - val_loss: 157.5856\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 129.1314 - val_loss: 155.8924\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 129.2158 - val_loss: 154.9422\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 120.8964 - val_loss: 153.2848\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 130.9028 - val_loss: 151.8263\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 127.2189 - val_loss: 150.4688\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 139.1964 - val_loss: 149.2220\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 132.7571 - val_loss: 148.1485\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 131.6774 - val_loss: 147.0800\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 121.2341 - val_loss: 146.0924\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 116.5944 - val_loss: 144.8245\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 117.2315 - val_loss: 143.8154\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 130.9510 - val_loss: 142.7738\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 109.9732 - val_loss: 142.2676\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 131.5014 - val_loss: 141.0303\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 117.3645 - val_loss: 140.1397\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 113.9434 - val_loss: 139.1669\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 128.4678 - val_loss: 137.9973\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 125.1328 - val_loss: 137.5303\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 112.9338 - val_loss: 136.5754\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 115.9100 - val_loss: 135.4262\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 105.4757 - val_loss: 134.9789\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 118.8015 - val_loss: 133.9506\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 113.3970 - val_loss: 133.0555\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 115.5450 - val_loss: 132.2745\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 111.1787 - val_loss: 131.4400\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 100.9414 - val_loss: 130.9187\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 103.0109 - val_loss: 130.0164\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 99.5421 - val_loss: 129.4602\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 105.9767 - val_loss: 128.9147\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 107.1685 - val_loss: 128.0366\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 98.3907 - val_loss: 127.4863\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 113.8241 - val_loss: 126.5644\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 102.2238 - val_loss: 125.9899\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 95.5256 - val_loss: 125.4090\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 105.5884 - val_loss: 124.6886\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 94.1480 - val_loss: 123.8919\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 112.2668 - val_loss: 123.4006\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 93.9959 - val_loss: 123.1760\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 100.5892 - val_loss: 122.2472\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 100.9592 - val_loss: 121.7919\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  32 \n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 15ms/step - loss: 1550.8848 - val_loss: 1583.9312\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1527.6337 - val_loss: 1564.1628\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1478.2234 - val_loss: 1542.0101\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1429.9191 - val_loss: 1517.8453\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1442.4799 - val_loss: 1489.9790\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1381.7842 - val_loss: 1457.9294\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1387.2474 - val_loss: 1419.8617\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1236.4263 - val_loss: 1376.2307\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1236.9776 - val_loss: 1325.3326\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1229.0677 - val_loss: 1268.0574\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 1160.4971 - val_loss: 1205.0325\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1095.6613 - val_loss: 1136.0709\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1070.7732 - val_loss: 1059.6497\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 969.5217 - val_loss: 980.1270\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 956.9846 - val_loss: 897.7552\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 886.7445 - val_loss: 812.9777\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 793.2329 - val_loss: 729.6938\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 676.0345 - val_loss: 650.4568\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 630.3911 - val_loss: 574.4161\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 555.3789 - val_loss: 504.7903\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 529.9743 - val_loss: 444.0463\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 391.4645 - val_loss: 394.0580\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 361.6947 - val_loss: 352.2125\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 332.3277 - val_loss: 318.7644\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 277.4198 - val_loss: 293.6776\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 250.3407 - val_loss: 275.0158\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 244.4422 - val_loss: 261.2003\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 246.7609 - val_loss: 251.6860\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 239.0401 - val_loss: 244.2700\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 209.7655 - val_loss: 239.2590\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 208.8815 - val_loss: 235.1398\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 189.7435 - val_loss: 231.7499\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 184.8175 - val_loss: 227.7112\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 181.2336 - val_loss: 224.2408\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 188.3580 - val_loss: 221.4046\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 182.7960 - val_loss: 218.8041\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 180.6881 - val_loss: 215.5927\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 176.2959 - val_loss: 213.1389\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 166.7157 - val_loss: 210.6535\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 172.8639 - val_loss: 208.2006\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 151.3467 - val_loss: 206.3018\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 173.5814 - val_loss: 203.3539\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 175.3038 - val_loss: 201.5030\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 163.0216 - val_loss: 199.2633\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 153.7040 - val_loss: 197.6879\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 176.3149 - val_loss: 195.3346\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 165.7817 - val_loss: 193.4853\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 166.6445 - val_loss: 191.4831\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 163.7533 - val_loss: 190.1126\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 161.8670 - val_loss: 188.6289\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 171.6582 - val_loss: 186.1762\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 158.6773 - val_loss: 184.4839\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 163.2389 - val_loss: 183.7402\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 154.3583 - val_loss: 181.5750\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 147.3645 - val_loss: 179.8503\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 157.3441 - val_loss: 178.5647\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 143.0567 - val_loss: 176.4793\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 141.3258 - val_loss: 175.9506\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 147.4447 - val_loss: 175.1373\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 141.9553 - val_loss: 172.9197\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 160.4409 - val_loss: 170.8099\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 159.9489 - val_loss: 169.9786\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 139.0225 - val_loss: 168.8022\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 134.6216 - val_loss: 167.5038\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 142.3273 - val_loss: 166.1359\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 127.4586 - val_loss: 164.3986\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 149.4803 - val_loss: 162.5711\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 135.5342 - val_loss: 161.6060\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 139.1403 - val_loss: 160.0156\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 122.2879 - val_loss: 159.3576\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 130.7360 - val_loss: 157.2818\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 135.4587 - val_loss: 156.1369\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 136.9769 - val_loss: 154.8766\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 126.8487 - val_loss: 153.5330\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 133.9148 - val_loss: 152.5880\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 127.4966 - val_loss: 151.1157\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 121.4870 - val_loss: 149.9330\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 128.3697 - val_loss: 148.8743\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 131.5154 - val_loss: 147.3081\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 127.2147 - val_loss: 146.7881\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 116.8146 - val_loss: 145.3379\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 118.7724 - val_loss: 144.1368\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 120.8641 - val_loss: 142.6397\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 114.0089 - val_loss: 141.3892\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 122.0136 - val_loss: 140.5190\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 116.0787 - val_loss: 139.0855\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 111.9864 - val_loss: 137.7792\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 118.2753 - val_loss: 136.5866\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 108.0398 - val_loss: 135.7168\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 103.3690 - val_loss: 134.4975\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 116.0669 - val_loss: 133.9445\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 107.9661 - val_loss: 132.7631\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 113.4467 - val_loss: 131.2700\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 104.8602 - val_loss: 130.4126\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 104.3954 - val_loss: 129.0802\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 105.1678 - val_loss: 127.9084\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 104.6581 - val_loss: 127.3442\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 106.8905 - val_loss: 125.7605\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 109.4929 - val_loss: 124.8515\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 100.4389 - val_loss: 123.4114\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  33 \n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 24ms/step - loss: 1676.5918 - val_loss: 1637.3160\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1618.9908 - val_loss: 1619.1962\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1615.0051 - val_loss: 1603.7271\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1557.0560 - val_loss: 1590.8689\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1480.3786 - val_loss: 1579.3711\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1570.2947 - val_loss: 1568.2434\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1550.0000 - val_loss: 1556.6133\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1497.3915 - val_loss: 1543.6475\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1566.7530 - val_loss: 1528.4674\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1468.9724 - val_loss: 1511.1980\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1470.8231 - val_loss: 1489.8848\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1416.6905 - val_loss: 1464.9073\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1453.3643 - val_loss: 1435.4176\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1372.2208 - val_loss: 1401.1945\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1385.7941 - val_loss: 1361.5753\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1272.4349 - val_loss: 1317.8232\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1318.4787 - val_loss: 1267.5520\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1177.1886 - val_loss: 1213.8258\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1149.9297 - val_loss: 1155.0990\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1105.3930 - val_loss: 1091.4927\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1059.3810 - val_loss: 1025.3816\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 945.8196 - val_loss: 958.2499\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 901.4312 - val_loss: 890.0964\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 776.9183 - val_loss: 822.5895\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 770.4904 - val_loss: 756.2220\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 676.7548 - val_loss: 693.7048\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 602.4109 - val_loss: 634.3177\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 535.1800 - val_loss: 579.3488\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 459.0072 - val_loss: 529.5646\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 423.5308 - val_loss: 485.1726\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 381.2510 - val_loss: 444.6206\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 362.4983 - val_loss: 408.9139\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 362.6463 - val_loss: 378.1772\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 297.1829 - val_loss: 351.9237\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 295.2709 - val_loss: 329.7968\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 257.2550 - val_loss: 312.0765\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 285.4434 - val_loss: 295.8627\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 225.8359 - val_loss: 282.8033\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 226.1892 - val_loss: 272.0634\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 200.5269 - val_loss: 262.8600\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 205.2422 - val_loss: 255.8667\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 217.5085 - val_loss: 248.2637\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 194.6027 - val_loss: 242.9459\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 214.8734 - val_loss: 237.5359\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 209.9879 - val_loss: 232.7677\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 198.6494 - val_loss: 228.5030\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 187.0644 - val_loss: 224.6056\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 180.6799 - val_loss: 220.9402\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 198.7499 - val_loss: 217.2993\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 180.3452 - val_loss: 213.7989\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 193.5898 - val_loss: 210.8001\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 196.7138 - val_loss: 207.9675\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 170.5223 - val_loss: 205.2234\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 158.7287 - val_loss: 202.4543\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 177.3283 - val_loss: 200.3857\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 179.0137 - val_loss: 198.0638\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 164.8846 - val_loss: 195.5095\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 172.5886 - val_loss: 194.0480\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 165.5533 - val_loss: 192.1374\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 167.7758 - val_loss: 189.9069\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 146.1269 - val_loss: 188.2877\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 158.3021 - val_loss: 186.1978\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 151.4597 - val_loss: 184.8888\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 162.1334 - val_loss: 183.2804\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 164.1177 - val_loss: 181.6358\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 150.3873 - val_loss: 180.1420\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 138.6805 - val_loss: 178.3587\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 145.9134 - val_loss: 177.2335\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 151.1951 - val_loss: 175.9231\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 156.7780 - val_loss: 174.5044\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 155.5764 - val_loss: 173.3280\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 147.9799 - val_loss: 171.9023\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 142.1013 - val_loss: 170.5811\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 145.7909 - val_loss: 169.4518\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 166.7601 - val_loss: 168.5409\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 143.0083 - val_loss: 167.4940\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 151.9881 - val_loss: 166.7302\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 134.3372 - val_loss: 165.5892\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 151.7933 - val_loss: 164.7993\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 130.3763 - val_loss: 163.7968\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 127.3182 - val_loss: 162.8172\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 134.1055 - val_loss: 162.0577\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 143.6780 - val_loss: 161.4587\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 127.6950 - val_loss: 160.6027\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 143.9645 - val_loss: 159.9163\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 132.6275 - val_loss: 159.0778\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 127.8481 - val_loss: 158.7093\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 131.0314 - val_loss: 158.1607\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 125.0541 - val_loss: 157.5695\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 131.9564 - val_loss: 157.1586\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 110.9393 - val_loss: 156.5814\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 127.0130 - val_loss: 156.0588\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 120.1986 - val_loss: 155.6445\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 137.1695 - val_loss: 155.4120\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 119.7209 - val_loss: 154.9519\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 129.4618 - val_loss: 154.3742\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 130.5220 - val_loss: 153.9948\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 127.5153 - val_loss: 153.3605\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 121.3133 - val_loss: 153.2989\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 122.1903 - val_loss: 152.7597\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  34 \n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 15ms/step - loss: 1572.5582 - val_loss: 1660.6241\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1414.5839 - val_loss: 1648.6068\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1506.1364 - val_loss: 1634.3586\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1509.0664 - val_loss: 1617.4524\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 1438.6078 - val_loss: 1597.9104\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1483.0823 - val_loss: 1574.9629\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1404.1317 - val_loss: 1548.9045\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1425.7442 - val_loss: 1517.8822\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1439.3527 - val_loss: 1481.7413\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1379.6784 - val_loss: 1441.0579\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1277.0374 - val_loss: 1395.2491\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1182.2057 - val_loss: 1344.4309\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1148.2895 - val_loss: 1285.7190\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1121.9163 - val_loss: 1221.6393\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1107.2523 - val_loss: 1151.4553\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1047.0971 - val_loss: 1075.1152\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 932.5344 - val_loss: 995.9093\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 832.6907 - val_loss: 912.8257\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 784.8950 - val_loss: 826.5225\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 694.2946 - val_loss: 743.5210\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 685.0152 - val_loss: 661.5280\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 537.7360 - val_loss: 586.3439\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 512.2084 - val_loss: 515.5007\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 404.4530 - val_loss: 454.3800\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 376.1400 - val_loss: 400.9680\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 317.9929 - val_loss: 358.0939\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 282.5996 - val_loss: 325.2139\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 254.4088 - val_loss: 298.8747\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 263.0401 - val_loss: 279.7664\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 200.6730 - val_loss: 266.7535\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 221.9088 - val_loss: 255.5554\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 216.1084 - val_loss: 248.4840\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 193.5250 - val_loss: 243.4221\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 194.5980 - val_loss: 239.3081\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 185.9228 - val_loss: 236.0582\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 200.0047 - val_loss: 234.2986\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 176.7325 - val_loss: 232.4400\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 169.9072 - val_loss: 231.3965\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 163.8152 - val_loss: 230.4592\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 170.7734 - val_loss: 229.2869\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 156.0610 - val_loss: 228.5206\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 164.7833 - val_loss: 227.4526\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 153.9222 - val_loss: 226.7998\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 157.8535 - val_loss: 226.1392\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 159.0014 - val_loss: 225.9785\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 170.8928 - val_loss: 224.9952\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 157.6899 - val_loss: 225.0496\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 166.0402 - val_loss: 224.5258\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 161.6794 - val_loss: 224.0566\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 150.1651 - val_loss: 223.8358\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 148.8721 - val_loss: 223.7946\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 137.8958 - val_loss: 222.9152\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 150.0626 - val_loss: 222.0340\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 135.2641 - val_loss: 221.7167\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 150.7361 - val_loss: 221.9471\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 151.2937 - val_loss: 220.5862\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 130.7631 - val_loss: 219.8124\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 146.3854 - val_loss: 219.5741\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 131.2594 - val_loss: 218.7358\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 128.4930 - val_loss: 218.5100\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 146.9169 - val_loss: 217.2562\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 133.2142 - val_loss: 216.7020\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 125.9528 - val_loss: 216.6133\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 129.1947 - val_loss: 215.6085\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 133.5948 - val_loss: 214.8365\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 133.5823 - val_loss: 213.9369\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 137.6701 - val_loss: 211.8770\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 129.8600 - val_loss: 211.7272\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 129.2572 - val_loss: 211.4182\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 119.7475 - val_loss: 211.1979\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 124.0171 - val_loss: 210.5896\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 134.7991 - val_loss: 209.1667\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 119.1667 - val_loss: 207.6645\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 134.8307 - val_loss: 207.0813\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 138.7883 - val_loss: 205.9225\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 120.0722 - val_loss: 205.8512\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 120.3868 - val_loss: 205.3154\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 121.5801 - val_loss: 205.3515\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 129.4154 - val_loss: 204.4191\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 129.6566 - val_loss: 203.4788\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 126.4569 - val_loss: 203.1268\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 122.3450 - val_loss: 201.6367\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 128.7701 - val_loss: 200.5022\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 127.2792 - val_loss: 200.5546\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 107.6900 - val_loss: 200.2142\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 116.9067 - val_loss: 198.5268\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 115.9862 - val_loss: 199.0440\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 117.1693 - val_loss: 197.6450\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 124.6359 - val_loss: 197.0035\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 117.4091 - val_loss: 197.0284\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 115.9700 - val_loss: 194.9545\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 111.8059 - val_loss: 195.3430\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 113.6690 - val_loss: 194.7934\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 117.8332 - val_loss: 194.3229\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 109.9058 - val_loss: 194.1268\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 118.9239 - val_loss: 193.0008\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 110.5095 - val_loss: 192.6326\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 111.2865 - val_loss: 192.8158\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 111.6151 - val_loss: 191.0155\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 122.2504 - val_loss: 191.4748\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  35 \n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 16ms/step - loss: 1466.3763 - val_loss: 1589.2561\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1611.4853 - val_loss: 1577.2003\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1454.4649 - val_loss: 1562.9425\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1499.6493 - val_loss: 1545.1650\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1382.9315 - val_loss: 1523.6892\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1473.7213 - val_loss: 1496.3560\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1430.5446 - val_loss: 1461.7091\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1321.7565 - val_loss: 1418.9487\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1323.1109 - val_loss: 1367.5150\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 1210.2160 - val_loss: 1307.7853\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1200.2135 - val_loss: 1238.0959\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1204.1525 - val_loss: 1159.3207\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1068.0757 - val_loss: 1072.9911\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 985.9343 - val_loss: 979.9356\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 906.6486 - val_loss: 878.8334\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 833.9670 - val_loss: 776.9501\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 649.6254 - val_loss: 681.2128\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 567.8242 - val_loss: 589.2474\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 530.5135 - val_loss: 510.0402\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 458.3370 - val_loss: 442.0084\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 352.6717 - val_loss: 389.4599\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 301.2530 - val_loss: 346.7536\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 276.5491 - val_loss: 314.3748\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 304.4442 - val_loss: 288.8433\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 261.4672 - val_loss: 271.2772\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 243.5949 - val_loss: 257.1955\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 233.1490 - val_loss: 247.2265\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 215.7672 - val_loss: 239.0672\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 204.3595 - val_loss: 231.8439\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 220.3430 - val_loss: 225.3364\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 200.1286 - val_loss: 219.8964\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 218.6234 - val_loss: 215.0079\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 193.8177 - val_loss: 211.0622\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 191.8788 - val_loss: 206.7254\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 191.5800 - val_loss: 202.8711\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 188.1847 - val_loss: 199.8949\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 169.1004 - val_loss: 197.1574\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 183.7899 - val_loss: 194.0988\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 187.8620 - val_loss: 191.2888\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 183.4890 - val_loss: 188.6274\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 173.1260 - val_loss: 186.1992\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 181.0451 - val_loss: 183.4926\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 155.6373 - val_loss: 180.9380\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 158.5933 - val_loss: 179.0079\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 168.0712 - val_loss: 177.0287\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 160.8775 - val_loss: 175.0651\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 159.2202 - val_loss: 173.0419\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 151.8843 - val_loss: 171.5367\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 169.2085 - val_loss: 169.4502\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 145.2676 - val_loss: 167.9307\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 152.9289 - val_loss: 166.1223\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 167.7714 - val_loss: 164.2481\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 168.0148 - val_loss: 162.8707\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 150.9018 - val_loss: 161.2686\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 151.0420 - val_loss: 159.7234\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 154.5324 - val_loss: 158.7315\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 162.4965 - val_loss: 156.9852\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 150.3095 - val_loss: 155.6922\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 141.0871 - val_loss: 154.2824\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 155.3005 - val_loss: 153.2763\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 146.8720 - val_loss: 151.9369\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 144.1944 - val_loss: 150.9321\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 140.8633 - val_loss: 149.6919\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 148.3939 - val_loss: 148.6109\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 129.5717 - val_loss: 147.7079\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 147.6351 - val_loss: 147.0235\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 129.4260 - val_loss: 145.4477\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 141.5340 - val_loss: 144.3847\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 127.2692 - val_loss: 143.6367\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 129.1402 - val_loss: 142.9008\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 131.9972 - val_loss: 141.7798\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 136.7009 - val_loss: 141.0387\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 129.6097 - val_loss: 139.8345\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 123.1342 - val_loss: 139.1589\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 135.2257 - val_loss: 138.3119\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 136.3891 - val_loss: 137.5298\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 126.6372 - val_loss: 136.7064\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 129.6350 - val_loss: 136.1108\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 132.1918 - val_loss: 135.5158\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 128.6614 - val_loss: 134.6429\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 130.1054 - val_loss: 134.1720\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 130.9002 - val_loss: 133.2439\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 125.2697 - val_loss: 132.8639\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 120.4265 - val_loss: 132.1259\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 132.1137 - val_loss: 131.5734\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 133.3109 - val_loss: 131.3081\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 132.0122 - val_loss: 130.4930\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 122.4252 - val_loss: 129.9852\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 128.5555 - val_loss: 129.5007\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 128.3402 - val_loss: 128.8735\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 124.4027 - val_loss: 128.2062\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 120.4851 - val_loss: 127.7700\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 116.0420 - val_loss: 127.6014\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 114.8516 - val_loss: 126.8612\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 127.2826 - val_loss: 126.3902\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 120.1656 - val_loss: 126.1140\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 130.7329 - val_loss: 125.5520\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 120.0564 - val_loss: 124.9793\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 110.0153 - val_loss: 124.7589\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 125.5595 - val_loss: 124.4190\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  36 \n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 15ms/step - loss: 1586.5047 - val_loss: 1567.2035\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1517.2342 - val_loss: 1549.9164\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1543.8796 - val_loss: 1531.3572\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1573.1536 - val_loss: 1511.1868\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1566.6587 - val_loss: 1489.9260\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1517.5584 - val_loss: 1465.4479\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1581.8609 - val_loss: 1437.5061\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1481.9050 - val_loss: 1406.4485\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1339.5199 - val_loss: 1372.6399\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1324.1964 - val_loss: 1334.5386\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1399.0658 - val_loss: 1292.4509\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1319.9360 - val_loss: 1247.7660\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1243.0345 - val_loss: 1198.3192\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1185.9878 - val_loss: 1144.5531\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1185.5646 - val_loss: 1086.1339\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1127.2581 - val_loss: 1025.1072\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1018.8129 - val_loss: 960.6016\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1032.8091 - val_loss: 893.4322\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 877.0459 - val_loss: 825.0688\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 787.5374 - val_loss: 755.8370\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 779.5421 - val_loss: 685.8826\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 728.4279 - val_loss: 620.6271\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 602.7221 - val_loss: 558.4464\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 588.0671 - val_loss: 500.7414\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 537.0001 - val_loss: 449.2243\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 473.1088 - val_loss: 405.1419\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 398.5252 - val_loss: 367.7461\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 373.1186 - val_loss: 336.8165\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 351.2701 - val_loss: 310.9717\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 310.6265 - val_loss: 291.2490\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 280.3915 - val_loss: 275.5507\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 269.5872 - val_loss: 263.0970\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 264.1525 - val_loss: 253.5660\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 252.7171 - val_loss: 246.2922\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 218.1876 - val_loss: 239.8661\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 225.5351 - val_loss: 234.5035\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 219.2837 - val_loss: 229.3731\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 214.3607 - val_loss: 225.4916\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 194.0784 - val_loss: 222.1464\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 224.3859 - val_loss: 218.6809\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 197.4292 - val_loss: 215.5892\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 229.7787 - val_loss: 213.1777\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 194.6604 - val_loss: 209.7075\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 178.2838 - val_loss: 207.5440\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 202.4336 - val_loss: 204.9868\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 201.6800 - val_loss: 202.3773\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 190.3594 - val_loss: 199.4949\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 187.8941 - val_loss: 197.9093\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 184.5685 - val_loss: 195.2718\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 192.8266 - val_loss: 193.1215\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 188.8015 - val_loss: 192.3172\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 196.4680 - val_loss: 189.3859\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 162.3230 - val_loss: 186.8706\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 179.1074 - val_loss: 186.0020\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 183.9494 - val_loss: 183.8689\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 178.4500 - val_loss: 182.1115\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 170.9550 - val_loss: 180.3970\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 161.6800 - val_loss: 178.6187\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 164.6879 - val_loss: 177.6507\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 175.0717 - val_loss: 175.7971\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 165.9043 - val_loss: 174.5467\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 175.7219 - val_loss: 173.2928\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 155.2131 - val_loss: 172.5177\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 156.7776 - val_loss: 171.2151\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 165.7385 - val_loss: 169.7132\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 164.2813 - val_loss: 168.5390\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 152.5182 - val_loss: 168.2364\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 162.7017 - val_loss: 166.4379\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 152.0559 - val_loss: 165.6727\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 152.4242 - val_loss: 165.0023\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 147.1414 - val_loss: 164.7751\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 164.6280 - val_loss: 163.2306\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 159.4982 - val_loss: 162.1123\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 157.9941 - val_loss: 162.2157\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 163.9769 - val_loss: 160.8905\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 160.4367 - val_loss: 160.1719\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 156.1900 - val_loss: 159.3315\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 156.0827 - val_loss: 158.7724\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 157.1297 - val_loss: 158.1084\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 151.0558 - val_loss: 157.0019\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 140.4870 - val_loss: 157.0473\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 146.4414 - val_loss: 156.4694\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 144.3520 - val_loss: 155.3582\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 139.9448 - val_loss: 154.5076\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 144.3222 - val_loss: 153.9600\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 147.2879 - val_loss: 153.0633\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 147.1282 - val_loss: 152.0003\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 140.5264 - val_loss: 151.3758\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 142.8278 - val_loss: 150.9488\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 152.7463 - val_loss: 150.3188\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 148.0029 - val_loss: 150.0428\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 154.6862 - val_loss: 149.7119\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 129.0310 - val_loss: 148.9001\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 151.9313 - val_loss: 148.1320\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 142.1042 - val_loss: 147.4054\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 141.4493 - val_loss: 147.1120\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 136.4479 - val_loss: 147.1123\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 129.6640 - val_loss: 146.1089\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 137.8352 - val_loss: 145.7189\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 134.5745 - val_loss: 144.9308\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  37 \n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 15ms/step - loss: 1564.1403 - val_loss: 1474.1949\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1568.4812 - val_loss: 1455.8986\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1552.4382 - val_loss: 1435.3317\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1545.4729 - val_loss: 1411.6017\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1529.4621 - val_loss: 1383.4745\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1551.7229 - val_loss: 1350.7543\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1365.3719 - val_loss: 1312.6715\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1443.5563 - val_loss: 1267.4922\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1411.3079 - val_loss: 1214.8766\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1275.4592 - val_loss: 1154.1318\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1268.5465 - val_loss: 1086.2399\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1175.1084 - val_loss: 1011.4732\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1163.2445 - val_loss: 930.4628\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1050.2148 - val_loss: 846.5648\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 957.9253 - val_loss: 762.1063\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 917.9919 - val_loss: 675.4818\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 774.3881 - val_loss: 593.2419\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 695.6252 - val_loss: 516.7299\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 594.3907 - val_loss: 446.5701\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 610.0401 - val_loss: 384.6474\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 481.3744 - val_loss: 334.6831\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 416.9775 - val_loss: 294.1473\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 427.5798 - val_loss: 262.5746\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 347.4832 - val_loss: 240.0342\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 309.2292 - val_loss: 224.1595\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 258.1782 - val_loss: 214.4929\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 264.4840 - val_loss: 206.8411\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 241.2573 - val_loss: 202.0865\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 255.0612 - val_loss: 198.4038\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 221.6637 - val_loss: 195.6390\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 248.8720 - val_loss: 192.4737\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 228.7233 - val_loss: 190.5881\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 206.2387 - val_loss: 188.8652\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 222.3041 - val_loss: 186.8577\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 212.6015 - val_loss: 184.8845\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 204.1567 - val_loss: 182.7657\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 209.4871 - val_loss: 181.1125\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 176.7614 - val_loss: 179.4033\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 181.5508 - val_loss: 177.2797\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 186.2810 - val_loss: 175.5822\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 199.5705 - val_loss: 174.2496\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 168.3522 - val_loss: 173.3450\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 169.6808 - val_loss: 170.9421\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 181.9567 - val_loss: 169.5484\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 146.0264 - val_loss: 167.9037\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 170.2185 - val_loss: 166.2522\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 150.2987 - val_loss: 165.9115\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 164.0905 - val_loss: 164.3806\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 156.1669 - val_loss: 163.2883\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 166.6238 - val_loss: 162.5238\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 158.8788 - val_loss: 161.8015\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 178.9768 - val_loss: 160.9354\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 164.4492 - val_loss: 159.1991\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 162.1549 - val_loss: 158.5169\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 147.0868 - val_loss: 158.6824\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 142.0924 - val_loss: 157.9517\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 150.2528 - val_loss: 156.8779\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 145.7976 - val_loss: 156.2119\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 153.1309 - val_loss: 155.5602\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 142.6441 - val_loss: 154.7114\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 138.4448 - val_loss: 154.8617\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 153.2112 - val_loss: 153.6928\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 148.0979 - val_loss: 152.7545\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 147.2525 - val_loss: 153.5235\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 141.6876 - val_loss: 153.3729\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 137.8102 - val_loss: 151.9201\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 134.7556 - val_loss: 152.2339\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 141.6605 - val_loss: 150.8910\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 146.0171 - val_loss: 150.1528\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 129.0702 - val_loss: 150.4480\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 140.3875 - val_loss: 148.9197\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 135.9861 - val_loss: 149.2014\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 149.7351 - val_loss: 148.3096\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 134.2068 - val_loss: 148.1152\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 129.4063 - val_loss: 148.6921\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 115.8113 - val_loss: 148.2619\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 139.5738 - val_loss: 146.6385\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 128.8578 - val_loss: 145.9723\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 124.2766 - val_loss: 145.8431\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 117.5393 - val_loss: 146.3632\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 135.1229 - val_loss: 145.2473\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 124.3151 - val_loss: 145.9230\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 137.0150 - val_loss: 145.2718\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 118.9142 - val_loss: 144.8674\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 127.8602 - val_loss: 144.2273\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 124.2778 - val_loss: 143.9383\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 136.8307 - val_loss: 143.6979\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 120.7550 - val_loss: 144.4699\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 126.9511 - val_loss: 143.6026\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 121.5503 - val_loss: 143.1578\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 113.6683 - val_loss: 142.4817\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 122.4660 - val_loss: 142.7368\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 120.2588 - val_loss: 143.6266\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 120.7904 - val_loss: 142.8505\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 122.7987 - val_loss: 141.3202\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 113.0613 - val_loss: 141.1769\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 116.1926 - val_loss: 141.0179\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 120.6601 - val_loss: 140.2834\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 118.0533 - val_loss: 139.4839\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 112.7215 - val_loss: 139.9525\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  38 \n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 15ms/step - loss: 1635.1882 - val_loss: 1576.7739\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1594.1638 - val_loss: 1558.7922\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1500.2690 - val_loss: 1539.4862\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1501.6392 - val_loss: 1517.4799\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 1472.9034 - val_loss: 1491.5061\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1524.5986 - val_loss: 1459.8064\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1426.5338 - val_loss: 1421.2212\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1433.8141 - val_loss: 1375.5190\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1370.7620 - val_loss: 1321.9391\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1336.1626 - val_loss: 1260.1407\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1221.6593 - val_loss: 1188.0850\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1175.7822 - val_loss: 1107.6823\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1092.9527 - val_loss: 1017.5137\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1126.7449 - val_loss: 919.9885\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 921.9912 - val_loss: 822.5981\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 825.8317 - val_loss: 725.1221\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 701.9120 - val_loss: 634.8831\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 599.9222 - val_loss: 550.0114\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 506.2402 - val_loss: 477.7081\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 473.6063 - val_loss: 415.9547\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 417.6091 - val_loss: 368.3117\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 334.9600 - val_loss: 332.9997\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 307.2305 - val_loss: 307.8089\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 294.0216 - val_loss: 289.6191\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 241.3122 - val_loss: 276.4599\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 261.1405 - val_loss: 266.5080\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 242.0572 - val_loss: 258.9097\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 222.0637 - val_loss: 252.6976\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 228.4562 - val_loss: 247.2200\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 224.6447 - val_loss: 242.4512\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 200.5254 - val_loss: 238.2843\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 218.1514 - val_loss: 233.4264\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 210.2577 - val_loss: 229.3555\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 198.2784 - val_loss: 225.9165\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 195.4346 - val_loss: 222.4680\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 182.8951 - val_loss: 219.1335\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 191.8929 - val_loss: 216.0764\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 183.9750 - val_loss: 213.1734\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 184.2691 - val_loss: 210.1293\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 185.8504 - val_loss: 206.9178\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 175.0776 - val_loss: 204.8723\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 171.2493 - val_loss: 202.3987\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 182.1976 - val_loss: 199.9317\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 175.5703 - val_loss: 197.5624\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 180.4962 - val_loss: 195.3849\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 166.7878 - val_loss: 192.6832\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 160.7076 - val_loss: 190.8730\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 155.3694 - val_loss: 189.3947\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 148.2917 - val_loss: 187.7013\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 150.8340 - val_loss: 186.1891\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 160.6795 - val_loss: 183.8001\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 151.4303 - val_loss: 182.1180\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 144.1683 - val_loss: 180.7632\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 152.4150 - val_loss: 178.9800\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 150.4049 - val_loss: 177.7546\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 144.2547 - val_loss: 176.5166\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 149.5200 - val_loss: 175.2331\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 147.4638 - val_loss: 173.6989\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 143.6107 - val_loss: 172.6917\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 136.5810 - val_loss: 171.5498\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 137.2127 - val_loss: 170.5184\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 142.7148 - val_loss: 169.2622\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 137.4207 - val_loss: 168.5574\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 143.7726 - val_loss: 167.6628\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 141.8391 - val_loss: 166.6504\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 123.4999 - val_loss: 166.2130\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 138.0734 - val_loss: 164.7014\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 133.9208 - val_loss: 163.9655\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 132.3252 - val_loss: 163.3752\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 133.8896 - val_loss: 162.5909\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 131.0223 - val_loss: 161.9208\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 125.8928 - val_loss: 161.4229\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 128.1680 - val_loss: 160.8573\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 124.7994 - val_loss: 160.6284\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 131.5651 - val_loss: 159.4005\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 124.0343 - val_loss: 159.0842\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 135.3833 - val_loss: 158.2895\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 136.8870 - val_loss: 157.7412\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 127.7938 - val_loss: 157.4192\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 128.8508 - val_loss: 156.8702\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 127.0722 - val_loss: 156.4421\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 122.0864 - val_loss: 155.4481\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 131.8248 - val_loss: 154.8935\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 119.1591 - val_loss: 154.5592\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 120.9308 - val_loss: 154.0872\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 127.2617 - val_loss: 153.7611\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 124.1728 - val_loss: 153.2933\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 126.8003 - val_loss: 152.4753\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 117.4615 - val_loss: 152.3997\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 114.9787 - val_loss: 151.8121\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 123.6895 - val_loss: 151.1924\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 113.6412 - val_loss: 150.9061\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 117.5642 - val_loss: 150.8091\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 113.0807 - val_loss: 149.7566\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 122.6419 - val_loss: 149.0157\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 121.1241 - val_loss: 149.3709\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 118.7971 - val_loss: 148.6929\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 109.7104 - val_loss: 148.2102\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 108.8493 - val_loss: 148.2252\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 109.8042 - val_loss: 147.5219\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  39 \n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 15ms/step - loss: 1606.6891 - val_loss: 1615.7828\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1590.4437 - val_loss: 1592.9978\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1592.4248 - val_loss: 1568.3098\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1529.3129 - val_loss: 1539.9749\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1442.9173 - val_loss: 1506.7971\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1478.7766 - val_loss: 1466.8091\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1413.3916 - val_loss: 1420.2627\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1478.5483 - val_loss: 1367.4854\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1370.2854 - val_loss: 1311.4926\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1340.8273 - val_loss: 1251.0743\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1285.2286 - val_loss: 1184.8019\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1126.3580 - val_loss: 1113.1486\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1019.0328 - val_loss: 1035.0776\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1000.7364 - val_loss: 953.8243\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 932.4034 - val_loss: 872.4908\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 842.3695 - val_loss: 792.8198\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 737.8654 - val_loss: 716.5636\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 680.0138 - val_loss: 642.7271\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 596.8084 - val_loss: 576.1353\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 538.6817 - val_loss: 516.6502\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 443.2209 - val_loss: 463.6522\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 403.6582 - val_loss: 418.8888\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 383.1117 - val_loss: 380.3315\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 355.5922 - val_loss: 348.8585\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 304.5149 - val_loss: 325.1134\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 271.8174 - val_loss: 306.2196\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 283.3194 - val_loss: 291.0896\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 285.0170 - val_loss: 277.9792\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 247.4696 - val_loss: 267.7622\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 209.2961 - val_loss: 259.7162\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 217.6363 - val_loss: 252.1265\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 221.7189 - val_loss: 245.4201\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 230.5508 - val_loss: 239.9210\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 221.1967 - val_loss: 234.4773\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 219.8464 - val_loss: 229.7730\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 230.6394 - val_loss: 225.3576\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 215.6133 - val_loss: 221.6515\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 202.6599 - val_loss: 218.0732\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 192.4844 - val_loss: 214.8538\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 203.0250 - val_loss: 211.9763\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 185.2541 - val_loss: 208.8780\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 186.5860 - val_loss: 206.1619\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 208.6841 - val_loss: 203.7976\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 174.0320 - val_loss: 201.9965\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 201.8239 - val_loss: 199.1996\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 177.9151 - val_loss: 197.3336\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 201.1580 - val_loss: 195.4729\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 188.9126 - val_loss: 193.4016\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 180.9705 - val_loss: 191.3561\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 178.9681 - val_loss: 190.0780\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 198.8057 - val_loss: 187.6984\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 192.5524 - val_loss: 186.1345\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 170.4217 - val_loss: 184.6682\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 178.1970 - val_loss: 182.8861\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 179.0893 - val_loss: 181.5219\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 156.0703 - val_loss: 180.3288\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 160.5663 - val_loss: 179.0188\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 172.5244 - val_loss: 177.1073\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 175.9367 - val_loss: 175.9329\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 179.2963 - val_loss: 174.5581\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 149.9159 - val_loss: 173.1451\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 163.8340 - val_loss: 171.8294\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 156.5758 - val_loss: 170.4301\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 156.7686 - val_loss: 169.2458\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 158.5510 - val_loss: 167.8062\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 152.0095 - val_loss: 166.8738\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 148.4363 - val_loss: 165.4147\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 150.8674 - val_loss: 164.5051\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 167.1981 - val_loss: 163.2912\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 154.6629 - val_loss: 162.6756\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 148.1057 - val_loss: 161.6201\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 146.8619 - val_loss: 160.5837\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 166.9993 - val_loss: 159.8360\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 143.1175 - val_loss: 158.4737\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 160.5039 - val_loss: 158.0601\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 142.1156 - val_loss: 157.1995\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 139.9639 - val_loss: 156.4936\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 154.6528 - val_loss: 154.9786\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 139.0688 - val_loss: 154.1945\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 140.8371 - val_loss: 153.4902\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 154.0370 - val_loss: 152.7324\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 147.7328 - val_loss: 151.8962\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 149.8119 - val_loss: 151.0036\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 152.8622 - val_loss: 150.0227\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 138.2941 - val_loss: 149.6970\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 139.2125 - val_loss: 148.8836\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 132.9473 - val_loss: 148.2485\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 149.7053 - val_loss: 147.8686\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 137.1804 - val_loss: 146.8705\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 138.4000 - val_loss: 146.5387\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 145.2690 - val_loss: 145.7415\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 135.8058 - val_loss: 145.0574\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 129.5671 - val_loss: 144.3423\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 136.1061 - val_loss: 143.6108\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 145.2775 - val_loss: 143.1362\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 137.8732 - val_loss: 142.2662\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 141.8380 - val_loss: 141.9814\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 134.5453 - val_loss: 141.2366\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 138.9100 - val_loss: 140.7214\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 133.4509 - val_loss: 139.9649\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  40 \n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 24ms/step - loss: 1645.9052 - val_loss: 1467.3678\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1701.7477 - val_loss: 1455.6255\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1620.3522 - val_loss: 1444.2051\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1592.3560 - val_loss: 1432.5078\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1580.9966 - val_loss: 1419.5959\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1634.9360 - val_loss: 1404.8130\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1536.7991 - val_loss: 1387.3286\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 1574.9511 - val_loss: 1365.7679\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1545.3760 - val_loss: 1339.7823\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1468.6889 - val_loss: 1308.7118\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1433.3986 - val_loss: 1271.4346\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1418.1844 - val_loss: 1227.3735\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1385.4710 - val_loss: 1175.3401\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1288.1905 - val_loss: 1113.5559\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1252.1477 - val_loss: 1044.8624\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1131.0501 - val_loss: 971.4924\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1064.8514 - val_loss: 894.3056\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 931.8256 - val_loss: 813.6459\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 874.3172 - val_loss: 733.2665\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 797.3759 - val_loss: 654.1433\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 711.8384 - val_loss: 578.0277\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 622.1324 - val_loss: 505.8212\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 579.9129 - val_loss: 440.2991\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 527.5951 - val_loss: 383.5005\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 449.3235 - val_loss: 335.5632\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 405.6087 - val_loss: 296.0122\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 336.6509 - val_loss: 265.4131\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 312.1380 - val_loss: 240.8525\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 274.0169 - val_loss: 223.1075\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 262.3093 - val_loss: 209.8971\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 219.9431 - val_loss: 200.5228\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 226.7839 - val_loss: 193.8682\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 227.6099 - val_loss: 189.3825\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 192.9969 - val_loss: 185.1228\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 197.4611 - val_loss: 182.4583\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 186.8923 - val_loss: 179.7855\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 178.0897 - val_loss: 177.4683\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 199.3431 - val_loss: 175.5912\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 187.8442 - val_loss: 173.5338\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 192.2309 - val_loss: 172.0479\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 186.1761 - val_loss: 170.1632\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 185.9017 - val_loss: 169.0757\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 175.0744 - val_loss: 167.4996\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 169.8469 - val_loss: 166.1218\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 161.4640 - val_loss: 164.8094\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 198.3269 - val_loss: 163.3148\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 170.8499 - val_loss: 161.9406\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 174.8809 - val_loss: 160.6579\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 188.8530 - val_loss: 159.5517\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 170.8358 - val_loss: 157.9197\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 173.2694 - val_loss: 156.8875\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 188.2554 - val_loss: 155.7209\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 170.3827 - val_loss: 154.6035\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 172.5634 - val_loss: 153.2381\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 172.0122 - val_loss: 152.1646\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 162.8326 - val_loss: 151.1882\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 150.8276 - val_loss: 150.0772\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 169.4799 - val_loss: 148.8185\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 154.6603 - val_loss: 147.9246\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 147.7582 - val_loss: 146.7915\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 177.0554 - val_loss: 145.7114\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 150.6328 - val_loss: 144.7851\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 165.2221 - val_loss: 143.6740\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 163.4821 - val_loss: 143.1532\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 163.2628 - val_loss: 141.7895\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 157.8432 - val_loss: 141.1926\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 134.7394 - val_loss: 140.2377\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 142.0369 - val_loss: 139.5253\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 151.6798 - val_loss: 138.2095\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 142.5643 - val_loss: 137.5620\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 160.8352 - val_loss: 136.6742\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 144.1829 - val_loss: 135.8357\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 155.1986 - val_loss: 134.8832\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 148.2191 - val_loss: 134.0648\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 138.0400 - val_loss: 133.0872\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 144.6597 - val_loss: 132.6970\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 136.9072 - val_loss: 132.0109\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 139.8992 - val_loss: 130.8771\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 140.4983 - val_loss: 130.0711\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 137.5162 - val_loss: 130.1101\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 144.8033 - val_loss: 129.2017\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 150.8848 - val_loss: 128.2399\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 132.2598 - val_loss: 127.3390\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 130.4091 - val_loss: 126.9170\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 140.3634 - val_loss: 126.5893\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 128.4616 - val_loss: 125.6161\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 139.6846 - val_loss: 124.8724\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 137.3690 - val_loss: 124.3095\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 124.0479 - val_loss: 124.0446\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 139.7533 - val_loss: 123.2221\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 141.6985 - val_loss: 123.0475\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 129.5160 - val_loss: 122.6013\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 124.4153 - val_loss: 121.9440\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 131.9828 - val_loss: 121.6207\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 140.1437 - val_loss: 121.0532\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 126.2266 - val_loss: 120.4943\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 123.7776 - val_loss: 120.0527\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 118.0784 - val_loss: 119.5451\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 132.4035 - val_loss: 119.1453\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 132.1696 - val_loss: 118.6435\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  41 \n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 15ms/step - loss: 1562.0097 - val_loss: 1555.1434\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1608.1030 - val_loss: 1542.5417\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1542.9825 - val_loss: 1532.1877\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1598.5706 - val_loss: 1523.3790\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1599.2319 - val_loss: 1515.8357\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1595.7959 - val_loss: 1509.0817\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1595.5361 - val_loss: 1502.5762\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1565.5643 - val_loss: 1495.9265\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1606.1018 - val_loss: 1489.0297\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1551.9108 - val_loss: 1481.4281\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1505.4108 - val_loss: 1472.7693\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1638.3793 - val_loss: 1462.6096\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1537.9957 - val_loss: 1450.8175\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1561.7866 - val_loss: 1436.6011\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1444.6985 - val_loss: 1419.9568\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1445.0863 - val_loss: 1400.2433\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1415.6729 - val_loss: 1376.9957\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1470.8786 - val_loss: 1350.6472\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1357.0331 - val_loss: 1320.2970\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1366.7483 - val_loss: 1286.1444\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1321.7223 - val_loss: 1247.8326\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1302.0241 - val_loss: 1206.2169\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1259.5580 - val_loss: 1160.9900\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1255.3081 - val_loss: 1112.4954\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1136.1095 - val_loss: 1061.5283\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1072.8457 - val_loss: 1007.7308\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1008.2289 - val_loss: 951.9346\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 964.8162 - val_loss: 895.4722\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 834.7271 - val_loss: 839.4029\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 857.5043 - val_loss: 782.4912\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 757.5152 - val_loss: 726.9352\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 673.0411 - val_loss: 674.3116\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 621.7697 - val_loss: 622.6655\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 577.9828 - val_loss: 575.0830\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 586.0398 - val_loss: 530.2949\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 488.3397 - val_loss: 489.8694\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 431.3430 - val_loss: 452.6870\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 427.4725 - val_loss: 419.5458\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 420.2112 - val_loss: 391.7429\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 416.2658 - val_loss: 367.1572\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 322.8130 - val_loss: 346.2306\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 278.2951 - val_loss: 328.4925\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 294.9503 - val_loss: 312.1398\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 265.8238 - val_loss: 298.7809\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 252.2295 - val_loss: 287.3991\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 252.9188 - val_loss: 276.8705\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 263.7724 - val_loss: 267.7286\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 249.7409 - val_loss: 259.4553\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 257.7607 - val_loss: 252.3863\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 240.5196 - val_loss: 246.0730\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 239.4738 - val_loss: 240.3619\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 199.4987 - val_loss: 234.9814\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 206.2762 - val_loss: 230.2175\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 199.4040 - val_loss: 225.5842\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 199.3321 - val_loss: 221.0774\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 217.6067 - val_loss: 217.0526\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 191.6862 - val_loss: 213.4940\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 199.3693 - val_loss: 210.2483\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 188.7509 - val_loss: 207.1483\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 179.7140 - val_loss: 204.4480\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 193.4620 - val_loss: 202.0273\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 204.5437 - val_loss: 199.6984\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 165.8366 - val_loss: 197.5153\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 177.5668 - val_loss: 195.2727\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 165.1500 - val_loss: 193.7361\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 165.0174 - val_loss: 191.7387\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 177.4565 - val_loss: 189.8990\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 161.3286 - val_loss: 188.1889\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 161.6096 - val_loss: 186.4604\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 169.4026 - val_loss: 185.1490\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 177.1366 - val_loss: 183.6532\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 165.6152 - val_loss: 182.3928\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 149.6423 - val_loss: 181.3880\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 148.9946 - val_loss: 179.8617\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 157.7794 - val_loss: 178.7009\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 168.4876 - val_loss: 177.4467\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 148.4418 - val_loss: 176.5051\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 142.8005 - val_loss: 175.2815\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 163.2174 - val_loss: 174.2640\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 155.1245 - val_loss: 173.2009\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 155.7909 - val_loss: 172.2620\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 150.7756 - val_loss: 171.0468\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 155.8108 - val_loss: 170.1087\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 148.0955 - val_loss: 168.6635\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 147.1778 - val_loss: 168.1088\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 149.5091 - val_loss: 167.3460\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 144.2212 - val_loss: 166.2518\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 140.2717 - val_loss: 165.6097\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 137.6921 - val_loss: 164.8222\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 130.3628 - val_loss: 163.8555\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 141.5566 - val_loss: 163.2148\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 143.0250 - val_loss: 162.2367\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 132.8654 - val_loss: 161.8325\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 140.6252 - val_loss: 160.8293\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 139.9897 - val_loss: 160.2597\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 142.2824 - val_loss: 159.5743\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 145.1288 - val_loss: 158.9939\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 139.2369 - val_loss: 158.0700\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 137.8770 - val_loss: 157.3040\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 135.7927 - val_loss: 156.7276\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  42 \n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 15ms/step - loss: 1554.7877 - val_loss: 1468.2064\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1765.9371 - val_loss: 1449.8429\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1584.7874 - val_loss: 1430.9689\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1672.2967 - val_loss: 1410.5337\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1580.4189 - val_loss: 1387.4966\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1667.8359 - val_loss: 1360.8676\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1583.8031 - val_loss: 1330.6715\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1420.9586 - val_loss: 1296.0150\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1413.5933 - val_loss: 1256.3558\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1460.4689 - val_loss: 1210.1981\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1381.2425 - val_loss: 1156.6664\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1280.8193 - val_loss: 1097.2277\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1188.8298 - val_loss: 1032.0598\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1132.5962 - val_loss: 960.7547\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1115.3481 - val_loss: 885.1988\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1052.8352 - val_loss: 807.0626\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 930.7276 - val_loss: 729.1311\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 773.9392 - val_loss: 651.0458\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 668.3575 - val_loss: 576.0878\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 653.1758 - val_loss: 505.3019\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 522.4906 - val_loss: 442.3770\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 458.6921 - val_loss: 387.7874\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 367.9175 - val_loss: 340.2679\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 376.4286 - val_loss: 299.2265\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 318.4446 - val_loss: 266.7738\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 287.0936 - val_loss: 241.1731\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 265.2738 - val_loss: 221.4710\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 268.6114 - val_loss: 206.9259\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 244.3454 - val_loss: 194.2076\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 227.1832 - val_loss: 185.4365\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 224.5598 - val_loss: 178.0733\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 190.3755 - val_loss: 172.3689\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 210.8196 - val_loss: 167.8391\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 209.0240 - val_loss: 164.1170\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 205.0445 - val_loss: 160.5795\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 206.0494 - val_loss: 157.5621\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 192.9386 - val_loss: 155.2801\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 202.0591 - val_loss: 152.9210\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 203.4846 - val_loss: 151.0716\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 189.1800 - val_loss: 148.9670\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 164.0612 - val_loss: 147.4572\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 184.9818 - val_loss: 145.7442\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 173.2902 - val_loss: 144.1199\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 168.9313 - val_loss: 142.9225\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 167.3985 - val_loss: 141.9815\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 163.6068 - val_loss: 140.9063\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 165.2824 - val_loss: 139.7854\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 165.6193 - val_loss: 138.3445\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 162.9619 - val_loss: 137.4482\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 159.0381 - val_loss: 136.7120\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 156.2713 - val_loss: 135.8313\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 185.2477 - val_loss: 135.0032\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 180.3733 - val_loss: 134.1908\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 152.2127 - val_loss: 133.2913\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 149.3262 - val_loss: 132.5453\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 164.5289 - val_loss: 131.9651\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 155.6307 - val_loss: 131.1382\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 159.8108 - val_loss: 130.4358\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 151.5329 - val_loss: 129.7810\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 149.6297 - val_loss: 129.2592\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 156.6555 - val_loss: 128.6982\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 139.3379 - val_loss: 127.9408\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 153.2033 - val_loss: 127.6193\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 147.6456 - val_loss: 127.2880\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 149.5879 - val_loss: 126.7718\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 149.9479 - val_loss: 126.2875\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 150.7259 - val_loss: 125.7885\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 142.8102 - val_loss: 125.3394\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 160.5787 - val_loss: 125.0660\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 149.7330 - val_loss: 124.6716\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 146.5775 - val_loss: 124.2985\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 144.9035 - val_loss: 123.7064\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 139.5587 - val_loss: 123.4515\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 152.4977 - val_loss: 123.2138\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 134.1208 - val_loss: 122.8133\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 143.7804 - val_loss: 122.4178\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 141.2374 - val_loss: 122.0700\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 146.2923 - val_loss: 121.8803\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 153.8825 - val_loss: 121.7232\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 129.9024 - val_loss: 121.0988\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 140.9341 - val_loss: 120.6381\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 134.7165 - val_loss: 120.3145\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 128.4946 - val_loss: 120.1241\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 145.9869 - val_loss: 120.1051\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 141.0508 - val_loss: 119.7207\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 132.5243 - val_loss: 119.3751\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 149.5196 - val_loss: 119.2411\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 134.6089 - val_loss: 119.0411\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 130.5749 - val_loss: 118.6636\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 124.6692 - val_loss: 118.2556\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 129.6973 - val_loss: 117.9094\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 116.7993 - val_loss: 117.8987\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 113.6049 - val_loss: 117.8443\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 135.2674 - val_loss: 117.4232\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 132.4733 - val_loss: 116.9505\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 123.4601 - val_loss: 116.8583\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 127.7495 - val_loss: 116.4121\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 124.1717 - val_loss: 116.0096\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 119.3807 - val_loss: 116.2631\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 123.1935 - val_loss: 116.1267\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  43 \n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 15ms/step - loss: 1581.1248 - val_loss: 1575.2183\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1450.2653 - val_loss: 1562.6769\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1421.0723 - val_loss: 1548.1793\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1589.6777 - val_loss: 1531.6451\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1482.2343 - val_loss: 1513.5471\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1477.8681 - val_loss: 1493.1233\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1436.5625 - val_loss: 1469.1199\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1429.1357 - val_loss: 1441.9336\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1335.6701 - val_loss: 1410.2379\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1397.9202 - val_loss: 1373.2510\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1366.5862 - val_loss: 1331.1852\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1286.7108 - val_loss: 1284.3507\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1232.4386 - val_loss: 1232.2446\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1233.0758 - val_loss: 1172.1191\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1186.7969 - val_loss: 1104.4762\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1130.8138 - val_loss: 1027.0479\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 929.1938 - val_loss: 946.2849\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 941.0327 - val_loss: 859.5371\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 908.0573 - val_loss: 772.5385\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 783.9372 - val_loss: 687.8708\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 747.8494 - val_loss: 605.0710\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 599.9296 - val_loss: 526.9884\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 478.4782 - val_loss: 459.5357\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 433.5823 - val_loss: 399.6410\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 400.3677 - val_loss: 353.5052\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 298.1140 - val_loss: 319.1899\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 322.1827 - val_loss: 292.8629\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 270.8649 - val_loss: 274.9389\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 259.9847 - val_loss: 262.6935\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 208.9985 - val_loss: 254.0111\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 203.6800 - val_loss: 248.5945\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 196.8766 - val_loss: 243.8647\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 193.3051 - val_loss: 239.5587\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 200.1586 - val_loss: 235.4435\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 195.3068 - val_loss: 231.6334\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 158.5077 - val_loss: 228.9378\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 169.7593 - val_loss: 226.1655\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 177.3385 - val_loss: 222.5563\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 185.2966 - val_loss: 219.7181\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 176.1956 - val_loss: 216.4395\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 171.6124 - val_loss: 213.5289\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 164.1469 - val_loss: 210.8667\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 157.5429 - val_loss: 209.8326\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 155.0821 - val_loss: 206.3584\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 156.8361 - val_loss: 203.7044\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 161.3139 - val_loss: 202.8419\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 150.0405 - val_loss: 199.9893\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 159.7516 - val_loss: 197.1033\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 137.4528 - val_loss: 195.8004\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 142.4769 - val_loss: 193.7762\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 149.2887 - val_loss: 191.8814\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 156.6960 - val_loss: 189.6798\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 145.9316 - val_loss: 188.0645\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 146.6939 - val_loss: 186.6924\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 153.6262 - val_loss: 184.9347\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 146.1887 - val_loss: 183.8433\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 140.1350 - val_loss: 181.6672\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 135.2907 - val_loss: 180.3049\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 134.1187 - val_loss: 179.1882\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 140.0296 - val_loss: 177.3558\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 136.5924 - val_loss: 176.3513\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 137.3092 - val_loss: 175.8968\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 129.2281 - val_loss: 174.4865\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 130.3739 - val_loss: 172.6723\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 126.9945 - val_loss: 172.0351\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 135.3820 - val_loss: 170.4747\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 133.5559 - val_loss: 169.5337\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 127.7443 - val_loss: 169.3905\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 124.8124 - val_loss: 168.5913\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 134.8449 - val_loss: 166.5555\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 124.6519 - val_loss: 166.7394\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 122.9733 - val_loss: 164.6626\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 120.4573 - val_loss: 164.3055\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 121.6485 - val_loss: 163.7090\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 112.8615 - val_loss: 162.4466\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 126.6013 - val_loss: 161.7331\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 126.8698 - val_loss: 161.2388\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 117.9309 - val_loss: 160.6598\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 122.5824 - val_loss: 159.9022\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 118.3864 - val_loss: 159.5205\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 114.4079 - val_loss: 158.4799\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 114.8628 - val_loss: 158.1821\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 124.4313 - val_loss: 157.3598\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 119.4571 - val_loss: 156.6488\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 121.2470 - val_loss: 155.8876\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 106.1940 - val_loss: 156.2908\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 125.6955 - val_loss: 155.0688\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 113.0518 - val_loss: 154.3733\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 111.1004 - val_loss: 153.8538\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 116.1421 - val_loss: 153.6588\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 107.8451 - val_loss: 153.4225\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 110.9854 - val_loss: 153.0768\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 108.9628 - val_loss: 152.2859\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 123.2048 - val_loss: 151.9154\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 107.8644 - val_loss: 152.6947\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 107.6619 - val_loss: 151.1119\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 112.6808 - val_loss: 151.4277\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 112.5333 - val_loss: 150.5399\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 104.8432 - val_loss: 150.6267\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 118.9615 - val_loss: 150.0009\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  44 \n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 15ms/step - loss: 1540.1907 - val_loss: 1684.5730\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1520.1758 - val_loss: 1664.5530\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1564.6574 - val_loss: 1645.8230\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 1521.7052 - val_loss: 1626.0599\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1514.5365 - val_loss: 1604.4409\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1415.6099 - val_loss: 1579.4360\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1400.6086 - val_loss: 1549.9617\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1490.3054 - val_loss: 1516.3138\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1408.4792 - val_loss: 1477.5267\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1196.6153 - val_loss: 1433.3335\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1337.0891 - val_loss: 1380.3921\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1253.2872 - val_loss: 1322.9202\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1170.7474 - val_loss: 1255.1869\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1077.8385 - val_loss: 1180.6023\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1023.8880 - val_loss: 1097.7445\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 976.8564 - val_loss: 1009.4342\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 850.9771 - val_loss: 919.3529\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 744.2911 - val_loss: 826.3798\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 737.2872 - val_loss: 734.0975\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 604.2159 - val_loss: 649.8445\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 538.5046 - val_loss: 570.7722\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 460.8323 - val_loss: 500.9295\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 386.9236 - val_loss: 442.2599\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 344.8234 - val_loss: 392.1208\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 305.8902 - val_loss: 353.1294\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 286.6809 - val_loss: 321.6921\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 262.8033 - val_loss: 296.5923\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 257.7139 - val_loss: 278.8508\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 253.2055 - val_loss: 264.8865\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 216.5110 - val_loss: 254.4609\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 215.0513 - val_loss: 246.2019\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 212.5021 - val_loss: 238.8617\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 218.3788 - val_loss: 232.7948\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 212.0320 - val_loss: 227.3596\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 193.4334 - val_loss: 223.1651\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 197.5177 - val_loss: 218.9669\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 190.0124 - val_loss: 215.2242\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 204.0919 - val_loss: 211.6531\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 184.2364 - val_loss: 208.5035\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 178.1942 - val_loss: 205.8595\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 164.2344 - val_loss: 203.3044\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 162.9709 - val_loss: 200.7442\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 183.8304 - val_loss: 198.1424\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 172.2927 - val_loss: 196.0785\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 172.8868 - val_loss: 194.3313\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 163.8552 - val_loss: 192.5109\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 161.5294 - val_loss: 190.5569\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 165.4837 - val_loss: 188.9829\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 153.0760 - val_loss: 187.4660\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 170.6067 - val_loss: 185.8130\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 165.6077 - val_loss: 184.6024\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 160.9777 - val_loss: 183.0901\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 159.6506 - val_loss: 182.0738\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 157.6350 - val_loss: 180.7416\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 165.7822 - val_loss: 179.8959\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 143.0901 - val_loss: 178.7617\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 148.4063 - val_loss: 177.8492\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 154.0958 - val_loss: 176.7798\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 150.6231 - val_loss: 175.9544\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 156.3853 - val_loss: 175.1553\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 152.3260 - val_loss: 174.3508\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 147.9059 - val_loss: 173.3091\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 154.6985 - val_loss: 172.5295\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 150.9844 - val_loss: 171.5469\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 133.2498 - val_loss: 171.1248\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 143.0885 - val_loss: 170.1987\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 153.9034 - val_loss: 169.8106\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 148.1448 - val_loss: 168.9798\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 163.0372 - val_loss: 168.1133\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 146.6121 - val_loss: 167.2892\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 140.9774 - val_loss: 166.8236\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 152.7677 - val_loss: 166.2830\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 139.5995 - val_loss: 165.8190\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 140.9697 - val_loss: 165.0589\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 127.1434 - val_loss: 164.6281\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 139.9924 - val_loss: 163.9513\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 145.6747 - val_loss: 163.5308\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 129.9574 - val_loss: 162.9200\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 136.6395 - val_loss: 162.3445\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 137.1072 - val_loss: 161.7347\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 136.0410 - val_loss: 161.1987\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 141.1486 - val_loss: 160.8047\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 131.1468 - val_loss: 160.1521\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 138.6217 - val_loss: 159.8992\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 134.2081 - val_loss: 159.1571\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 132.2966 - val_loss: 158.6665\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 132.3171 - val_loss: 158.3690\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 137.0964 - val_loss: 157.5793\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 133.8374 - val_loss: 157.0905\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 133.0366 - val_loss: 156.6759\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 138.3429 - val_loss: 155.6553\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 137.0931 - val_loss: 155.3354\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 140.7536 - val_loss: 155.0061\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 125.3781 - val_loss: 154.3088\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 126.6159 - val_loss: 153.9502\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 132.7882 - val_loss: 153.2431\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 135.4992 - val_loss: 153.0438\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 142.9828 - val_loss: 152.5512\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 134.1391 - val_loss: 152.2032\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 132.9898 - val_loss: 151.7711\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  45 \n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 15ms/step - loss: 1595.8721 - val_loss: 1632.8424\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1502.7845 - val_loss: 1620.1517\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1556.3211 - val_loss: 1607.8984\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1525.1819 - val_loss: 1595.9591\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1481.3578 - val_loss: 1583.8663\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1560.1674 - val_loss: 1571.0322\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1523.4215 - val_loss: 1555.9890\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 1569.1868 - val_loss: 1538.5387\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1508.0808 - val_loss: 1517.8074\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1400.5684 - val_loss: 1492.8912\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1351.9879 - val_loss: 1462.9011\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1423.2320 - val_loss: 1425.3691\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1433.8660 - val_loss: 1378.5364\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1262.5392 - val_loss: 1321.4503\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1243.4575 - val_loss: 1252.1195\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1137.3721 - val_loss: 1171.7664\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1149.7512 - val_loss: 1080.3671\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1086.7707 - val_loss: 982.3913\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 879.3947 - val_loss: 883.6758\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 814.3666 - val_loss: 782.2223\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 733.6609 - val_loss: 682.4124\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 683.3653 - val_loss: 587.4402\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 529.4052 - val_loss: 505.7102\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 482.9634 - val_loss: 432.6782\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 411.6428 - val_loss: 369.9270\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 367.8859 - val_loss: 321.1762\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 324.3378 - val_loss: 283.3478\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 281.1127 - val_loss: 254.0665\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 282.7828 - val_loss: 233.0188\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 256.1233 - val_loss: 217.9931\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 221.2806 - val_loss: 206.8121\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 227.7400 - val_loss: 198.3412\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 224.8485 - val_loss: 190.7164\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 206.1823 - val_loss: 185.8546\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 214.2991 - val_loss: 181.6180\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 204.7147 - val_loss: 179.0776\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 200.2260 - val_loss: 175.7676\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 197.5140 - val_loss: 173.4732\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 192.8075 - val_loss: 170.9803\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 191.8801 - val_loss: 168.6119\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 186.4958 - val_loss: 167.1070\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 188.4005 - val_loss: 165.0498\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 184.0213 - val_loss: 163.5210\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 160.3157 - val_loss: 162.4943\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 163.3964 - val_loss: 160.7528\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 194.8198 - val_loss: 159.4131\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 156.4746 - val_loss: 158.3506\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 167.3538 - val_loss: 157.2111\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 169.6973 - val_loss: 156.1638\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 167.8121 - val_loss: 155.2737\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 164.3075 - val_loss: 154.6914\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 160.8044 - val_loss: 153.7030\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 163.2360 - val_loss: 152.6855\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 140.6463 - val_loss: 151.4855\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 153.5735 - val_loss: 150.9628\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 157.4783 - val_loss: 150.2200\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 157.7298 - val_loss: 149.4460\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 148.3390 - val_loss: 148.4570\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 147.6850 - val_loss: 147.9708\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 163.2350 - val_loss: 147.1905\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 155.5383 - val_loss: 146.6289\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 135.0045 - val_loss: 145.8321\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 145.5393 - val_loss: 145.1660\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 166.4761 - val_loss: 144.6017\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 174.0062 - val_loss: 143.9885\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 139.9845 - val_loss: 143.4003\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 139.6143 - val_loss: 142.7585\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 134.4681 - val_loss: 142.0578\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 146.5029 - val_loss: 141.6641\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 156.3456 - val_loss: 140.8157\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 146.5959 - val_loss: 140.5219\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 156.7271 - val_loss: 139.8066\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 145.8373 - val_loss: 139.8946\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 150.3288 - val_loss: 138.9308\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 134.4954 - val_loss: 138.4716\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 140.2686 - val_loss: 138.1108\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 152.5175 - val_loss: 137.4659\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 141.9559 - val_loss: 137.1770\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 147.8957 - val_loss: 136.7910\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 148.7722 - val_loss: 136.3221\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 136.4334 - val_loss: 135.9276\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 144.4426 - val_loss: 135.5467\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 132.9351 - val_loss: 135.3009\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 135.1148 - val_loss: 134.8702\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 131.0394 - val_loss: 134.2210\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 133.6966 - val_loss: 133.9945\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 137.6259 - val_loss: 134.1173\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 138.5243 - val_loss: 133.5606\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 138.3430 - val_loss: 133.1081\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 150.4714 - val_loss: 133.0504\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 132.7065 - val_loss: 132.6564\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 124.6561 - val_loss: 132.3454\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 136.0611 - val_loss: 132.2382\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 123.6867 - val_loss: 132.0812\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 131.1546 - val_loss: 131.6292\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 137.3004 - val_loss: 131.4156\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 124.3158 - val_loss: 131.2417\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 131.7060 - val_loss: 131.0690\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 135.8373 - val_loss: 130.4716\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 134.4076 - val_loss: 130.0396\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  46 \n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 14ms/step - loss: 1550.7441 - val_loss: 1605.4679\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1524.3627 - val_loss: 1580.8633\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1494.7569 - val_loss: 1557.5479\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1565.6003 - val_loss: 1533.7874\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1483.6010 - val_loss: 1508.1079\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1428.7394 - val_loss: 1480.5071\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1460.0796 - val_loss: 1449.5764\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1398.3920 - val_loss: 1415.3131\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1417.8450 - val_loss: 1376.8419\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1249.6601 - val_loss: 1334.4960\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1181.3655 - val_loss: 1288.4492\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1286.4499 - val_loss: 1238.7902\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1128.0580 - val_loss: 1186.5781\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1145.3736 - val_loss: 1132.0619\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 998.5799 - val_loss: 1075.9397\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 976.1569 - val_loss: 1017.3967\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 898.2045 - val_loss: 959.6033\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 943.8955 - val_loss: 900.9180\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 846.7640 - val_loss: 843.4695\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 778.1981 - val_loss: 787.7860\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 769.1757 - val_loss: 734.0804\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 644.1382 - val_loss: 683.9769\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 643.9176 - val_loss: 635.9162\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 594.2553 - val_loss: 591.3287\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 516.0054 - val_loss: 550.2924\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 461.6818 - val_loss: 513.5140\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 431.2221 - val_loss: 479.3089\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 409.8010 - val_loss: 447.6657\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 379.4920 - val_loss: 419.5313\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 333.9455 - val_loss: 394.8322\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 331.3658 - val_loss: 372.3014\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 266.6900 - val_loss: 353.4150\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 266.2138 - val_loss: 335.7827\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 304.3321 - val_loss: 319.9229\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 251.7435 - val_loss: 307.0249\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 248.3456 - val_loss: 295.7547\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 249.0110 - val_loss: 285.0281\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 209.9306 - val_loss: 276.3218\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 215.4172 - val_loss: 268.1969\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 219.6558 - val_loss: 259.6129\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 201.4603 - val_loss: 253.0967\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 197.5968 - val_loss: 246.9087\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 200.0994 - val_loss: 241.2635\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 195.7907 - val_loss: 236.5335\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 186.1874 - val_loss: 231.2809\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 191.5194 - val_loss: 226.8798\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 170.2243 - val_loss: 222.8213\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 182.8778 - val_loss: 218.5968\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 177.6936 - val_loss: 215.6247\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 175.6497 - val_loss: 212.4369\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 166.0919 - val_loss: 209.0675\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 171.7885 - val_loss: 206.0386\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 172.9586 - val_loss: 203.1910\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 157.7954 - val_loss: 200.8114\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 155.4399 - val_loss: 198.2548\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 174.3817 - val_loss: 196.8758\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 166.8182 - val_loss: 194.6757\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 164.4673 - val_loss: 192.2905\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 172.3418 - val_loss: 190.7301\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 141.7145 - val_loss: 189.1739\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 161.5045 - val_loss: 187.6880\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 142.1457 - val_loss: 186.4939\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 171.5098 - val_loss: 185.1965\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 145.1656 - val_loss: 183.7439\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 153.6716 - val_loss: 182.0331\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 143.7399 - val_loss: 180.9175\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 147.1688 - val_loss: 179.8744\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 136.8912 - val_loss: 178.7704\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 160.6048 - val_loss: 177.4359\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 138.8907 - val_loss: 176.6081\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 138.1198 - val_loss: 175.3450\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 137.7574 - val_loss: 173.9994\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 133.7444 - val_loss: 173.0896\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 133.4534 - val_loss: 172.7389\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 137.1849 - val_loss: 171.1551\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 133.3444 - val_loss: 170.5719\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 138.7402 - val_loss: 169.7620\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 134.9884 - val_loss: 169.3714\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 131.2166 - val_loss: 168.2722\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 129.6954 - val_loss: 167.3975\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 132.4016 - val_loss: 166.0707\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 133.0105 - val_loss: 165.2754\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 130.2855 - val_loss: 164.4242\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 129.4882 - val_loss: 163.6058\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 124.5764 - val_loss: 162.7735\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 124.3426 - val_loss: 161.5057\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 116.4659 - val_loss: 160.7559\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 113.5682 - val_loss: 160.2639\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 117.5956 - val_loss: 159.4391\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 117.3687 - val_loss: 158.6128\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 113.9589 - val_loss: 158.4046\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 124.6821 - val_loss: 157.3714\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 119.0623 - val_loss: 156.9368\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 117.0597 - val_loss: 155.9410\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 115.6478 - val_loss: 155.2722\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 112.3331 - val_loss: 154.4783\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 113.1289 - val_loss: 155.0517\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 106.2467 - val_loss: 154.2423\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 114.8472 - val_loss: 152.6829\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 108.6704 - val_loss: 152.3750\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  47 \n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 15ms/step - loss: 1633.6263 - val_loss: 1498.5569\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1643.3169 - val_loss: 1483.4871\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1598.6129 - val_loss: 1468.2893\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1545.5939 - val_loss: 1453.7059\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1539.8237 - val_loss: 1437.7156\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1557.8280 - val_loss: 1419.9773\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1550.7280 - val_loss: 1398.3593\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1531.1720 - val_loss: 1372.2086\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1517.4024 - val_loss: 1341.4474\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1459.2665 - val_loss: 1306.2340\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1448.7576 - val_loss: 1267.4592\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1430.8826 - val_loss: 1222.2196\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1293.7539 - val_loss: 1173.1580\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1227.8447 - val_loss: 1118.0537\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1215.3082 - val_loss: 1056.0957\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1155.0716 - val_loss: 989.2941\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1090.7005 - val_loss: 917.4651\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 933.3018 - val_loss: 844.6146\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 908.2543 - val_loss: 768.9659\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 875.8958 - val_loss: 694.7176\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 744.5935 - val_loss: 623.7800\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 601.2833 - val_loss: 557.1324\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 551.3807 - val_loss: 496.3655\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 502.8073 - val_loss: 443.0954\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 450.2785 - val_loss: 397.0629\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 360.9945 - val_loss: 359.7174\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 357.2904 - val_loss: 329.7765\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 306.5090 - val_loss: 307.2640\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 275.4437 - val_loss: 289.8674\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 261.8754 - val_loss: 276.1480\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 248.3903 - val_loss: 265.1724\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 220.1189 - val_loss: 256.9369\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 223.3954 - val_loss: 249.9453\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 223.4420 - val_loss: 243.6318\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 213.3272 - val_loss: 237.9873\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 198.4125 - val_loss: 232.8049\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 185.7875 - val_loss: 227.9138\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 202.6076 - val_loss: 223.2857\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 189.5733 - val_loss: 219.0935\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 170.4080 - val_loss: 214.7905\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 172.2090 - val_loss: 210.8730\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 175.1201 - val_loss: 207.5176\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 166.0870 - val_loss: 204.6647\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 165.2346 - val_loss: 201.7289\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 176.7246 - val_loss: 198.4261\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 167.8125 - val_loss: 196.3118\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 158.6344 - val_loss: 194.0027\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 155.4984 - val_loss: 191.7234\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 143.5994 - val_loss: 189.0202\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 149.2821 - val_loss: 187.4516\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 175.7303 - val_loss: 185.4013\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 136.2656 - val_loss: 184.2248\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 143.8264 - val_loss: 182.5728\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 157.5288 - val_loss: 181.6283\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 145.8505 - val_loss: 179.8747\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 150.1134 - val_loss: 178.6117\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 149.5917 - val_loss: 177.2715\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 145.6830 - val_loss: 176.2681\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 140.3172 - val_loss: 175.0471\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 145.8513 - val_loss: 173.9393\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 139.5231 - val_loss: 172.9352\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 139.0019 - val_loss: 172.0212\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 156.3998 - val_loss: 171.0359\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 138.5658 - val_loss: 170.5705\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 128.1247 - val_loss: 169.8207\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 127.5385 - val_loss: 169.0480\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 146.5522 - val_loss: 168.0784\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 146.6710 - val_loss: 167.3968\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 133.4333 - val_loss: 166.7754\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 120.2518 - val_loss: 166.5494\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 142.9219 - val_loss: 165.3950\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 140.9022 - val_loss: 165.3528\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 138.7346 - val_loss: 164.8475\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 136.0314 - val_loss: 163.9304\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 135.7891 - val_loss: 163.5169\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 128.3572 - val_loss: 162.7945\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 122.2720 - val_loss: 162.1084\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 129.2826 - val_loss: 161.7186\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 123.1145 - val_loss: 160.8284\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 132.6259 - val_loss: 160.2171\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 126.0410 - val_loss: 159.4588\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 132.4196 - val_loss: 159.6281\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 123.9719 - val_loss: 158.8880\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 126.6332 - val_loss: 157.8512\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 124.5891 - val_loss: 157.1116\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 136.3834 - val_loss: 156.8254\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 127.1783 - val_loss: 156.2116\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 120.9911 - val_loss: 155.9662\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 130.0446 - val_loss: 155.1996\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 120.2299 - val_loss: 155.2077\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 115.0732 - val_loss: 154.0605\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 129.2481 - val_loss: 153.2414\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 122.2090 - val_loss: 152.9495\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 120.7947 - val_loss: 152.7614\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 119.7332 - val_loss: 152.4732\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 120.9606 - val_loss: 151.6029\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 123.4091 - val_loss: 151.6341\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 122.1114 - val_loss: 150.9672\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 122.7242 - val_loss: 150.3612\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 127.3289 - val_loss: 150.0294\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  48 \n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 14ms/step - loss: 1517.2832 - val_loss: 1630.1869\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1615.7411 - val_loss: 1619.7975\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1491.8653 - val_loss: 1610.1296\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1467.8719 - val_loss: 1600.4108\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 1477.3331 - val_loss: 1590.0854\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1578.5469 - val_loss: 1578.2675\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1468.7564 - val_loss: 1564.6200\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1461.8262 - val_loss: 1547.1394\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1487.4460 - val_loss: 1525.7177\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1435.8943 - val_loss: 1500.3496\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1400.5015 - val_loss: 1470.0282\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1367.7897 - val_loss: 1434.0184\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1322.8795 - val_loss: 1391.3876\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1360.4906 - val_loss: 1340.6252\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1244.4548 - val_loss: 1283.0956\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1159.6674 - val_loss: 1217.3619\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1124.1108 - val_loss: 1144.1990\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1109.1343 - val_loss: 1064.5724\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 926.2875 - val_loss: 980.8377\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 930.6124 - val_loss: 892.6082\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 753.7729 - val_loss: 805.4540\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 712.9444 - val_loss: 719.5706\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 645.0260 - val_loss: 636.2305\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 586.4595 - val_loss: 558.8268\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 540.3840 - val_loss: 487.6770\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 493.2647 - val_loss: 424.7070\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 376.7275 - val_loss: 371.3640\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 345.7783 - val_loss: 324.8383\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 332.2534 - val_loss: 287.3150\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 292.7052 - val_loss: 257.7951\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 264.1825 - val_loss: 234.2814\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 228.5681 - val_loss: 217.5544\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 215.5320 - val_loss: 204.4537\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 201.6372 - val_loss: 194.5119\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 208.7787 - val_loss: 188.0692\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 191.9857 - val_loss: 182.3474\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 198.0316 - val_loss: 177.9109\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 183.9688 - val_loss: 174.0995\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 174.9062 - val_loss: 171.1609\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 199.5211 - val_loss: 168.5395\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 172.7976 - val_loss: 166.4789\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 167.2447 - val_loss: 163.9075\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 187.0198 - val_loss: 161.8669\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 183.4350 - val_loss: 159.9674\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 164.1221 - val_loss: 158.4668\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 178.2433 - val_loss: 156.7304\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 165.5906 - val_loss: 155.0733\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 165.6550 - val_loss: 153.7069\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 157.7985 - val_loss: 152.2281\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 163.8581 - val_loss: 150.7673\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 183.4346 - val_loss: 149.3469\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 158.8159 - val_loss: 148.0973\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 165.5794 - val_loss: 147.2901\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 159.0395 - val_loss: 146.0705\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 153.6788 - val_loss: 144.9950\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 159.8415 - val_loss: 143.7894\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 154.2909 - val_loss: 142.7001\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 159.6401 - val_loss: 141.6147\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 154.1685 - val_loss: 140.8108\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 151.5308 - val_loss: 140.1945\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 157.9963 - val_loss: 139.4222\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 155.9144 - val_loss: 138.3546\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 155.1048 - val_loss: 137.5557\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 155.5501 - val_loss: 137.0543\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 152.7824 - val_loss: 136.4755\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 143.8372 - val_loss: 135.5770\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 143.8724 - val_loss: 134.8982\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 144.6743 - val_loss: 134.5095\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 145.3606 - val_loss: 133.7237\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 139.3522 - val_loss: 133.0575\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 137.1198 - val_loss: 132.6464\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 135.3542 - val_loss: 132.5008\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 142.7246 - val_loss: 131.8780\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 146.3812 - val_loss: 131.3799\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 141.3068 - val_loss: 130.9224\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 130.6188 - val_loss: 130.6395\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 140.1041 - val_loss: 130.2459\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 132.4512 - val_loss: 129.6276\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 141.4596 - val_loss: 128.9823\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 136.5827 - val_loss: 128.9490\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 135.4304 - val_loss: 128.4718\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 134.9768 - val_loss: 128.2582\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 132.9748 - val_loss: 128.0435\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 145.1240 - val_loss: 127.3456\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 130.2650 - val_loss: 127.1661\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 140.4362 - val_loss: 127.0357\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 127.7161 - val_loss: 126.8586\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 138.2804 - val_loss: 126.3351\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 124.3517 - val_loss: 126.0617\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 134.2307 - val_loss: 125.8170\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 118.5838 - val_loss: 125.3237\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 132.7335 - val_loss: 125.1650\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 132.2245 - val_loss: 124.9780\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 132.5741 - val_loss: 124.5596\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 127.9900 - val_loss: 124.2619\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 129.9477 - val_loss: 124.0092\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 120.4765 - val_loss: 123.7587\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 121.7443 - val_loss: 123.4838\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 132.0333 - val_loss: 123.3872\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 124.9525 - val_loss: 123.2632\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  49 \n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 15ms/step - loss: 1627.7175 - val_loss: 1589.5353\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1593.2002 - val_loss: 1576.9865\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1561.0956 - val_loss: 1565.2666\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1567.9758 - val_loss: 1553.9823\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1515.5661 - val_loss: 1542.4861\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1539.8867 - val_loss: 1529.7755\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1467.1239 - val_loss: 1515.4357\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1488.1717 - val_loss: 1497.2911\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1463.9207 - val_loss: 1475.1281\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1418.9871 - val_loss: 1448.6725\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1461.1995 - val_loss: 1416.6353\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1349.4048 - val_loss: 1379.0131\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1273.6274 - val_loss: 1334.6105\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1214.4823 - val_loss: 1283.1395\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1243.7376 - val_loss: 1225.1993\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1107.7438 - val_loss: 1161.3324\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1149.8971 - val_loss: 1092.5178\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1007.2356 - val_loss: 1020.3977\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 950.0450 - val_loss: 944.6103\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 825.0092 - val_loss: 869.4210\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 790.4882 - val_loss: 791.5045\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 727.6453 - val_loss: 715.4848\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 654.4924 - val_loss: 644.1649\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 600.5696 - val_loss: 574.5669\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 503.2136 - val_loss: 511.9567\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 419.4989 - val_loss: 457.2721\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 370.6034 - val_loss: 410.0892\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 326.3421 - val_loss: 369.7035\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 301.6383 - val_loss: 337.4366\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 246.0747 - val_loss: 312.6283\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 243.5523 - val_loss: 292.4324\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 223.9960 - val_loss: 276.4443\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 239.4791 - val_loss: 265.3088\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 203.2060 - val_loss: 256.9367\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 201.8989 - val_loss: 250.2182\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 208.3843 - val_loss: 244.4564\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 177.1934 - val_loss: 239.8380\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 182.0640 - val_loss: 235.4286\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 189.6363 - val_loss: 231.8929\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 179.8370 - val_loss: 228.4725\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 173.6718 - val_loss: 225.4106\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 178.5166 - val_loss: 222.5598\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 192.9161 - val_loss: 220.1714\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 175.7622 - val_loss: 217.6762\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 178.4282 - val_loss: 215.4387\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 167.1108 - val_loss: 212.7053\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 170.2901 - val_loss: 210.8003\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 158.1538 - val_loss: 208.2005\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 168.6417 - val_loss: 205.9612\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 160.8425 - val_loss: 204.8133\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 151.1774 - val_loss: 202.3109\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 157.6288 - val_loss: 200.2677\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 148.3231 - val_loss: 198.4771\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 159.7121 - val_loss: 196.3823\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 165.5765 - val_loss: 194.9582\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 152.2287 - val_loss: 193.2932\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 146.7866 - val_loss: 191.7656\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 166.3128 - val_loss: 189.7376\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 151.1961 - val_loss: 188.6259\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 139.8340 - val_loss: 187.5987\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 148.0706 - val_loss: 186.1639\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 151.0984 - val_loss: 184.7982\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 153.8134 - val_loss: 183.1532\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 146.9339 - val_loss: 182.0217\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 146.1315 - val_loss: 181.0005\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 144.8759 - val_loss: 180.0109\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 146.9574 - val_loss: 178.5486\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 142.2371 - val_loss: 177.2095\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 148.1445 - val_loss: 176.5517\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 136.1369 - val_loss: 175.2175\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 150.5402 - val_loss: 173.7107\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 145.0581 - val_loss: 172.9002\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 139.3966 - val_loss: 172.0009\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 133.6471 - val_loss: 171.4387\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 135.3464 - val_loss: 170.6556\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 132.5309 - val_loss: 170.1528\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 126.9994 - val_loss: 168.7253\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 134.7667 - val_loss: 168.1050\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 125.8375 - val_loss: 167.4699\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 136.8147 - val_loss: 166.3740\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 133.4758 - val_loss: 165.8678\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 134.1146 - val_loss: 164.8202\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 123.7317 - val_loss: 163.7885\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 130.4671 - val_loss: 163.1998\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 129.6943 - val_loss: 162.8880\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 126.9306 - val_loss: 162.4398\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 127.5765 - val_loss: 161.2924\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 119.7630 - val_loss: 160.3590\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 119.4619 - val_loss: 159.7372\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 117.7892 - val_loss: 159.7966\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 137.2284 - val_loss: 158.6220\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 120.5098 - val_loss: 158.4200\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 125.4106 - val_loss: 157.3977\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 122.6237 - val_loss: 157.2075\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 123.9998 - val_loss: 157.0758\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 123.4876 - val_loss: 156.4574\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 108.7796 - val_loss: 156.1281\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 121.4385 - val_loss: 155.3706\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 115.1579 - val_loss: 154.7587\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 118.9236 - val_loss: 154.2786\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  50 \n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 23ms/step - loss: 1551.3443 - val_loss: 1465.6713\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1557.1318 - val_loss: 1452.9382\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1482.4231 - val_loss: 1437.9591\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1416.8094 - val_loss: 1419.3115\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1468.0719 - val_loss: 1396.2954\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1424.7999 - val_loss: 1369.3468\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1414.6988 - val_loss: 1337.7852\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1310.9575 - val_loss: 1301.5732\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1418.8281 - val_loss: 1259.1201\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1312.1202 - val_loss: 1211.5464\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1275.1560 - val_loss: 1157.4890\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1220.6626 - val_loss: 1096.4174\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1092.7379 - val_loss: 1029.0692\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1069.1828 - val_loss: 954.9131\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 998.5144 - val_loss: 876.6210\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 904.9767 - val_loss: 796.8481\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 825.1375 - val_loss: 716.6635\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 729.3884 - val_loss: 637.6224\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 591.6260 - val_loss: 561.0703\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 575.1430 - val_loss: 489.3251\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 512.6530 - val_loss: 424.5735\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 429.3319 - val_loss: 371.5270\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 401.0639 - val_loss: 326.5292\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 358.0271 - val_loss: 291.5532\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 293.1804 - val_loss: 263.4616\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 273.6967 - val_loss: 242.9104\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 243.3874 - val_loss: 226.4890\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 252.9674 - val_loss: 214.7871\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 235.7380 - val_loss: 205.9942\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 214.5004 - val_loss: 199.2389\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 225.1219 - val_loss: 193.4208\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 199.4753 - val_loss: 188.0651\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 223.2494 - val_loss: 184.3743\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 209.6407 - val_loss: 181.2546\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 203.3886 - val_loss: 177.9106\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 198.3070 - val_loss: 174.7254\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 204.5166 - val_loss: 172.4315\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 190.7268 - val_loss: 169.4566\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 177.2590 - val_loss: 167.1149\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 184.5827 - val_loss: 163.9975\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 195.2574 - val_loss: 162.7850\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 179.2362 - val_loss: 160.3253\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 173.1128 - val_loss: 158.5421\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 167.1952 - val_loss: 156.5214\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 169.5179 - val_loss: 154.9055\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 178.0892 - val_loss: 153.4526\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 155.0721 - val_loss: 151.5529\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 168.3154 - val_loss: 150.1187\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 160.1792 - val_loss: 148.4680\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 152.6493 - val_loss: 146.9597\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 151.5334 - val_loss: 145.9537\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 158.5330 - val_loss: 144.0364\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 153.6385 - val_loss: 143.4764\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 138.1661 - val_loss: 141.1065\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 164.3246 - val_loss: 139.9932\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 149.4305 - val_loss: 138.5754\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 131.8621 - val_loss: 137.5545\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 148.7462 - val_loss: 135.9170\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 139.4051 - val_loss: 134.7911\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 145.2322 - val_loss: 133.1629\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 146.1659 - val_loss: 131.9764\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 134.2278 - val_loss: 130.4928\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 136.0062 - val_loss: 129.1738\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 135.6128 - val_loss: 127.9679\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 127.1360 - val_loss: 127.3966\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 132.0135 - val_loss: 125.6854\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 128.1410 - val_loss: 124.4014\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 123.9803 - val_loss: 123.3361\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 115.0049 - val_loss: 122.0852\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 120.6979 - val_loss: 121.1281\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 120.2312 - val_loss: 120.3160\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 114.0869 - val_loss: 119.3179\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 123.0530 - val_loss: 117.6242\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 114.1291 - val_loss: 116.6968\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 111.9634 - val_loss: 115.3057\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 114.8557 - val_loss: 114.4720\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 111.9311 - val_loss: 113.3568\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 101.7600 - val_loss: 113.5151\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 110.9461 - val_loss: 111.9852\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 104.3686 - val_loss: 110.6999\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 113.7838 - val_loss: 109.4295\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 105.5182 - val_loss: 108.7104\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 112.6013 - val_loss: 108.3860\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 102.2598 - val_loss: 107.3385\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 97.8955 - val_loss: 106.6800\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 93.4075 - val_loss: 105.8817\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 95.7391 - val_loss: 104.5794\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 93.7483 - val_loss: 103.9962\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 98.2528 - val_loss: 103.3325\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 103.2863 - val_loss: 103.0735\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 87.2805 - val_loss: 101.9119\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 104.9910 - val_loss: 101.6345\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 81.9933 - val_loss: 100.5277\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 96.3218 - val_loss: 100.1747\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 97.5270 - val_loss: 99.8264\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 91.6511 - val_loss: 99.1475\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 90.8806 - val_loss: 98.2061\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 92.3180 - val_loss: 98.9150\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 93.0105 - val_loss: 97.7741\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 91.7533 - val_loss: 96.9415\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Total Execution Time :  0:09:00.988446\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pml_i6OY1Bqc",
        "outputId": "a8c48f78-4784-49ea-9af3-092fe4cb4789"
      },
      "source": [
        "# Calculate the Mean of the MSE of 50 models\r\n",
        "mean_of_mse = stats.mean(list_of_mse)\r\n",
        "\r\n",
        "# Calculate the Standard Deviation of the MSE of 50 models\r\n",
        "std_of_mse = stats.stdev(list_of_mse)\r\n",
        "\r\n",
        "# Print the Mean and Standard Deviation of MSE of 50 models\r\n",
        "print('\\n\\nMean of the MSE of 50 Models : ' , str(mean_of_mse))\r\n",
        "print('Standard Deviation of MSE of 50 Models : ' , str(std_of_mse))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Mean of the MSE of 50 Models :  133.7938433559365\n",
            "Standard Deviation of MSE of 50 Models :  12.140593462789218\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ps6cczK1G4f"
      },
      "source": [
        "#### <font color = green> Comparision of Mean of MSE with Mean of MSE from PART B </font>\r\n",
        "<table style=\"width:20%\">\r\n",
        "  <tr>\r\n",
        "    <th>Mean of MSE of PART A</th>\r\n",
        "    <th>Mean of MSE of PART B</th>\r\n",
        "    <th>Mean of MSE of PART C</th>\r\n",
        "  </tr>\r\n",
        "  <tr>\r\n",
        "    <td>177.27</td>\r\n",
        "    <td>176.27</td>\r\n",
        "    <td>133.79</td>\r\n",
        "  </tr>\r\n",
        "</table>\r\n",
        "\r\n",
        "The table above compares the Mean of **MSE for PART A**, **Mean of MSE for PART B** and **Mean of MSE for PART C**. As can be seen, the value of Mean of MSE of PART C is marginally larger than that of PART B. This shows that the effect of **normalizing the features** as well as **increasing the number of epochs by 2** did not improve the performance of the regression model and helps it in finding the line of best fit"
      ]
    }
  ]
}