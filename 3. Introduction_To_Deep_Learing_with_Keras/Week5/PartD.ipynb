{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PartD.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ghn3WIOzKELl"
      },
      "source": [
        "# Importing the neccessary libraries\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import statistics as stats\r\n",
        "import os\r\n",
        "import keras\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from sklearn import preprocessing\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import mean_squared_error\r\n",
        "from datetime import datetime\r\n",
        "\r\n",
        "# Forcing keras to use CPU.\r\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "id": "HzGihXDrKInQ",
        "outputId": "a7c8c0bd-eb50-4776-832f-255083d47471"
      },
      "source": [
        "# Reading the Data and storing it in a dataframe\r\n",
        "\r\n",
        "df = pd.read_csv('https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0101EN/labs/data/concrete_data.csv')\r\n",
        "df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Cement</th>\n",
              "      <th>Blast Furnace Slag</th>\n",
              "      <th>Fly Ash</th>\n",
              "      <th>Water</th>\n",
              "      <th>Superplasticizer</th>\n",
              "      <th>Coarse Aggregate</th>\n",
              "      <th>Fine Aggregate</th>\n",
              "      <th>Age</th>\n",
              "      <th>Strength</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>540.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>162.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1040.0</td>\n",
              "      <td>676.0</td>\n",
              "      <td>28</td>\n",
              "      <td>79.99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>540.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>162.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1055.0</td>\n",
              "      <td>676.0</td>\n",
              "      <td>28</td>\n",
              "      <td>61.89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>332.5</td>\n",
              "      <td>142.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>228.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>932.0</td>\n",
              "      <td>594.0</td>\n",
              "      <td>270</td>\n",
              "      <td>40.27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>332.5</td>\n",
              "      <td>142.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>228.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>932.0</td>\n",
              "      <td>594.0</td>\n",
              "      <td>365</td>\n",
              "      <td>41.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>198.6</td>\n",
              "      <td>132.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>192.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>978.4</td>\n",
              "      <td>825.5</td>\n",
              "      <td>360</td>\n",
              "      <td>44.30</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Cement  Blast Furnace Slag  Fly Ash  ...  Fine Aggregate  Age  Strength\n",
              "0   540.0                 0.0      0.0  ...           676.0   28     79.99\n",
              "1   540.0                 0.0      0.0  ...           676.0   28     61.89\n",
              "2   332.5               142.5      0.0  ...           594.0  270     40.27\n",
              "3   332.5               142.5      0.0  ...           594.0  365     41.05\n",
              "4   198.6               132.4      0.0  ...           825.5  360     44.30\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqTYkPvzKLTm",
        "outputId": "7fcfb8b3-413a-48e2-f617-2c8630623e2a"
      },
      "source": [
        "print('\\nShape of dataframe : ',df.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Shape of dataframe :  (1030, 9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "r8niVpLzKMhh",
        "outputId": "aa4620f8-0cb2-4c8b-adb5-ff2e630fdfa0"
      },
      "source": [
        "# Summary of the dataset\r\n",
        "df.describe()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Cement</th>\n",
              "      <th>Blast Furnace Slag</th>\n",
              "      <th>Fly Ash</th>\n",
              "      <th>Water</th>\n",
              "      <th>Superplasticizer</th>\n",
              "      <th>Coarse Aggregate</th>\n",
              "      <th>Fine Aggregate</th>\n",
              "      <th>Age</th>\n",
              "      <th>Strength</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>281.167864</td>\n",
              "      <td>73.895825</td>\n",
              "      <td>54.188350</td>\n",
              "      <td>181.567282</td>\n",
              "      <td>6.204660</td>\n",
              "      <td>972.918932</td>\n",
              "      <td>773.580485</td>\n",
              "      <td>45.662136</td>\n",
              "      <td>35.817961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>104.506364</td>\n",
              "      <td>86.279342</td>\n",
              "      <td>63.997004</td>\n",
              "      <td>21.354219</td>\n",
              "      <td>5.973841</td>\n",
              "      <td>77.753954</td>\n",
              "      <td>80.175980</td>\n",
              "      <td>63.169912</td>\n",
              "      <td>16.705742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>102.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>121.800000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>801.000000</td>\n",
              "      <td>594.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.330000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>192.375000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>164.900000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>932.000000</td>\n",
              "      <td>730.950000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>23.710000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>272.900000</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>185.000000</td>\n",
              "      <td>6.400000</td>\n",
              "      <td>968.000000</td>\n",
              "      <td>779.500000</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>34.445000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>350.000000</td>\n",
              "      <td>142.950000</td>\n",
              "      <td>118.300000</td>\n",
              "      <td>192.000000</td>\n",
              "      <td>10.200000</td>\n",
              "      <td>1029.400000</td>\n",
              "      <td>824.000000</td>\n",
              "      <td>56.000000</td>\n",
              "      <td>46.135000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>540.000000</td>\n",
              "      <td>359.400000</td>\n",
              "      <td>200.100000</td>\n",
              "      <td>247.000000</td>\n",
              "      <td>32.200000</td>\n",
              "      <td>1145.000000</td>\n",
              "      <td>992.600000</td>\n",
              "      <td>365.000000</td>\n",
              "      <td>82.600000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Cement  Blast Furnace Slag  ...          Age     Strength\n",
              "count  1030.000000         1030.000000  ...  1030.000000  1030.000000\n",
              "mean    281.167864           73.895825  ...    45.662136    35.817961\n",
              "std     104.506364           86.279342  ...    63.169912    16.705742\n",
              "min     102.000000            0.000000  ...     1.000000     2.330000\n",
              "25%     192.375000            0.000000  ...     7.000000    23.710000\n",
              "50%     272.900000           22.000000  ...    28.000000    34.445000\n",
              "75%     350.000000          142.950000  ...    56.000000    46.135000\n",
              "max     540.000000          359.400000  ...   365.000000    82.600000\n",
              "\n",
              "[8 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reQaLIoHKOXl",
        "outputId": "e73ad2cf-a27a-498d-edb1-0b9bf8e3c1f1"
      },
      "source": [
        "# Creating dataframes of features (X) and target (Y)\r\n",
        "X = df.iloc[:, 0:8]\r\n",
        "Y = df.iloc[:,8]\r\n",
        "\r\n",
        "# Printing the dataframes X and Y to ensure we have created the dataframes with the correct columns\r\n",
        "print('The features or the predictors (X) are : ', X, '\\n\\n') \r\n",
        "print('The target (Y) is : ', Y, '\\n\\n')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The features or the predictors (X) are :        Cement  Blast Furnace Slag  ...  Fine Aggregate  Age\n",
            "0      540.0                 0.0  ...           676.0   28\n",
            "1      540.0                 0.0  ...           676.0   28\n",
            "2      332.5               142.5  ...           594.0  270\n",
            "3      332.5               142.5  ...           594.0  365\n",
            "4      198.6               132.4  ...           825.5  360\n",
            "...      ...                 ...  ...             ...  ...\n",
            "1025   276.4               116.0  ...           768.3   28\n",
            "1026   322.2                 0.0  ...           813.4   28\n",
            "1027   148.5               139.4  ...           780.0   28\n",
            "1028   159.1               186.7  ...           788.9   28\n",
            "1029   260.9               100.5  ...           761.5   28\n",
            "\n",
            "[1030 rows x 8 columns] \n",
            "\n",
            "\n",
            "The target (Y) is :  0       79.99\n",
            "1       61.89\n",
            "2       40.27\n",
            "3       41.05\n",
            "4       44.30\n",
            "        ...  \n",
            "1025    44.28\n",
            "1026    31.18\n",
            "1027    23.70\n",
            "1028    32.77\n",
            "1029    32.40\n",
            "Name: Strength, Length: 1030, dtype: float64 \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpeIxkgTKSwn"
      },
      "source": [
        "<b>Note 1</b> : Unlike the method in this course, the splitting is done using indexing instead of using the names of the columns. Additionally, a different notation is used. The word <i>features</i> is used instead of <i>predictors</i>.\r\n",
        "\r\n",
        "\r\n",
        "<b>Note 2</b> : Pandas indexes columns starting from 0. Note in the code below for the features (X) indexing is used as `[:, 0:8]`. The first part preceding the coma `(:)` tells pandas to include ALL rows of the original dataframe (df) in the new dataframe called X while the part succedding the comma `(0:8)` tells pandas to include all columns of the original dataframe (df) starting from column with index = 0 and ending with column with index = 7, <b> but not to include the column with index = 8 </b>  \r\n",
        "\r\n",
        "<b>Note 3</b> : In order to split the data into train and test sets, the train_test_split function of the sklearn library is used. `The random_state` is used to ensure that the train and test split is the same each time, i.e. the train set and the test set have the same samples each time the code is run which is good for reproducing the results. If left empty, the random state is used by `np.random`. Since the Project requires splitting data the into <b>random</b> sets, hence `random_state` is not used, i.e. no value is set for random state. As the data hase to be split randomly  into train and test sets <b>50</b> times, a for loop will be used to to split the data in train test sets for <b>each model</b> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GhHM1EdKVKn"
      },
      "source": [
        "def regression_model() :\r\n",
        "    \r\n",
        "    # Create the model\r\n",
        "    model = Sequential()\r\n",
        "    model.add(Dense(10, activation='relu', input_shape=(X.shape[1],)))\r\n",
        "    model.add(Dense(10, activation='relu'))\r\n",
        "    model.add(Dense(1))\r\n",
        "\r\n",
        "    # Compile the model\r\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\r\n",
        "    \r\n",
        "    return model"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1_0sjSwKXxO"
      },
      "source": [
        "def data_split() :\r\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)\r\n",
        "    \r\n",
        "    # Create a list containing X_train, X_test, Y_train, Y_test and return the list\r\n",
        "    splits = [X_train, X_test, Y_train, Y_test] \r\n",
        "    return splits"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXcPvjk6KaJx"
      },
      "source": [
        "<b>Note </b> : In the above function `data_split()` the <i>X_train, X_test, Y_train, Y_test</i> sets are stored in a list and list is returned. This is to ensure that the <i>X_train, X_test, Y_train, Y_test</i> sets are not printed when the function is called"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2zoMyifKZcU"
      },
      "source": [
        "def predict() :\r\n",
        "    return model.predict(X_test)\r\n",
        "\r\n",
        "def calculate_mse() :\r\n",
        "    return mean_squared_error(Y_test,Y_predicted)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ0h9iIhKfhp"
      },
      "source": [
        "<b>Note 1</b> : The function `regression_model` as defined above only **compiles** the model and doesn't fit the model to training set. This is because in PART C, the number of epochs are changed and this would allow to fit the model with new number of epochs. This function will be used for **PART A**, **PART B** and **PART C**.\r\n",
        "\r\n",
        "<b>Note 2</b> : Since the splitting data, prediction and mean squared error calculations are all being used repeatly as well, hence there are separate functions created for them to remove redundancy of typing the same line of code while also making the code more neat. However,  as we the features (X) are to be normalized only **once**, hence there is no need to create a function for it\r\n",
        "\r\n",
        "<b>Note 3</b> : As the split function is executed first, the training and test sets are obtained and hence there is no need to explicitly pass any arguments to `predict()` and `calculate_mse()` functions. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qEKgtGa35xP"
      },
      "source": [
        "# <font color = blue> PART D : BASELINE MODEL WITH INCREASED HIDDEN LAYERS </font>\r\n",
        "\r\n",
        "\r\n",
        "In this part, all the tasks from <b>PART B</b> are performed, but this time the number of hidden layers are increased to 3\r\n",
        "\r\n",
        "<b>The new model will have : </b>\r\n",
        "<ul>\r\n",
        "        <li> Input layer with 10 nodes </li>\r\n",
        "        <li> 3 hidden layers, each with 10 nodes and ReLU activation function </li>\r\n",
        "        <li> Adam optimizer and mean squared error loss function </li>\r\n",
        "</ul>\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsDK3Sva39r_"
      },
      "source": [
        "## <font color = darkorange>Task 1 : Train and Test the Baseline Model with 100 epochs and Increased Hidden Layers</font>\r\n",
        "\r\n",
        "In order to train and test the the baseline model with normalized features, 100 epochs and increased hidden layers, the following steps are performed :\r\n",
        "<ol>\r\n",
        "    <li>Normalize the features (X)</li>\r\n",
        "    <li>Randomly split the data into <i>X_train, X_test, Y_train, Y_test</i> sets</li>\r\n",
        "    <li>Create a new model with 100 epochs</li>\r\n",
        "    <li>Train the model using <i>X_train, Y_train</i> using <b>50</b> epochs</li>\r\n",
        "    <li>Evaluate the model using <i>X_test, Y_test</i></li>\r\n",
        "    <li>Get the predictions on the <i>X_test</i> set </li>\r\n",
        "    <li>Compute the <b>mean squared error</b> on the test set using sklearn</li>\r\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ye1l3yR54AIa"
      },
      "source": [
        "### <font color = #2980B9> Step 1 : Normalize the features (X) </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jk5mRe4q4P6I"
      },
      "source": [
        "X=(X-X.mean())/X.std()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h35q2qal4Yz-"
      },
      "source": [
        "### <font color = #2980B9> Step 2 : Randomly split the data into <i>X_train, X_test, Y_train, Y_test </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3InusNB4lYz"
      },
      "source": [
        "# Creating X_train, X_test, Y_train and Y_test sets\r\n",
        "X_train, X_test, Y_train, Y_test = data_split()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s555_V_Z4nEo"
      },
      "source": [
        "### <font color = #2980B9> Step 3 : Create a new regression model with 3 hidden layers, each with 10 nodes and ReLU activation  </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDg1gumM4q7w"
      },
      "source": [
        "def three_layer_regression_model () :\r\n",
        "    \r\n",
        "    # Create the model\r\n",
        "    model = Sequential()\r\n",
        "    model.add(Dense(10, activation='relu', input_shape=(X.shape[1],)))\r\n",
        "    model.add(Dense(10, activation='relu'))\r\n",
        "    model.add(Dense(10, activation='relu'))\r\n",
        "    model.add(Dense(10, activation='relu'))\r\n",
        "    model.add(Dense(1))\r\n",
        "\r\n",
        "    # Compile the model\r\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\r\n",
        "    \r\n",
        "    return model\r\n",
        "    "
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRCmZflG4sFG"
      },
      "source": [
        "### <font color = #2980B9> Step 4 : Train the model using <i>X_train, Y_train</i> using 50 epochs </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8I9H9_d4vv8",
        "outputId": "b25150f6-18a9-4028-e9af-98189ba55cb2"
      },
      "source": [
        "model = three_layer_regression_model()\r\n",
        "\r\n",
        "# Fit the model on the train set\r\n",
        "model.fit(X_train, Y_train, validation_split=0.3, epochs=100)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 15ms/step - loss: 1650.8742 - val_loss: 1620.0457\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1525.2866 - val_loss: 1589.8502\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1486.6295 - val_loss: 1550.0085\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1457.4682 - val_loss: 1492.6698\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1454.5857 - val_loss: 1408.7964\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1399.5018 - val_loss: 1286.7604\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1250.5824 - val_loss: 1115.7271\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1049.6070 - val_loss: 898.6346\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 811.2394 - val_loss: 661.2366\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 542.3541 - val_loss: 456.8322\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 399.9432 - val_loss: 328.7790\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 288.7055 - val_loss: 279.2950\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 230.5761 - val_loss: 255.1274\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 220.7987 - val_loss: 239.0852\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 223.0077 - val_loss: 226.3127\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 207.1435 - val_loss: 215.2620\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 233.2419 - val_loss: 206.3848\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 199.6958 - val_loss: 199.8518\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 201.5254 - val_loss: 193.7552\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 199.1419 - val_loss: 188.4026\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 203.3278 - val_loss: 183.0893\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 200.0735 - val_loss: 180.8476\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 190.8195 - val_loss: 176.1101\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 182.6578 - val_loss: 172.7563\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 180.7138 - val_loss: 171.8267\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 178.1339 - val_loss: 169.0977\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 181.6100 - val_loss: 166.8865\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 173.8141 - val_loss: 164.2875\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 168.6446 - val_loss: 164.7827\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 169.1623 - val_loss: 161.6098\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 168.0946 - val_loss: 159.4695\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 172.1931 - val_loss: 158.9124\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 172.4761 - val_loss: 158.3535\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 156.1512 - val_loss: 156.0789\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 176.1907 - val_loss: 154.4774\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 154.0248 - val_loss: 153.6552\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 165.8464 - val_loss: 152.6200\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 166.5854 - val_loss: 151.0669\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 156.5902 - val_loss: 150.1334\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 175.9590 - val_loss: 150.0987\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 143.3864 - val_loss: 147.9100\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 158.1815 - val_loss: 147.8542\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 154.2452 - val_loss: 145.8822\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 163.3845 - val_loss: 145.4420\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 156.2676 - val_loss: 145.5644\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 143.6125 - val_loss: 143.6385\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 154.5893 - val_loss: 142.7799\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 158.8669 - val_loss: 141.9027\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 157.5828 - val_loss: 141.6266\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 163.3536 - val_loss: 141.5420\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 140.3446 - val_loss: 139.4554\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 137.9374 - val_loss: 139.5286\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 144.8446 - val_loss: 138.5177\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 155.3088 - val_loss: 138.3241\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 150.6254 - val_loss: 137.5882\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 135.4224 - val_loss: 136.3850\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 149.5943 - val_loss: 136.1953\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 123.1553 - val_loss: 134.7140\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 140.4499 - val_loss: 134.6608\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 152.2520 - val_loss: 135.7695\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 139.2309 - val_loss: 133.6229\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 144.0273 - val_loss: 132.8170\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 132.4064 - val_loss: 132.5751\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 139.5879 - val_loss: 132.3396\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 145.6175 - val_loss: 131.9565\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 144.7211 - val_loss: 130.6869\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 137.1880 - val_loss: 130.2156\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 129.4724 - val_loss: 129.6581\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 137.4965 - val_loss: 129.5651\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 128.3011 - val_loss: 129.2002\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 136.1579 - val_loss: 128.7375\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 130.7618 - val_loss: 128.1033\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 128.2178 - val_loss: 127.8203\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 142.0078 - val_loss: 127.1144\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 124.9061 - val_loss: 126.6878\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 129.5432 - val_loss: 126.2011\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 133.0468 - val_loss: 126.1461\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 128.6960 - val_loss: 125.9302\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 129.0137 - val_loss: 125.1784\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 135.9211 - val_loss: 124.8470\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 125.6845 - val_loss: 124.1126\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 129.2038 - val_loss: 123.8963\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 115.1822 - val_loss: 123.9408\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 112.2909 - val_loss: 122.4889\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 124.2662 - val_loss: 122.1859\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 124.7556 - val_loss: 123.0212\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 121.4064 - val_loss: 121.4344\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 127.7584 - val_loss: 121.1474\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 129.4473 - val_loss: 120.5690\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 128.0817 - val_loss: 120.8920\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 119.2636 - val_loss: 119.9088\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 111.9599 - val_loss: 119.9757\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 120.4959 - val_loss: 119.0589\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 121.3201 - val_loss: 119.1489\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 114.6904 - val_loss: 118.7524\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 123.8880 - val_loss: 118.3649\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 111.5818 - val_loss: 118.1474\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 114.6228 - val_loss: 117.1193\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 123.5222 - val_loss: 117.9110\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 115.4273 - val_loss: 117.1298\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f318c8114e0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECOFMZ7d42aw"
      },
      "source": [
        "### <font color = #2980B9> Step 5 : Get the predictions on the X_test </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCj9uWYi44MO"
      },
      "source": [
        "# Store the predictions in a variable Y_Predicted\r\n",
        "Y_predicted = predict()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOydd62X45pi"
      },
      "source": [
        "### <font color = #2980B9> Step 6 : Compute the <i>mean squared error</i> on the test set using sklearn </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkwyms0948b2",
        "outputId": "5a189101-3eff-45a4-ce63-22f7817412e9"
      },
      "source": [
        "# Calculate the mean square error\r\n",
        "\r\n",
        "mse = calculate_mse()\r\n",
        "print('Mean Square Error (MSE) of the Baseline Model with Normalized Features is : ' , str(mse))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean Square Error (MSE) of the Baseline Model with Normalized Features is :  109.7375954263577\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRpkeJwG4-X3"
      },
      "source": [
        "## <font color = darkorange>Task 2 : Create 50 Models and Calculate the <i>µ</i> & <i>σ</i> of their MSE Errors with new Features</font>\r\n",
        "\r\n",
        "In order to train 50 models with new features (normalized features) and calulate the mean (µ) and standard deviation (σ) of their mean square errors (MSE) the following steps are performed :\r\n",
        "<ol>\r\n",
        "    <li>Create an empty list <code>list_of_mse</code> to store the mean square error of each of the models</li>\r\n",
        "    <li>Define a for loop and perform each of the following steps in the loop</li>\r\n",
        "        <ol>\r\n",
        "        <li>Randomly split the data into <i>X_train, X_test, Y_train, Y_test</i> sets</li>\r\n",
        "        <li>Train the model using <i>X_train, Y_train</i> using <b>50</b> epochs</li>\r\n",
        "        <li>Evaluate the model using <i>X_test, Y_test</i></li>\r\n",
        "        <li>Get the predictions on the <i>X_test</i> set </li>\r\n",
        "        <li>Compute the <b>mean squared error</b> on the test set using sklearn</li>\r\n",
        "    </ol>\r\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t17xnN1b5Au7"
      },
      "source": [
        "### <font color = #2980B9> Step 1 : Creating the <i>list_of_mse</i> list</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUHhGWVo5EIn"
      },
      "source": [
        "# Create the empty lists\r\n",
        "list_of_mse = []"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nhJmPnN5Fxm"
      },
      "source": [
        "### <font color = #2980B9> Step 2 : Creating 50 Models, Calculating The MSE for Each and <i>µ</i> & <i>σ</i> of the 50 MSE Values in <i>list_of_mse</i> </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qw-kICWD5HFe",
        "outputId": "87600a68-d1fa-46ea-d8e9-292d470cebcd"
      },
      "source": [
        "# Create the for loop to split the data, create, compile & fit model, evaluate & nake predictions, caluclate mse and store\r\n",
        "# in list_of_mse\r\n",
        "\r\n",
        "start_time = datetime.now() # Starting time of the for loop execution\r\n",
        "\r\n",
        "for i in range(50) :\r\n",
        "    # Split the data into train and test set\r\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)\r\n",
        "    \r\n",
        "    # Create and compile the regression model using the function regression_model as defined in TASK 1\r\n",
        "    model = regression_model()\r\n",
        "\r\n",
        "    # Fit the model on the train set\r\n",
        "    print('\\n\\n\\nTraining Model # ' , i+1 , '\\n\\n') # Print the Model Number that is being trained\r\n",
        "    model.fit(X_train, Y_train, validation_split=0.3, epochs=100)\r\n",
        "    print('\\n')\r\n",
        "    \r\n",
        "    # Make prediction on the test set\r\n",
        "    Y_predicted = model.predict(X_test)\r\n",
        "    \r\n",
        "    # Calculate the mean square error\r\n",
        "    mse = mean_squared_error(Y_test, Y_predicted)\r\n",
        "    \r\n",
        "    # Add the mse to the list_of_mse list\r\n",
        "    list_of_mse.append(mse)\r\n",
        "\r\n",
        "end_time = datetime.now() # Ending time of the for loop execution\r\n",
        "\r\n",
        "# Print time taken for fitting 50 models and calucating the Mean and Standard Deviation of MSE of 50 models\r\n",
        "print('\\n\\nTotal Execution Time : ' , format(end_time - start_time))\r\n",
        "    "
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 137.1151 - val_loss: 151.3782\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 132.7588 - val_loss: 150.8077\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  27 \n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 15ms/step - loss: 1601.5312 - val_loss: 1521.0061\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1506.2367 - val_loss: 1505.6121\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1453.6903 - val_loss: 1487.8011\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1496.3445 - val_loss: 1466.6372\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 1484.0416 - val_loss: 1441.6147\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1454.3788 - val_loss: 1411.9061\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1421.5746 - val_loss: 1376.7839\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1364.0750 - val_loss: 1335.3467\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1266.7397 - val_loss: 1287.7870\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1224.6153 - val_loss: 1233.8611\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1255.1056 - val_loss: 1171.4828\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1161.8561 - val_loss: 1103.4620\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1096.6070 - val_loss: 1029.2499\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 991.0210 - val_loss: 949.9007\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 916.6048 - val_loss: 865.2281\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 868.5492 - val_loss: 781.2067\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 745.7797 - val_loss: 696.8365\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 686.7340 - val_loss: 615.5013\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 556.9006 - val_loss: 540.5604\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 525.1015 - val_loss: 470.5703\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 483.4098 - val_loss: 409.5623\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 384.9159 - val_loss: 359.3087\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 358.9660 - val_loss: 316.5705\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 286.2570 - val_loss: 284.1089\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 256.9925 - val_loss: 259.4203\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 240.6273 - val_loss: 240.5844\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 238.7160 - val_loss: 227.2777\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 214.6744 - val_loss: 217.1533\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 204.2874 - val_loss: 210.0021\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 196.4407 - val_loss: 204.3309\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 200.9141 - val_loss: 199.3882\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 183.3872 - val_loss: 196.0685\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 206.1434 - val_loss: 192.8531\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 176.6357 - val_loss: 189.9831\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 177.9179 - val_loss: 187.7592\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 180.1313 - val_loss: 185.2388\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 168.5811 - val_loss: 183.1913\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 173.7084 - val_loss: 181.2285\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 174.9823 - val_loss: 179.2823\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 158.3188 - val_loss: 177.3569\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 166.3546 - val_loss: 175.8866\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 150.2303 - val_loss: 174.2540\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 166.8014 - val_loss: 172.7338\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 159.9289 - val_loss: 171.4840\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 159.0447 - val_loss: 170.1227\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 146.8601 - val_loss: 169.0291\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 151.8457 - val_loss: 167.9020\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 149.6238 - val_loss: 166.9455\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 162.1210 - val_loss: 165.8202\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 156.7299 - val_loss: 164.6345\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 148.7896 - val_loss: 163.7605\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 150.8838 - val_loss: 162.8102\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 157.3260 - val_loss: 161.8930\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 152.2630 - val_loss: 161.1527\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 162.4219 - val_loss: 160.4121\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 145.6667 - val_loss: 159.7902\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 137.4740 - val_loss: 159.0076\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 149.0710 - val_loss: 158.2113\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 148.7142 - val_loss: 157.6115\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 153.3015 - val_loss: 156.9643\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 133.9316 - val_loss: 156.1360\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 147.9038 - val_loss: 155.6424\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 151.6587 - val_loss: 155.3072\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 142.4523 - val_loss: 154.4928\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 139.1781 - val_loss: 154.2274\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 146.2854 - val_loss: 153.5583\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 136.4117 - val_loss: 153.2919\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 143.0934 - val_loss: 152.8217\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 139.3055 - val_loss: 152.4693\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 140.4029 - val_loss: 152.3274\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 139.1849 - val_loss: 152.0540\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 140.6878 - val_loss: 151.5531\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 129.0719 - val_loss: 151.2664\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 127.4557 - val_loss: 150.6856\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 146.4728 - val_loss: 150.5370\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 135.5651 - val_loss: 149.9907\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 135.0175 - val_loss: 149.6599\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 135.6507 - val_loss: 149.1819\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 143.3181 - val_loss: 148.9814\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 130.9410 - val_loss: 148.7316\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 142.1543 - val_loss: 148.4058\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 117.8817 - val_loss: 148.3041\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 143.1987 - val_loss: 147.6889\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 129.4588 - val_loss: 147.4577\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 131.5776 - val_loss: 146.8060\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 135.5059 - val_loss: 146.7932\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 145.4148 - val_loss: 146.1938\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 124.7380 - val_loss: 145.9465\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 131.1478 - val_loss: 145.7625\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 129.6196 - val_loss: 145.3855\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 124.7221 - val_loss: 145.0681\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 121.8978 - val_loss: 144.6973\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 109.8712 - val_loss: 144.1575\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 114.9956 - val_loss: 144.0295\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 118.9124 - val_loss: 143.7546\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 121.5164 - val_loss: 143.4626\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 122.2632 - val_loss: 142.9222\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 125.8140 - val_loss: 142.6500\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 124.6300 - val_loss: 142.5940\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 121.9578 - val_loss: 142.0903\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  28 \n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 15ms/step - loss: 1517.8775 - val_loss: 1609.8689\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1509.6796 - val_loss: 1595.1731\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1592.3519 - val_loss: 1579.2511\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1542.6245 - val_loss: 1561.7831\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1478.5893 - val_loss: 1541.0710\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 1543.6081 - val_loss: 1516.4344\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1471.8949 - val_loss: 1487.3691\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1459.9849 - val_loss: 1452.8883\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1345.4472 - val_loss: 1411.2933\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1322.2607 - val_loss: 1362.6383\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1342.1068 - val_loss: 1306.4722\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1324.4036 - val_loss: 1243.0665\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1245.0241 - val_loss: 1175.1913\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1166.5247 - val_loss: 1100.8236\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1073.0860 - val_loss: 1021.1166\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 957.0033 - val_loss: 939.1576\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 908.5876 - val_loss: 855.8644\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 886.7425 - val_loss: 770.7699\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 718.7442 - val_loss: 691.0858\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 698.5207 - val_loss: 615.8104\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 599.8085 - val_loss: 545.6638\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 538.0218 - val_loss: 481.9511\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 497.0097 - val_loss: 428.6522\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 433.6809 - val_loss: 381.2637\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 354.2729 - val_loss: 344.3237\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 328.3573 - val_loss: 312.7014\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 353.0321 - val_loss: 285.4397\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 301.9433 - val_loss: 265.1750\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 279.6735 - val_loss: 248.9748\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 262.8863 - val_loss: 234.5528\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 227.5417 - val_loss: 223.0483\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 226.5897 - val_loss: 213.5346\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 221.0666 - val_loss: 205.9391\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 202.5931 - val_loss: 199.2959\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 222.5366 - val_loss: 193.8711\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 194.9898 - val_loss: 189.5303\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 198.2736 - val_loss: 185.8238\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 194.9968 - val_loss: 182.5070\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 179.8479 - val_loss: 179.8681\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 182.8606 - val_loss: 177.7113\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 183.6353 - val_loss: 175.7893\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 182.0808 - val_loss: 173.6936\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 175.4253 - val_loss: 171.7871\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 160.6971 - val_loss: 170.0757\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 175.8413 - val_loss: 168.2024\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 160.4746 - val_loss: 166.3488\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 157.8190 - val_loss: 164.8340\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 160.1826 - val_loss: 163.3531\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 159.0135 - val_loss: 161.6382\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 159.1462 - val_loss: 160.1129\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 157.7498 - val_loss: 158.7182\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 162.1065 - val_loss: 157.6313\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 150.8413 - val_loss: 156.4436\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 154.8907 - val_loss: 155.3127\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 150.5850 - val_loss: 154.3404\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 149.8744 - val_loss: 152.8933\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 151.5243 - val_loss: 152.1013\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 148.0693 - val_loss: 150.7276\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 149.2840 - val_loss: 149.6002\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 150.7927 - val_loss: 148.8819\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 133.6052 - val_loss: 148.2683\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 143.6008 - val_loss: 147.3464\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 130.6926 - val_loss: 146.1811\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 144.8811 - val_loss: 145.6714\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 144.7257 - val_loss: 143.9993\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 130.8897 - val_loss: 143.7108\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 135.6798 - val_loss: 143.0690\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 139.3046 - val_loss: 142.6925\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 135.4118 - val_loss: 141.6516\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 139.2250 - val_loss: 140.3575\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 151.1571 - val_loss: 139.7350\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 130.1432 - val_loss: 139.4197\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 132.5404 - val_loss: 138.4957\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 139.6654 - val_loss: 137.8018\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 136.6787 - val_loss: 137.2792\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 130.9349 - val_loss: 136.5910\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 137.0001 - val_loss: 136.5724\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 140.9355 - val_loss: 135.4363\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 146.4192 - val_loss: 134.8842\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 118.3110 - val_loss: 134.3278\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 136.9090 - val_loss: 133.3408\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 145.4292 - val_loss: 132.5908\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 127.2967 - val_loss: 132.0562\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 130.4055 - val_loss: 131.1494\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 132.2284 - val_loss: 130.4047\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 121.4336 - val_loss: 130.0555\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 129.1243 - val_loss: 129.1334\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 123.2572 - val_loss: 128.7081\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 120.4607 - val_loss: 127.8786\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 129.8836 - val_loss: 127.3146\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 123.6272 - val_loss: 127.0333\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 115.2374 - val_loss: 126.6924\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 128.2721 - val_loss: 125.2981\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 120.9803 - val_loss: 124.9047\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 121.9444 - val_loss: 124.1515\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 125.3180 - val_loss: 123.6530\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 122.5410 - val_loss: 122.7777\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 130.3237 - val_loss: 122.5239\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 112.3915 - val_loss: 121.8534\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 126.5225 - val_loss: 121.9198\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  29 \n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 15ms/step - loss: 1593.2005 - val_loss: 1492.8453\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1517.2503 - val_loss: 1472.8671\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1449.5033 - val_loss: 1451.4493\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1544.6890 - val_loss: 1427.7145\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1548.6364 - val_loss: 1400.9698\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1438.3221 - val_loss: 1371.1794\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1504.0552 - val_loss: 1336.3885\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1430.9483 - val_loss: 1297.1699\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1343.7297 - val_loss: 1254.1704\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1356.8927 - val_loss: 1205.3979\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1269.8867 - val_loss: 1152.9174\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1202.1899 - val_loss: 1094.6741\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1167.7393 - val_loss: 1033.7313\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1031.2044 - val_loss: 969.2329\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1013.7141 - val_loss: 901.6568\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 929.4138 - val_loss: 832.8092\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 841.4734 - val_loss: 761.9590\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 823.6947 - val_loss: 693.3929\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 742.7772 - val_loss: 625.1410\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 652.0371 - val_loss: 560.2430\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 574.0593 - val_loss: 498.7332\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 524.1737 - val_loss: 442.5369\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 489.4167 - val_loss: 391.4683\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 421.8163 - val_loss: 346.8792\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 375.4373 - val_loss: 308.6187\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 359.9256 - val_loss: 276.7501\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 313.3672 - val_loss: 252.4212\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 279.9127 - val_loss: 233.7729\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 275.7802 - val_loss: 219.2119\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 243.7203 - val_loss: 209.2358\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 217.6480 - val_loss: 202.1369\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 213.1673 - val_loss: 196.9533\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 207.0364 - val_loss: 193.0338\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 214.1817 - val_loss: 189.5811\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 205.2497 - val_loss: 186.9555\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 192.5041 - val_loss: 184.5669\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 191.9405 - val_loss: 182.4640\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 176.4404 - val_loss: 180.6373\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 174.2362 - val_loss: 178.9824\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 166.1096 - val_loss: 177.5180\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 172.2994 - val_loss: 175.7115\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 185.9420 - val_loss: 174.1158\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 172.6228 - val_loss: 172.5526\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 169.9729 - val_loss: 170.7174\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 173.7877 - val_loss: 169.7042\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 158.4978 - val_loss: 168.6030\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 171.7993 - val_loss: 167.2484\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 179.1706 - val_loss: 166.4729\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 177.5876 - val_loss: 165.1801\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 149.9776 - val_loss: 163.9959\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 151.7094 - val_loss: 162.9682\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 160.5221 - val_loss: 161.8193\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 168.4482 - val_loss: 160.3151\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 144.6893 - val_loss: 159.3497\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 156.4584 - val_loss: 158.5838\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 139.9919 - val_loss: 157.6826\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 161.5705 - val_loss: 156.8387\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 153.1553 - val_loss: 155.8437\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 154.5801 - val_loss: 155.2153\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 142.6521 - val_loss: 154.4289\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 151.1101 - val_loss: 153.8872\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 134.5464 - val_loss: 153.4035\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 157.8555 - val_loss: 152.3200\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 151.3186 - val_loss: 151.7696\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 140.3276 - val_loss: 151.7449\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 145.5130 - val_loss: 150.5686\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 136.6642 - val_loss: 150.6559\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 140.6385 - val_loss: 149.5830\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 140.8135 - val_loss: 149.5439\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 148.6757 - val_loss: 148.8735\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 139.2528 - val_loss: 148.1264\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 137.2720 - val_loss: 147.6535\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 132.3501 - val_loss: 147.2879\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 139.0855 - val_loss: 146.4779\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 140.0913 - val_loss: 146.4136\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 123.0805 - val_loss: 146.1531\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 141.5384 - val_loss: 145.3443\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 136.2686 - val_loss: 144.4520\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 146.9368 - val_loss: 144.6777\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 131.4113 - val_loss: 144.3951\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 130.1397 - val_loss: 144.0157\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 130.8364 - val_loss: 143.4526\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 139.5608 - val_loss: 142.6980\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 132.7991 - val_loss: 142.3354\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 127.0889 - val_loss: 142.3050\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 132.2006 - val_loss: 141.6659\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 129.4933 - val_loss: 141.1996\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 126.2954 - val_loss: 140.9605\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 127.8572 - val_loss: 140.8270\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 137.3022 - val_loss: 140.7135\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 123.9350 - val_loss: 140.2345\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 131.3530 - val_loss: 139.7245\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 131.1620 - val_loss: 139.1099\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 129.5374 - val_loss: 139.4045\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 131.2589 - val_loss: 139.3943\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 128.2942 - val_loss: 138.6389\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 127.5594 - val_loss: 138.4019\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 120.4373 - val_loss: 137.9008\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 123.5389 - val_loss: 137.6552\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 134.7542 - val_loss: 137.4722\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  30 \n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 16ms/step - loss: 1642.4409 - val_loss: 1415.1140\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1612.5430 - val_loss: 1400.9237\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1571.2704 - val_loss: 1384.6525\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1702.3671 - val_loss: 1365.0389\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1608.2020 - val_loss: 1341.2189\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1561.3236 - val_loss: 1312.1700\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1503.0675 - val_loss: 1277.6936\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1403.4226 - val_loss: 1237.6490\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1318.1275 - val_loss: 1192.2113\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1345.8113 - val_loss: 1139.6656\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1231.1728 - val_loss: 1081.9978\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1218.1636 - val_loss: 1018.1003\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1166.8570 - val_loss: 948.4764\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1121.4421 - val_loss: 876.8998\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1001.5305 - val_loss: 802.3597\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 889.8324 - val_loss: 729.5578\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 743.3824 - val_loss: 656.9280\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 643.8461 - val_loss: 589.2698\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 601.4996 - val_loss: 524.6841\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 571.3837 - val_loss: 468.6087\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 524.1379 - val_loss: 420.6306\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 398.8876 - val_loss: 381.9886\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 371.3505 - val_loss: 349.6259\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 342.4577 - val_loss: 325.0146\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 291.3301 - val_loss: 305.7687\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 297.3656 - val_loss: 290.9555\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 264.6427 - val_loss: 279.6536\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 262.6116 - val_loss: 270.4605\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 237.4170 - val_loss: 262.9400\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 231.8784 - val_loss: 256.1152\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 238.4070 - val_loss: 249.7710\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 222.9143 - val_loss: 244.1238\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 204.1495 - val_loss: 238.9147\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 209.9174 - val_loss: 234.4147\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 220.9697 - val_loss: 229.8142\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 182.2939 - val_loss: 225.6941\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 192.1103 - val_loss: 221.5240\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 195.3913 - val_loss: 217.7688\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 198.0564 - val_loss: 214.1900\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 190.8521 - val_loss: 210.5820\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 195.7069 - val_loss: 207.5416\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 174.1401 - val_loss: 204.7100\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 177.1134 - val_loss: 201.8131\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 172.9345 - val_loss: 199.3044\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 166.1827 - val_loss: 197.0409\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 178.4930 - val_loss: 194.4975\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 162.7482 - val_loss: 192.5563\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 179.2192 - val_loss: 190.1966\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 172.4566 - val_loss: 188.3626\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 164.6509 - val_loss: 186.5766\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 178.1929 - val_loss: 184.8812\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 154.4065 - val_loss: 183.0934\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 163.4318 - val_loss: 181.4559\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 163.9716 - val_loss: 180.3410\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 156.6933 - val_loss: 178.8852\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 155.8482 - val_loss: 177.4685\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 147.3203 - val_loss: 176.2352\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 165.9551 - val_loss: 174.8292\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 165.7872 - val_loss: 173.4371\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 159.7648 - val_loss: 172.4630\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 137.6993 - val_loss: 171.4074\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 157.9487 - val_loss: 170.2131\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 138.3744 - val_loss: 169.0710\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 146.1826 - val_loss: 168.0705\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 145.0936 - val_loss: 167.1308\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 158.0570 - val_loss: 165.8814\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 144.0058 - val_loss: 165.0959\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 125.5710 - val_loss: 164.0845\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 133.2995 - val_loss: 163.2703\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 153.0807 - val_loss: 162.1104\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 147.2575 - val_loss: 161.2830\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 137.8305 - val_loss: 160.6324\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 133.3607 - val_loss: 159.9250\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 147.9211 - val_loss: 159.1350\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 131.6260 - val_loss: 158.5341\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 149.3078 - val_loss: 157.5784\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 143.4066 - val_loss: 156.8930\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 135.7203 - val_loss: 156.1213\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 139.4415 - val_loss: 155.7483\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 146.9406 - val_loss: 155.0868\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 133.0429 - val_loss: 154.5099\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 130.6556 - val_loss: 153.8708\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 126.5220 - val_loss: 153.3575\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 133.1532 - val_loss: 152.9140\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 134.9133 - val_loss: 152.2655\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 132.7817 - val_loss: 151.8770\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 143.1898 - val_loss: 151.4848\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 133.1208 - val_loss: 151.1860\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 131.6437 - val_loss: 150.7150\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 128.3032 - val_loss: 150.3260\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 129.5945 - val_loss: 149.7288\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 117.7816 - val_loss: 149.5448\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 122.2599 - val_loss: 149.1086\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 130.2887 - val_loss: 148.7338\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 129.3413 - val_loss: 148.3079\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 125.6848 - val_loss: 147.9360\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 119.0363 - val_loss: 147.3978\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 126.0394 - val_loss: 147.1853\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 120.0006 - val_loss: 146.6732\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 125.9083 - val_loss: 146.3472\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  31 \n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 15ms/step - loss: 1529.3442 - val_loss: 1518.1698\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1518.6980 - val_loss: 1501.1593\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1486.9303 - val_loss: 1480.6171\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1437.2309 - val_loss: 1455.7493\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1402.0344 - val_loss: 1426.6455\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1455.5845 - val_loss: 1391.4098\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1384.4188 - val_loss: 1350.4357\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1248.1004 - val_loss: 1304.0034\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1224.4426 - val_loss: 1250.1007\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1256.5335 - val_loss: 1188.9470\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1179.5245 - val_loss: 1121.5024\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1125.6771 - val_loss: 1046.4221\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1085.9391 - val_loss: 965.0981\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 943.8150 - val_loss: 876.2002\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 810.9848 - val_loss: 784.1465\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 756.0536 - val_loss: 684.7628\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 648.6699 - val_loss: 589.1013\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 588.5072 - val_loss: 498.4552\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 482.7520 - val_loss: 420.5655\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 397.2905 - val_loss: 353.9155\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 326.9693 - val_loss: 302.0898\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 287.4395 - val_loss: 261.2433\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 270.0798 - val_loss: 233.2219\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 249.0570 - val_loss: 214.1068\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 238.3649 - val_loss: 203.0344\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 237.2613 - val_loss: 195.9925\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 220.4591 - val_loss: 191.7392\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 198.7957 - val_loss: 188.8527\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 189.5102 - val_loss: 186.9016\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 190.9443 - val_loss: 185.4005\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 202.2274 - val_loss: 183.9495\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 205.8164 - val_loss: 182.8802\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 197.9563 - val_loss: 181.5392\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 174.9006 - val_loss: 180.0838\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 195.2937 - val_loss: 178.5579\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 187.8246 - val_loss: 176.9878\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 201.2311 - val_loss: 176.2094\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 170.6974 - val_loss: 174.6039\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 186.7754 - val_loss: 173.4454\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 175.7501 - val_loss: 171.3200\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 177.4734 - val_loss: 170.2023\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 172.8466 - val_loss: 169.3015\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 168.7528 - val_loss: 168.5684\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 185.4454 - val_loss: 167.2843\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 176.0441 - val_loss: 165.8286\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 169.3001 - val_loss: 163.9988\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 188.5613 - val_loss: 163.2465\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 172.1855 - val_loss: 162.2584\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 156.0017 - val_loss: 161.6520\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 159.7132 - val_loss: 160.9287\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 160.0585 - val_loss: 159.8678\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 150.8603 - val_loss: 159.0708\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 156.1366 - val_loss: 157.9350\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 165.3965 - val_loss: 157.6299\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 158.6601 - val_loss: 156.7157\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 159.7391 - val_loss: 155.5876\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 173.9868 - val_loss: 154.7788\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 163.4764 - val_loss: 153.9583\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 161.6704 - val_loss: 153.5671\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 170.0322 - val_loss: 153.0518\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 154.1534 - val_loss: 151.6539\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 155.2758 - val_loss: 150.4261\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 163.0455 - val_loss: 150.3283\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 155.1877 - val_loss: 149.6348\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 157.3777 - val_loss: 148.7611\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 146.0016 - val_loss: 148.1199\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 156.8718 - val_loss: 147.5832\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 158.6597 - val_loss: 147.1438\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 161.0850 - val_loss: 146.3999\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 150.3324 - val_loss: 145.5358\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 147.4554 - val_loss: 144.4642\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 139.1228 - val_loss: 143.8797\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 147.8096 - val_loss: 143.7076\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 152.7165 - val_loss: 142.9883\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 134.3906 - val_loss: 142.6084\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 157.9099 - val_loss: 141.8102\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 137.6911 - val_loss: 141.4139\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 143.9360 - val_loss: 141.0991\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 144.4743 - val_loss: 139.6454\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 142.1953 - val_loss: 139.4947\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 145.0857 - val_loss: 139.2376\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 141.7238 - val_loss: 138.9491\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 140.4022 - val_loss: 138.1494\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 140.0688 - val_loss: 138.1077\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 138.4438 - val_loss: 137.0823\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 136.3158 - val_loss: 136.8241\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 149.0379 - val_loss: 136.5678\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 126.1501 - val_loss: 136.2885\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 132.7827 - val_loss: 135.6271\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 128.0685 - val_loss: 134.7833\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 140.3286 - val_loss: 134.4400\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 128.7890 - val_loss: 134.2355\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 129.6953 - val_loss: 133.9262\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 130.6700 - val_loss: 133.6773\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 133.2591 - val_loss: 132.7498\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 140.0442 - val_loss: 132.5106\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 130.5039 - val_loss: 132.3951\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 132.9647 - val_loss: 131.6606\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 133.0860 - val_loss: 131.0596\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 135.9052 - val_loss: 131.2864\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  32 \n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 15ms/step - loss: 1500.4313 - val_loss: 1642.5696\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1596.2563 - val_loss: 1631.7859\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1533.8657 - val_loss: 1621.1858\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1478.0537 - val_loss: 1610.0553\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1490.6892 - val_loss: 1597.3984\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1561.9410 - val_loss: 1582.4116\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1566.3359 - val_loss: 1564.6469\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1398.9145 - val_loss: 1543.3561\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1450.8556 - val_loss: 1517.1277\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1356.2573 - val_loss: 1486.2726\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1387.7389 - val_loss: 1449.6549\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 1241.3662 - val_loss: 1407.1191\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1307.9046 - val_loss: 1358.2820\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1219.0889 - val_loss: 1303.2804\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1217.2757 - val_loss: 1242.1998\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1131.0147 - val_loss: 1174.8198\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1061.7438 - val_loss: 1102.1926\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 969.8645 - val_loss: 1023.8040\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 863.6392 - val_loss: 941.7066\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 856.2212 - val_loss: 856.9379\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 681.8563 - val_loss: 773.1991\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 671.8753 - val_loss: 691.1087\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 607.5887 - val_loss: 613.3842\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 501.6021 - val_loss: 543.4158\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 415.3486 - val_loss: 481.8442\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 382.3377 - val_loss: 425.3291\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 396.4177 - val_loss: 377.4491\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 292.4915 - val_loss: 341.1214\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 274.4728 - val_loss: 310.3351\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 236.6010 - val_loss: 285.3959\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 217.5805 - val_loss: 267.6160\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 229.7170 - val_loss: 252.0367\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 208.2666 - val_loss: 241.1305\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 208.1549 - val_loss: 232.1887\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 214.1152 - val_loss: 225.4898\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 204.0888 - val_loss: 219.7623\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 179.1411 - val_loss: 215.4172\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 210.4326 - val_loss: 211.3456\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 189.9217 - val_loss: 207.9893\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 177.2995 - val_loss: 205.1915\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 188.4221 - val_loss: 201.8323\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 165.0736 - val_loss: 198.9766\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 184.4693 - val_loss: 196.7383\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 181.3583 - val_loss: 194.3395\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 185.2418 - val_loss: 192.0964\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 152.2843 - val_loss: 190.2023\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 172.9069 - val_loss: 188.1989\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 169.6393 - val_loss: 186.3370\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 164.1171 - val_loss: 184.8603\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 160.9640 - val_loss: 183.2225\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 165.3000 - val_loss: 181.3732\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 151.5607 - val_loss: 179.6246\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 160.4933 - val_loss: 177.9120\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 183.2357 - val_loss: 176.4634\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 157.4861 - val_loss: 175.0037\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 161.5784 - val_loss: 173.8302\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 154.3699 - val_loss: 172.5714\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 167.9315 - val_loss: 171.0527\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 170.2634 - val_loss: 170.0307\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 152.6465 - val_loss: 168.6289\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 153.7517 - val_loss: 167.4733\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 139.3966 - val_loss: 166.4092\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 149.9485 - val_loss: 164.8490\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 138.1137 - val_loss: 163.9567\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 135.2612 - val_loss: 162.7243\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 152.9172 - val_loss: 161.5676\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 147.3392 - val_loss: 160.3450\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 133.1379 - val_loss: 159.3290\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 143.8397 - val_loss: 158.3613\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 142.1980 - val_loss: 157.2313\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 154.0986 - val_loss: 156.5059\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 143.3108 - val_loss: 155.2390\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 147.7764 - val_loss: 154.3040\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 140.5471 - val_loss: 153.3632\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 137.3361 - val_loss: 152.3967\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 135.4143 - val_loss: 151.6502\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 140.7273 - val_loss: 150.6927\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 126.0917 - val_loss: 149.8217\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 142.6191 - val_loss: 148.8208\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 127.6017 - val_loss: 147.8911\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 124.9489 - val_loss: 147.2733\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 135.1607 - val_loss: 146.5757\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 125.5827 - val_loss: 146.0766\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 139.6675 - val_loss: 145.0197\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 127.1628 - val_loss: 143.8896\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 134.2777 - val_loss: 143.4319\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 118.0783 - val_loss: 142.6625\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 127.0112 - val_loss: 141.8995\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 132.0078 - val_loss: 141.3526\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 132.0956 - val_loss: 140.6053\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 133.9341 - val_loss: 139.9498\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 108.0989 - val_loss: 139.6505\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 124.1128 - val_loss: 138.7002\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 122.9841 - val_loss: 138.0143\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 121.8752 - val_loss: 137.5385\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 121.1627 - val_loss: 136.7129\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 119.9316 - val_loss: 137.2830\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 124.4283 - val_loss: 136.1038\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 118.0922 - val_loss: 135.2016\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 130.6816 - val_loss: 134.4429\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  33 \n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 14ms/step - loss: 1497.6288 - val_loss: 1573.7710\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1572.2221 - val_loss: 1558.6603\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1517.7773 - val_loss: 1542.2673\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1388.0559 - val_loss: 1523.5631\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1545.5319 - val_loss: 1501.5942\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1385.5463 - val_loss: 1475.9971\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1374.0983 - val_loss: 1444.3451\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1341.9181 - val_loss: 1407.1044\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1306.9533 - val_loss: 1361.3151\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1257.8078 - val_loss: 1307.5402\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1224.3438 - val_loss: 1242.2395\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 1097.5082 - val_loss: 1169.2426\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1096.4589 - val_loss: 1086.7975\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 969.4477 - val_loss: 999.0817\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 835.5253 - val_loss: 907.2740\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 786.3613 - val_loss: 813.7035\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 722.6475 - val_loss: 721.0157\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 642.5171 - val_loss: 634.2330\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 536.7206 - val_loss: 556.7348\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 472.9838 - val_loss: 487.9063\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 382.4297 - val_loss: 431.3389\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 340.4762 - val_loss: 385.7461\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 303.1214 - val_loss: 348.7083\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 274.8922 - val_loss: 321.1519\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 249.8207 - val_loss: 300.9393\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 240.3232 - val_loss: 285.4642\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 201.6929 - val_loss: 274.1806\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 214.2616 - val_loss: 265.2702\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 207.9687 - val_loss: 257.4079\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 197.3193 - val_loss: 250.9987\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 188.5768 - val_loss: 245.8493\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 186.0045 - val_loss: 241.2403\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 185.7662 - val_loss: 237.4354\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 188.4607 - val_loss: 232.6322\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 201.1787 - val_loss: 228.8728\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 186.6882 - val_loss: 225.3264\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 182.1194 - val_loss: 222.0602\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 195.4670 - val_loss: 218.0118\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 182.2058 - val_loss: 215.2689\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 182.9034 - val_loss: 212.4601\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 181.6465 - val_loss: 209.6731\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 196.4197 - val_loss: 206.9176\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 163.4428 - val_loss: 204.3846\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 161.2534 - val_loss: 202.1378\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 173.1253 - val_loss: 199.8552\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 176.8827 - val_loss: 197.2163\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 174.9809 - val_loss: 194.6732\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 158.3012 - val_loss: 192.6460\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 173.3613 - val_loss: 190.7517\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 143.5093 - val_loss: 189.2858\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 165.5224 - val_loss: 186.7856\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 153.9844 - val_loss: 185.0418\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 158.9212 - val_loss: 183.5030\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 149.1022 - val_loss: 181.7597\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 140.1809 - val_loss: 179.7186\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 138.7530 - val_loss: 178.1910\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 138.7305 - val_loss: 176.6280\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 138.2790 - val_loss: 174.6447\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 154.0775 - val_loss: 173.2324\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 141.2978 - val_loss: 171.6918\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 134.2334 - val_loss: 170.5527\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 141.7239 - val_loss: 168.7548\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 137.4383 - val_loss: 167.8741\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 135.2571 - val_loss: 166.4922\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 140.5795 - val_loss: 165.4653\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 139.9139 - val_loss: 164.0577\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 135.6189 - val_loss: 162.9352\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 131.2881 - val_loss: 161.7080\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 121.2126 - val_loss: 160.6265\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 124.9626 - val_loss: 159.8434\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 118.8499 - val_loss: 158.8011\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 147.2573 - val_loss: 157.6935\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 126.5661 - val_loss: 156.8919\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 131.3567 - val_loss: 155.8353\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 128.9337 - val_loss: 154.3564\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 128.8822 - val_loss: 153.8124\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 127.7966 - val_loss: 153.0819\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 125.8911 - val_loss: 152.4024\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 121.9404 - val_loss: 151.0771\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 129.1679 - val_loss: 150.4802\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 120.0029 - val_loss: 149.7091\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 125.1726 - val_loss: 148.7243\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 118.2735 - val_loss: 148.3781\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 116.4242 - val_loss: 147.4358\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 118.5569 - val_loss: 146.4116\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 122.4853 - val_loss: 145.8177\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 119.0677 - val_loss: 145.0317\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 113.6694 - val_loss: 144.4356\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 116.0033 - val_loss: 143.8435\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 123.2794 - val_loss: 143.0076\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 110.2893 - val_loss: 142.6076\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 116.0631 - val_loss: 141.5859\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 118.1275 - val_loss: 140.8258\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 112.6246 - val_loss: 140.6798\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 116.3566 - val_loss: 139.8053\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 117.0512 - val_loss: 139.5360\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 120.9530 - val_loss: 138.8470\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 106.2345 - val_loss: 138.1040\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 118.0893 - val_loss: 137.5600\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 100.6201 - val_loss: 136.8932\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  34 \n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 15ms/step - loss: 1435.7882 - val_loss: 1530.3458\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1598.4840 - val_loss: 1509.9409\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1514.8335 - val_loss: 1489.5963\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1483.6948 - val_loss: 1469.0111\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1528.6795 - val_loss: 1446.6633\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1448.3731 - val_loss: 1421.8074\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1432.8669 - val_loss: 1394.4207\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1425.5361 - val_loss: 1363.7195\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1389.3895 - val_loss: 1329.0339\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1286.8934 - val_loss: 1290.7454\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1215.1923 - val_loss: 1247.4669\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1293.3386 - val_loss: 1199.4921\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 1198.2100 - val_loss: 1146.6495\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1161.1296 - val_loss: 1089.5638\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1061.9063 - val_loss: 1029.6910\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 999.7393 - val_loss: 965.0394\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1031.3380 - val_loss: 896.9604\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 924.8196 - val_loss: 829.1984\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 779.9653 - val_loss: 760.1779\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 762.7830 - val_loss: 693.2369\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 651.0906 - val_loss: 628.1494\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 710.7233 - val_loss: 566.4935\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 571.9352 - val_loss: 513.3991\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 501.3791 - val_loss: 464.9628\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 474.6029 - val_loss: 422.8162\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 459.2397 - val_loss: 387.4417\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 388.0433 - val_loss: 359.1790\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 397.4015 - val_loss: 335.9608\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 345.1601 - val_loss: 318.5487\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 318.9332 - val_loss: 304.0736\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 275.8418 - val_loss: 293.4116\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 256.5500 - val_loss: 284.8490\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 263.8318 - val_loss: 277.8694\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 277.7880 - val_loss: 271.7626\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 232.1097 - val_loss: 267.1305\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 227.7193 - val_loss: 262.4257\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 218.9025 - val_loss: 257.7617\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 251.2268 - val_loss: 253.8372\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 229.9288 - val_loss: 250.0373\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 240.0299 - val_loss: 246.2859\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 206.1406 - val_loss: 243.5327\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 191.1126 - val_loss: 239.9519\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 196.0022 - val_loss: 237.1826\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 204.4601 - val_loss: 234.0424\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 216.1680 - val_loss: 231.3243\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 174.8158 - val_loss: 228.5600\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 223.7521 - val_loss: 225.5248\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 188.8528 - val_loss: 223.2113\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 186.9841 - val_loss: 221.0648\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 189.8245 - val_loss: 218.2364\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 210.9231 - val_loss: 216.0030\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 187.6263 - val_loss: 214.1900\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 190.7120 - val_loss: 212.2615\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 177.8456 - val_loss: 210.1019\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 157.9478 - val_loss: 208.1195\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 173.3132 - val_loss: 206.0751\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 168.2414 - val_loss: 204.1321\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 178.0344 - val_loss: 201.7693\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 166.2281 - val_loss: 199.1663\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 172.4521 - val_loss: 197.1064\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 168.2578 - val_loss: 195.3356\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 182.2660 - val_loss: 193.3527\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 168.9108 - val_loss: 191.6370\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 160.0181 - val_loss: 189.9838\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 165.4125 - val_loss: 188.6831\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 149.1132 - val_loss: 187.6502\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 156.1067 - val_loss: 185.6916\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 141.1885 - val_loss: 184.1538\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 145.1900 - val_loss: 183.2737\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 144.7567 - val_loss: 181.9498\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 141.2879 - val_loss: 180.2087\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 159.0092 - val_loss: 178.7245\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 145.0950 - val_loss: 177.5667\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 143.2304 - val_loss: 176.0551\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 146.1692 - val_loss: 175.2605\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 142.5246 - val_loss: 174.3001\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 138.5303 - val_loss: 173.0103\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 144.4249 - val_loss: 171.6478\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 150.7104 - val_loss: 170.5728\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 140.7790 - val_loss: 169.4640\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 136.2124 - val_loss: 167.9392\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 140.2600 - val_loss: 167.7259\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 152.6943 - val_loss: 166.5600\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 140.1862 - val_loss: 165.6541\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 136.5511 - val_loss: 164.8890\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 144.8500 - val_loss: 163.9519\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 127.6491 - val_loss: 163.9316\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 126.2485 - val_loss: 162.6376\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 129.8104 - val_loss: 161.5015\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 127.8992 - val_loss: 161.0342\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 131.4769 - val_loss: 160.3162\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 123.8098 - val_loss: 159.6468\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 140.9720 - val_loss: 158.7104\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 130.8426 - val_loss: 157.9287\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 127.0534 - val_loss: 157.4299\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 137.5109 - val_loss: 156.1892\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 128.5318 - val_loss: 155.6472\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 122.0768 - val_loss: 155.6965\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 126.6286 - val_loss: 154.5812\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 120.2833 - val_loss: 154.1511\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  35 \n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 24ms/step - loss: 1615.7786 - val_loss: 1527.8914\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1530.2256 - val_loss: 1511.4613\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1686.9903 - val_loss: 1494.6401\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1516.3018 - val_loss: 1476.7124\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1447.0970 - val_loss: 1457.0011\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1433.7183 - val_loss: 1435.4885\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1451.3237 - val_loss: 1411.5854\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1383.7331 - val_loss: 1383.6805\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1419.4120 - val_loss: 1351.6301\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1292.7037 - val_loss: 1313.6233\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1341.5460 - val_loss: 1269.9025\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1336.4645 - val_loss: 1218.9226\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1195.2220 - val_loss: 1161.9297\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1189.1129 - val_loss: 1097.7465\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1158.0233 - val_loss: 1028.0242\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1011.5917 - val_loss: 954.9388\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 907.0734 - val_loss: 877.7034\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 929.5650 - val_loss: 796.6806\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 791.3561 - val_loss: 715.7933\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 735.7799 - val_loss: 636.1828\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 654.7097 - val_loss: 562.2202\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 569.7465 - val_loss: 493.9979\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 459.6839 - val_loss: 433.4482\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 454.7472 - val_loss: 379.8503\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 392.6356 - val_loss: 336.8275\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 359.1702 - val_loss: 300.6478\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 322.3918 - val_loss: 272.5419\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 309.3176 - val_loss: 250.0380\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 273.5111 - val_loss: 233.7542\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 253.1120 - val_loss: 222.1360\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 259.0614 - val_loss: 213.7552\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 238.0435 - val_loss: 207.4409\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 224.0638 - val_loss: 202.7738\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 215.0692 - val_loss: 199.4814\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 198.8186 - val_loss: 196.6034\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 220.2558 - val_loss: 194.3253\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 210.2248 - val_loss: 192.4351\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 200.2195 - val_loss: 191.0641\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 202.6937 - val_loss: 189.7788\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 179.8023 - val_loss: 188.3511\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 190.4615 - val_loss: 187.0164\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 199.3606 - val_loss: 185.5046\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 221.9559 - val_loss: 184.3564\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 198.7245 - val_loss: 183.0381\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 190.6836 - val_loss: 182.0836\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 190.2881 - val_loss: 180.8969\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 190.2876 - val_loss: 179.2343\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 167.4170 - val_loss: 178.0901\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 176.9255 - val_loss: 176.8399\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 170.2652 - val_loss: 175.6764\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 165.7625 - val_loss: 174.7836\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 175.0320 - val_loss: 173.5045\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 175.0708 - val_loss: 172.5302\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 174.1873 - val_loss: 171.0414\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 173.8759 - val_loss: 169.7868\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 167.6221 - val_loss: 169.1798\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 177.8411 - val_loss: 168.1066\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 161.9024 - val_loss: 167.2765\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 167.7010 - val_loss: 166.1291\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 155.3338 - val_loss: 165.5478\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 167.6866 - val_loss: 164.4534\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 150.3590 - val_loss: 163.5816\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 167.8774 - val_loss: 162.5051\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 170.9639 - val_loss: 161.5366\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 160.2699 - val_loss: 160.7117\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 156.0155 - val_loss: 160.1770\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 172.4661 - val_loss: 159.1774\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 151.0240 - val_loss: 158.3223\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 159.4361 - val_loss: 157.6488\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 145.7820 - val_loss: 157.0154\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 167.8605 - val_loss: 156.1366\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 156.3270 - val_loss: 155.2745\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 150.7083 - val_loss: 154.0455\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 150.8370 - val_loss: 153.4551\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 146.0837 - val_loss: 152.9401\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 134.2189 - val_loss: 152.2847\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 156.2010 - val_loss: 151.7241\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 154.8475 - val_loss: 151.2938\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 143.8534 - val_loss: 150.6548\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 139.8260 - val_loss: 150.0746\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 147.3271 - val_loss: 149.4557\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 147.1250 - val_loss: 148.8995\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 142.9217 - val_loss: 148.5017\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 151.7367 - val_loss: 147.9671\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 135.7396 - val_loss: 147.3316\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 132.0573 - val_loss: 146.8692\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 145.8033 - val_loss: 146.3334\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 139.6555 - val_loss: 145.8838\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 142.4064 - val_loss: 145.4247\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 139.6816 - val_loss: 145.3371\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 149.8182 - val_loss: 144.7067\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 130.8732 - val_loss: 144.0892\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 134.0229 - val_loss: 143.9069\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 137.6822 - val_loss: 143.6902\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 136.3824 - val_loss: 143.1878\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 146.5808 - val_loss: 142.5849\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 137.4337 - val_loss: 142.2127\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 132.5200 - val_loss: 141.9763\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 140.0110 - val_loss: 141.1763\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 129.8564 - val_loss: 140.9711\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  36 \n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 13ms/step - loss: 1406.4025 - val_loss: 1426.0133\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1415.3724 - val_loss: 1399.0587\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1325.5084 - val_loss: 1369.6672\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 1311.3276 - val_loss: 1335.5723\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1241.1056 - val_loss: 1295.8761\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1264.1631 - val_loss: 1249.3402\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1294.8698 - val_loss: 1195.8408\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1181.5181 - val_loss: 1134.8885\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1110.0187 - val_loss: 1066.4338\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1040.4085 - val_loss: 988.9244\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 975.9280 - val_loss: 904.4312\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 878.3775 - val_loss: 817.6022\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 832.0781 - val_loss: 728.7183\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 714.5833 - val_loss: 642.5280\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 637.7600 - val_loss: 558.7078\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 562.4502 - val_loss: 481.1849\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 485.5837 - val_loss: 413.6668\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 427.1942 - val_loss: 360.0248\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 350.1169 - val_loss: 318.5027\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 324.7793 - val_loss: 288.5026\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 288.8218 - val_loss: 267.8380\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 253.4087 - val_loss: 255.0522\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 253.4528 - val_loss: 246.9167\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 243.7868 - val_loss: 241.0004\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 236.1016 - val_loss: 237.0253\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 203.5760 - val_loss: 233.8727\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 214.8435 - val_loss: 230.6007\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 220.4156 - val_loss: 227.9575\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 216.9025 - val_loss: 224.9933\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 204.3888 - val_loss: 221.7394\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 210.8380 - val_loss: 219.3068\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 197.7812 - val_loss: 216.6142\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 207.6656 - val_loss: 214.0576\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 186.3081 - val_loss: 211.6980\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 194.2693 - val_loss: 208.6462\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 193.9543 - val_loss: 206.3207\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 190.4552 - val_loss: 204.1987\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 177.6490 - val_loss: 201.5181\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 187.6560 - val_loss: 199.5770\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 177.2421 - val_loss: 197.8783\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 176.1149 - val_loss: 195.3451\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 164.4813 - val_loss: 193.3370\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 181.8981 - val_loss: 191.6334\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 175.8317 - val_loss: 189.3959\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 177.7073 - val_loss: 187.9465\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 161.1730 - val_loss: 186.1639\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 159.4685 - val_loss: 184.4031\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 159.1813 - val_loss: 182.6484\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 148.5362 - val_loss: 181.2918\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 168.6173 - val_loss: 179.3889\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 162.5770 - val_loss: 177.5076\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 137.9525 - val_loss: 176.0140\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 166.0725 - val_loss: 175.1536\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 153.6362 - val_loss: 173.6183\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 143.4759 - val_loss: 172.0155\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 155.5348 - val_loss: 170.5824\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 138.3624 - val_loss: 169.1751\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 131.5356 - val_loss: 168.0181\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 149.4919 - val_loss: 166.7493\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 137.0405 - val_loss: 165.5990\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 157.3331 - val_loss: 163.9068\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 140.5489 - val_loss: 163.1111\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 144.4019 - val_loss: 161.9922\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 149.4517 - val_loss: 160.6408\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 136.1938 - val_loss: 159.6249\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 135.7622 - val_loss: 158.4706\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 147.2458 - val_loss: 157.2386\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 136.6479 - val_loss: 155.5305\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 138.0757 - val_loss: 154.4714\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 134.8220 - val_loss: 153.6059\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 127.2404 - val_loss: 152.8891\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 131.0490 - val_loss: 151.5111\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 127.7064 - val_loss: 150.6914\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 136.9424 - val_loss: 149.0737\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 126.7631 - val_loss: 148.0763\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 129.0027 - val_loss: 147.2272\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 125.1097 - val_loss: 145.5819\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 134.3772 - val_loss: 144.5433\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 122.6653 - val_loss: 143.3631\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 147.4565 - val_loss: 142.2292\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 118.5844 - val_loss: 141.4556\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 115.1720 - val_loss: 140.1711\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 128.0781 - val_loss: 139.2154\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 128.1967 - val_loss: 138.0515\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 106.6928 - val_loss: 137.1968\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 119.4245 - val_loss: 136.1306\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 107.0116 - val_loss: 134.9154\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 115.6531 - val_loss: 133.8928\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 122.6881 - val_loss: 133.1539\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 115.8815 - val_loss: 131.9109\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 111.1696 - val_loss: 131.2632\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 114.0554 - val_loss: 129.9294\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 105.3470 - val_loss: 128.9307\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 110.3705 - val_loss: 127.9793\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 114.4430 - val_loss: 127.0967\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 114.7050 - val_loss: 126.3799\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 114.9006 - val_loss: 125.4208\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 109.9413 - val_loss: 124.5555\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 114.2099 - val_loss: 123.5883\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 99.4807 - val_loss: 122.8206\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  37 \n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 14ms/step - loss: 1529.0711 - val_loss: 1416.2500\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1446.3083 - val_loss: 1389.3816\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1442.3439 - val_loss: 1359.2606\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1473.9589 - val_loss: 1325.7941\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1363.5119 - val_loss: 1287.7249\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1411.6371 - val_loss: 1244.8580\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1343.4612 - val_loss: 1196.4258\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1214.2801 - val_loss: 1142.0200\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1212.0356 - val_loss: 1080.2634\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1167.6270 - val_loss: 1013.3142\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1089.2427 - val_loss: 942.4507\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1039.3653 - val_loss: 865.4308\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 903.0156 - val_loss: 786.4012\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 838.3290 - val_loss: 705.2872\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 718.6977 - val_loss: 626.1035\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 658.9170 - val_loss: 551.4076\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 566.1154 - val_loss: 485.3813\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 516.5556 - val_loss: 427.1515\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 446.5166 - val_loss: 379.0252\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 352.4541 - val_loss: 340.0150\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 344.3403 - val_loss: 308.7142\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 292.0617 - val_loss: 285.0656\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 308.6384 - val_loss: 266.6266\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 269.9108 - val_loss: 254.0285\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 269.3551 - val_loss: 243.4100\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 238.4691 - val_loss: 234.9087\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 233.8990 - val_loss: 227.9584\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 249.8893 - val_loss: 221.8318\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 211.3768 - val_loss: 216.9485\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 233.0294 - val_loss: 211.9473\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 215.5543 - val_loss: 208.1668\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 207.5560 - val_loss: 204.3419\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 204.6122 - val_loss: 200.7686\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 209.6591 - val_loss: 197.6174\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 184.8481 - val_loss: 194.7821\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 186.7608 - val_loss: 191.8809\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 187.4620 - val_loss: 189.4793\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 181.0239 - val_loss: 187.2691\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 183.1008 - val_loss: 184.9725\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 190.7566 - val_loss: 182.8405\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 171.1089 - val_loss: 180.8580\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 185.6927 - val_loss: 178.9263\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 179.5239 - val_loss: 177.1177\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 176.2788 - val_loss: 175.4998\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 170.5810 - val_loss: 173.9515\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 163.5741 - val_loss: 172.3182\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 163.0846 - val_loss: 170.9861\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 178.8176 - val_loss: 169.5549\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 161.1281 - val_loss: 168.2782\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 159.5765 - val_loss: 167.2293\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 154.7456 - val_loss: 166.0333\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 161.0491 - val_loss: 165.0587\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 171.8682 - val_loss: 163.9352\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 154.0289 - val_loss: 162.9655\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 149.4551 - val_loss: 161.9510\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 161.3366 - val_loss: 161.1118\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 160.8689 - val_loss: 160.1402\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 144.9177 - val_loss: 159.1776\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 143.9250 - val_loss: 158.3362\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 150.4693 - val_loss: 157.1935\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 141.5023 - val_loss: 156.3947\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 157.3045 - val_loss: 155.6102\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 145.9884 - val_loss: 154.9184\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 161.5958 - val_loss: 154.1575\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 147.2015 - val_loss: 153.4951\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 133.9757 - val_loss: 152.9016\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 151.3785 - val_loss: 152.0257\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 142.5667 - val_loss: 151.3562\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 143.9235 - val_loss: 150.9197\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 147.8163 - val_loss: 150.3974\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 135.6886 - val_loss: 149.6535\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 134.8101 - val_loss: 148.9888\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 131.2777 - val_loss: 148.3627\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 139.5319 - val_loss: 147.8614\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 132.7093 - val_loss: 147.1176\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 141.5597 - val_loss: 146.6267\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 138.0734 - val_loss: 146.2256\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 136.7673 - val_loss: 145.5712\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 145.7279 - val_loss: 145.2208\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 139.3494 - val_loss: 144.7892\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 144.8272 - val_loss: 144.1833\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 134.6777 - val_loss: 143.6765\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 139.4040 - val_loss: 143.4178\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 132.2563 - val_loss: 142.9776\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 121.7321 - val_loss: 142.5292\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 124.7069 - val_loss: 142.0825\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 137.7015 - val_loss: 141.6964\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 138.4072 - val_loss: 141.4705\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 138.2677 - val_loss: 141.0930\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 134.2748 - val_loss: 140.7728\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 130.6509 - val_loss: 140.5536\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 124.5971 - val_loss: 140.2487\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 129.4419 - val_loss: 139.8284\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 132.9719 - val_loss: 139.5330\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 140.0001 - val_loss: 138.8014\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 120.3022 - val_loss: 138.6669\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 128.7185 - val_loss: 138.1987\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 142.6415 - val_loss: 137.9615\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 121.5141 - val_loss: 137.8952\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 123.6649 - val_loss: 137.2297\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  38 \n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 14ms/step - loss: 1556.1948 - val_loss: 1606.4874\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1628.1684 - val_loss: 1591.3735\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1534.6031 - val_loss: 1576.4548\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1547.5370 - val_loss: 1560.8606\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1574.2931 - val_loss: 1544.0442\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1461.3998 - val_loss: 1525.6741\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1440.3988 - val_loss: 1503.7067\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1499.6445 - val_loss: 1478.1545\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1502.3910 - val_loss: 1449.3243\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1427.8836 - val_loss: 1416.6400\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1373.3069 - val_loss: 1379.1860\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1330.6297 - val_loss: 1337.3431\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1349.2386 - val_loss: 1290.5745\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1247.5107 - val_loss: 1240.0266\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1163.5712 - val_loss: 1185.0872\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1163.2359 - val_loss: 1125.7579\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1044.1104 - val_loss: 1062.4264\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 964.4645 - val_loss: 995.2141\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 920.7296 - val_loss: 924.8557\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 845.2730 - val_loss: 853.3870\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 830.2934 - val_loss: 780.9122\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 702.9817 - val_loss: 708.2912\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 659.6766 - val_loss: 637.5641\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 567.4881 - val_loss: 570.4526\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 518.6559 - val_loss: 505.5854\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 455.2243 - val_loss: 446.3410\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 386.6430 - val_loss: 394.4399\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 352.7425 - val_loss: 348.4219\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 308.9887 - val_loss: 310.2105\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 282.1588 - val_loss: 278.8987\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 253.9292 - val_loss: 253.9216\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 222.2171 - val_loss: 234.7283\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 212.8530 - val_loss: 219.6887\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 207.5951 - val_loss: 207.2870\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 197.1428 - val_loss: 198.1989\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 188.1850 - val_loss: 190.9114\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 194.5891 - val_loss: 185.1036\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 168.8836 - val_loss: 180.5505\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 173.3538 - val_loss: 176.6550\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 165.7521 - val_loss: 173.2911\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 169.2743 - val_loss: 171.0091\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 168.0767 - val_loss: 168.8616\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 166.3044 - val_loss: 166.8741\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 172.2193 - val_loss: 165.3404\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 162.4125 - val_loss: 163.9339\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 165.3633 - val_loss: 162.6284\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 148.8836 - val_loss: 161.4660\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 148.2465 - val_loss: 160.3578\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 156.5216 - val_loss: 159.2385\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 161.5534 - val_loss: 158.1793\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 143.3768 - val_loss: 157.1666\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 152.0327 - val_loss: 156.2882\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 141.5792 - val_loss: 155.4430\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 155.5627 - val_loss: 154.5938\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 149.0798 - val_loss: 153.8207\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 150.1306 - val_loss: 152.8989\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 143.0356 - val_loss: 152.0889\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 150.7802 - val_loss: 151.3245\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 147.7080 - val_loss: 150.5616\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 145.1234 - val_loss: 149.8563\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 132.4885 - val_loss: 149.0080\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 140.9098 - val_loss: 148.3999\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 140.8084 - val_loss: 147.6829\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 125.0522 - val_loss: 147.1033\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 135.4591 - val_loss: 146.4732\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 137.7174 - val_loss: 145.7216\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 133.9406 - val_loss: 145.1127\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 130.8596 - val_loss: 144.5556\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 139.9096 - val_loss: 143.9162\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 131.0550 - val_loss: 143.2909\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 137.1979 - val_loss: 142.7028\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 129.1290 - val_loss: 142.0591\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 132.1296 - val_loss: 141.4931\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 134.5342 - val_loss: 140.8528\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 127.5126 - val_loss: 140.2373\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 130.4247 - val_loss: 139.6825\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 131.0585 - val_loss: 139.1915\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 128.3768 - val_loss: 138.5725\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 133.6442 - val_loss: 138.0268\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 120.5339 - val_loss: 137.6477\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 133.1152 - val_loss: 137.1239\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 129.1938 - val_loss: 136.5839\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 120.9008 - val_loss: 136.0806\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 112.9596 - val_loss: 135.6502\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 126.9018 - val_loss: 135.1004\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 117.0696 - val_loss: 134.7856\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 114.4874 - val_loss: 134.1691\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 129.8735 - val_loss: 133.7075\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 121.5846 - val_loss: 133.2443\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 121.0736 - val_loss: 132.7526\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 117.3698 - val_loss: 132.2537\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 116.9013 - val_loss: 131.7615\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 120.9765 - val_loss: 131.4070\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 130.5218 - val_loss: 130.9427\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 117.7677 - val_loss: 130.3989\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 113.7021 - val_loss: 129.9212\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 110.4687 - val_loss: 129.4307\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 115.4245 - val_loss: 128.8469\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 125.7392 - val_loss: 128.5295\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 113.6577 - val_loss: 128.0703\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  39 \n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 15ms/step - loss: 1470.9525 - val_loss: 1442.3577\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1474.2022 - val_loss: 1418.1123\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1407.9119 - val_loss: 1389.9929\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1384.8396 - val_loss: 1356.7852\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1376.2490 - val_loss: 1317.2732\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1263.8730 - val_loss: 1271.2251\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1231.4600 - val_loss: 1218.3271\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1192.9521 - val_loss: 1158.9971\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1202.5689 - val_loss: 1092.2126\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1094.7840 - val_loss: 1021.4341\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1039.6696 - val_loss: 946.2539\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 897.0471 - val_loss: 869.2755\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 849.0512 - val_loss: 789.3824\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 763.9174 - val_loss: 711.4699\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 661.8972 - val_loss: 636.1456\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 642.2165 - val_loss: 562.8365\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 558.2016 - val_loss: 496.9330\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 528.6245 - val_loss: 438.3250\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 425.4809 - val_loss: 389.1276\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 391.6631 - val_loss: 346.1302\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 349.0364 - val_loss: 313.0752\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 296.8461 - val_loss: 288.3117\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 298.6651 - val_loss: 268.6849\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 273.2783 - val_loss: 255.8191\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 249.1557 - val_loss: 246.9674\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 231.3185 - val_loss: 240.6213\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 228.3564 - val_loss: 236.0641\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 231.5530 - val_loss: 233.3445\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 200.3970 - val_loss: 230.3176\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 185.3066 - val_loss: 227.9668\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 175.2593 - val_loss: 225.4247\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 195.9088 - val_loss: 223.0513\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 169.9866 - val_loss: 221.4347\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 185.8812 - val_loss: 218.8423\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 173.9390 - val_loss: 217.6652\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 178.6656 - val_loss: 214.5793\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 171.1685 - val_loss: 212.4983\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 158.0376 - val_loss: 210.4273\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 163.3701 - val_loss: 208.2273\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 168.7261 - val_loss: 205.8080\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 153.8548 - val_loss: 203.9451\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 165.4621 - val_loss: 200.8255\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 171.7484 - val_loss: 198.8380\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 163.7427 - val_loss: 197.0532\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 158.0729 - val_loss: 195.3187\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 165.1974 - val_loss: 193.4360\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 163.2806 - val_loss: 192.3353\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 165.4660 - val_loss: 190.3104\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 149.9892 - val_loss: 189.1614\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 161.9143 - val_loss: 186.2235\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 140.5384 - val_loss: 186.0424\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 146.3129 - val_loss: 184.2594\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 148.9709 - val_loss: 182.3343\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 140.7226 - val_loss: 181.8151\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 133.7612 - val_loss: 180.3564\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 136.5672 - val_loss: 178.4390\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 144.4904 - val_loss: 175.6824\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 140.8465 - val_loss: 175.6924\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 142.7006 - val_loss: 173.9438\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 129.7891 - val_loss: 174.1597\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 151.7338 - val_loss: 171.8080\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 138.8872 - val_loss: 171.1467\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 138.7158 - val_loss: 169.8306\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 139.2545 - val_loss: 168.7456\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 136.0979 - val_loss: 167.4895\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 133.9398 - val_loss: 167.5787\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 129.4604 - val_loss: 166.6550\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 138.2580 - val_loss: 165.3315\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 131.2529 - val_loss: 164.3042\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 132.9476 - val_loss: 164.3078\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 131.7092 - val_loss: 163.0583\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 137.5319 - val_loss: 161.1835\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 132.3449 - val_loss: 161.0031\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 125.0645 - val_loss: 160.4721\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 128.1386 - val_loss: 158.6537\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 128.1294 - val_loss: 157.9269\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 134.5401 - val_loss: 158.0092\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 131.3485 - val_loss: 156.8435\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 123.7004 - val_loss: 157.0371\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 138.4634 - val_loss: 156.9897\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 132.5889 - val_loss: 155.2122\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 124.3014 - val_loss: 154.6748\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 129.0296 - val_loss: 154.1784\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 122.4123 - val_loss: 154.4652\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 121.0644 - val_loss: 152.9853\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 129.0659 - val_loss: 152.8003\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 113.1813 - val_loss: 152.1604\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 135.6772 - val_loss: 151.3631\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 126.1686 - val_loss: 151.0056\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 127.4312 - val_loss: 150.2614\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 125.7375 - val_loss: 150.0191\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 132.5426 - val_loss: 150.1323\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 122.9238 - val_loss: 149.0481\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 129.7051 - val_loss: 148.7242\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 119.7701 - val_loss: 148.3722\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 124.7551 - val_loss: 148.6222\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 122.7543 - val_loss: 147.4152\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 124.0308 - val_loss: 147.2666\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 117.8085 - val_loss: 146.3284\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 113.2542 - val_loss: 146.1030\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  40 \n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 16ms/step - loss: 1629.4397 - val_loss: 1470.6469\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1648.1228 - val_loss: 1451.3636\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1475.9315 - val_loss: 1430.1968\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1547.9766 - val_loss: 1406.0793\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1523.6671 - val_loss: 1377.7695\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1429.0779 - val_loss: 1346.1138\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1473.9604 - val_loss: 1309.4993\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1428.6175 - val_loss: 1268.3221\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1438.3026 - val_loss: 1221.7129\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1335.0152 - val_loss: 1169.2239\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1270.7929 - val_loss: 1110.3127\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1155.4826 - val_loss: 1045.4683\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1155.3890 - val_loss: 973.4631\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1022.8648 - val_loss: 896.6918\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 962.6418 - val_loss: 817.7360\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 860.4204 - val_loss: 737.6529\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 828.3125 - val_loss: 660.0428\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 685.6419 - val_loss: 587.0822\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 581.3258 - val_loss: 519.5093\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 547.0838 - val_loss: 458.9721\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 491.4416 - val_loss: 408.4659\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 494.0050 - val_loss: 366.1552\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 398.3365 - val_loss: 333.5370\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 339.0068 - val_loss: 308.8783\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 310.1868 - val_loss: 289.4896\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 322.4739 - val_loss: 272.6198\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 269.0674 - val_loss: 260.5547\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 289.2509 - val_loss: 250.3822\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 290.9286 - val_loss: 242.4663\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 272.6279 - val_loss: 234.6017\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 244.4751 - val_loss: 228.2088\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 258.5641 - val_loss: 222.7335\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 223.2746 - val_loss: 217.5806\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 252.2736 - val_loss: 212.6101\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 226.5012 - val_loss: 208.3383\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 228.3033 - val_loss: 204.7084\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 217.5144 - val_loss: 201.7500\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 183.7172 - val_loss: 198.5579\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 180.3381 - val_loss: 195.7121\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 199.2099 - val_loss: 193.2090\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 217.0161 - val_loss: 190.7158\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 181.1247 - val_loss: 188.5663\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 173.5640 - val_loss: 186.3991\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 178.1207 - val_loss: 184.7320\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 195.1894 - val_loss: 182.6526\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 179.3534 - val_loss: 180.7068\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 190.5283 - val_loss: 179.1158\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 175.3601 - val_loss: 177.7613\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 172.7967 - val_loss: 176.1198\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 168.7117 - val_loss: 174.6246\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 178.2185 - val_loss: 173.5443\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 173.1955 - val_loss: 172.2626\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 158.6939 - val_loss: 171.0717\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 150.3170 - val_loss: 169.8344\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 173.0773 - val_loss: 168.6661\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 164.2492 - val_loss: 167.5010\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 160.5447 - val_loss: 166.2646\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 151.7346 - val_loss: 165.3041\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 167.4508 - val_loss: 163.9298\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 154.1978 - val_loss: 163.0729\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 160.8572 - val_loss: 161.9002\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 155.8186 - val_loss: 160.8540\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 154.3439 - val_loss: 159.5222\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 145.8714 - val_loss: 158.6909\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 139.3783 - val_loss: 157.5466\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 144.1419 - val_loss: 156.6138\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 152.9691 - val_loss: 155.7406\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 150.7270 - val_loss: 154.7841\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 153.6539 - val_loss: 153.8552\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 150.5329 - val_loss: 153.1909\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 150.0021 - val_loss: 152.3998\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 139.9944 - val_loss: 151.6869\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 145.6353 - val_loss: 150.8683\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 145.9804 - val_loss: 150.1559\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 151.2107 - val_loss: 149.4210\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 136.0266 - val_loss: 148.6449\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 142.2132 - val_loss: 148.0858\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 134.5155 - val_loss: 147.2849\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 136.4098 - val_loss: 146.7583\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 126.1853 - val_loss: 146.1483\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 139.5947 - val_loss: 145.2902\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 131.3880 - val_loss: 144.5287\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 127.0245 - val_loss: 143.9659\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 129.4105 - val_loss: 143.4834\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 124.9129 - val_loss: 142.7956\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 132.4075 - val_loss: 142.1747\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 126.2265 - val_loss: 141.5365\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 129.6095 - val_loss: 140.9280\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 127.4389 - val_loss: 140.4468\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 132.9166 - val_loss: 139.6955\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 131.7276 - val_loss: 139.3504\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 126.3132 - val_loss: 138.8230\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 123.9736 - val_loss: 138.2136\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 118.0465 - val_loss: 137.6338\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 127.5777 - val_loss: 137.1704\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 123.0723 - val_loss: 136.5989\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 123.2568 - val_loss: 136.1871\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 117.8350 - val_loss: 135.9033\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 122.8322 - val_loss: 135.1885\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 123.6952 - val_loss: 134.7656\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  41 \n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 16ms/step - loss: 1488.9528 - val_loss: 1567.5411\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1572.9456 - val_loss: 1554.1057\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1566.2357 - val_loss: 1538.7208\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1520.4756 - val_loss: 1520.4038\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1477.7492 - val_loss: 1497.6505\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1467.6207 - val_loss: 1467.7061\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1347.8256 - val_loss: 1430.3015\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1382.0285 - val_loss: 1382.1530\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1378.2479 - val_loss: 1324.1031\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1356.5738 - val_loss: 1255.4426\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1280.9585 - val_loss: 1178.0099\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1178.1183 - val_loss: 1090.4388\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1088.6737 - val_loss: 995.1318\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 960.9954 - val_loss: 894.8966\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 876.9117 - val_loss: 789.7510\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 779.1578 - val_loss: 683.3611\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 641.9694 - val_loss: 580.3944\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 518.9189 - val_loss: 482.8704\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 473.7942 - val_loss: 397.3079\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 374.3689 - val_loss: 333.8572\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 309.7508 - val_loss: 289.7346\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 237.2481 - val_loss: 262.3093\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 238.7768 - val_loss: 247.1177\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 207.3261 - val_loss: 239.4165\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 198.4060 - val_loss: 236.3235\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 190.5529 - val_loss: 234.2640\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 190.5818 - val_loss: 232.1016\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 181.0289 - val_loss: 230.7753\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 162.2896 - val_loss: 228.1764\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 167.3686 - val_loss: 224.7492\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 186.0874 - val_loss: 222.0878\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 169.9262 - val_loss: 220.5202\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 158.2403 - val_loss: 219.1326\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 181.3889 - val_loss: 216.8093\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 159.8042 - val_loss: 214.6550\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 146.3014 - val_loss: 212.5955\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 168.3843 - val_loss: 210.4241\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 167.3449 - val_loss: 209.3210\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 159.0152 - val_loss: 207.4375\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 151.3178 - val_loss: 205.6174\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 150.7760 - val_loss: 204.5061\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 161.0975 - val_loss: 202.1177\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 153.5736 - val_loss: 201.1664\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 131.2265 - val_loss: 199.6214\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 156.0286 - val_loss: 197.8875\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 147.6012 - val_loss: 196.1907\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 145.7168 - val_loss: 195.0191\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 127.5890 - val_loss: 194.3564\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 135.6650 - val_loss: 192.0518\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 143.2358 - val_loss: 189.5945\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 126.5917 - val_loss: 189.0289\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 122.8008 - val_loss: 187.1414\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 133.1225 - val_loss: 187.0782\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 121.5653 - val_loss: 183.7076\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 134.2676 - val_loss: 182.4623\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 134.2547 - val_loss: 181.8738\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 123.4273 - val_loss: 179.9898\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 129.4631 - val_loss: 179.2862\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 131.1228 - val_loss: 176.8844\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 132.4476 - val_loss: 175.7968\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 123.2076 - val_loss: 174.5252\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 124.5161 - val_loss: 173.6113\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 129.5727 - val_loss: 172.1926\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 115.5859 - val_loss: 171.0245\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 128.4583 - val_loss: 169.8408\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 118.3459 - val_loss: 168.5481\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 122.4383 - val_loss: 167.1901\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 117.6925 - val_loss: 166.1236\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 122.1477 - val_loss: 164.6325\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 112.1203 - val_loss: 163.4624\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 129.3123 - val_loss: 161.3145\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 122.9254 - val_loss: 160.9472\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 116.3744 - val_loss: 159.3473\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 112.3333 - val_loss: 158.9000\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 118.2377 - val_loss: 156.6733\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 100.4618 - val_loss: 154.9707\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 109.8158 - val_loss: 153.8257\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 105.5798 - val_loss: 152.9006\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 106.7053 - val_loss: 151.3207\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 110.9864 - val_loss: 149.1308\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 108.5317 - val_loss: 147.9269\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 105.3793 - val_loss: 146.2591\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 98.2838 - val_loss: 145.5004\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 103.1252 - val_loss: 143.6620\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 105.8743 - val_loss: 142.2299\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 110.3868 - val_loss: 140.5433\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 97.8982 - val_loss: 139.1489\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 104.5499 - val_loss: 138.1265\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 90.9963 - val_loss: 137.1126\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 103.0811 - val_loss: 133.9588\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 100.8092 - val_loss: 133.5197\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 90.6311 - val_loss: 132.5237\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 88.8655 - val_loss: 131.4029\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 98.6654 - val_loss: 129.5321\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 90.3578 - val_loss: 127.8971\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 92.8380 - val_loss: 126.4975\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 86.3124 - val_loss: 125.1955\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 88.0637 - val_loss: 124.2175\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 97.1417 - val_loss: 122.5952\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 89.8668 - val_loss: 120.9203\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  42 \n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 18ms/step - loss: 1521.0977 - val_loss: 1521.2869\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1477.7374 - val_loss: 1502.6033\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1485.2439 - val_loss: 1483.8494\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1455.3988 - val_loss: 1464.1610\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1446.1949 - val_loss: 1441.9658\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1456.1424 - val_loss: 1417.2880\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1368.5012 - val_loss: 1389.7466\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1282.4898 - val_loss: 1357.7418\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1374.6020 - val_loss: 1321.2826\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1318.5083 - val_loss: 1279.9950\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1231.0869 - val_loss: 1233.6852\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1300.9813 - val_loss: 1181.1149\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1143.7119 - val_loss: 1124.0210\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1121.0801 - val_loss: 1060.7791\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1028.9126 - val_loss: 992.3558\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 974.5572 - val_loss: 920.7985\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 872.6673 - val_loss: 847.8598\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 799.5822 - val_loss: 772.4687\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 734.1806 - val_loss: 698.4799\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 671.4556 - val_loss: 625.8076\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 595.3570 - val_loss: 558.5228\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 516.9093 - val_loss: 495.1596\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 474.0345 - val_loss: 440.0281\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 453.1923 - val_loss: 390.0267\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 354.1127 - val_loss: 350.6749\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 313.9444 - val_loss: 317.5351\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 318.8543 - val_loss: 290.6405\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 249.0154 - val_loss: 270.7323\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 232.6492 - val_loss: 255.3163\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 210.1319 - val_loss: 244.0031\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 200.6273 - val_loss: 235.4352\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 201.7231 - val_loss: 228.4521\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 178.5243 - val_loss: 223.5665\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 204.6976 - val_loss: 219.3113\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 161.5615 - val_loss: 215.8518\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 185.1208 - val_loss: 213.1775\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 162.5551 - val_loss: 210.3293\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 191.0026 - val_loss: 207.8733\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 173.7988 - val_loss: 205.9294\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 173.7200 - val_loss: 203.7515\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 176.9168 - val_loss: 201.5272\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 187.9493 - val_loss: 199.6325\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 160.0105 - val_loss: 197.8101\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 151.6569 - val_loss: 195.6315\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 170.9067 - val_loss: 193.8781\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 152.6738 - val_loss: 192.2580\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 172.1049 - val_loss: 190.6250\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 162.2669 - val_loss: 189.0471\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 144.9629 - val_loss: 187.5452\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 154.4361 - val_loss: 186.0303\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 150.9005 - val_loss: 184.4995\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 150.8976 - val_loss: 183.1766\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 157.2101 - val_loss: 181.5492\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 147.2411 - val_loss: 180.2581\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 142.3960 - val_loss: 179.2609\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 145.8842 - val_loss: 178.0777\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 147.1290 - val_loss: 176.7185\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 155.7095 - val_loss: 175.5254\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 135.0123 - val_loss: 174.6298\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 142.6473 - val_loss: 173.2426\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 144.2757 - val_loss: 171.9738\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 149.3593 - val_loss: 171.2584\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 141.9899 - val_loss: 170.1756\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 143.8101 - val_loss: 168.8338\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 132.4736 - val_loss: 168.1561\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 126.4726 - val_loss: 167.0326\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 145.6382 - val_loss: 166.2385\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 138.9031 - val_loss: 165.3481\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 131.0259 - val_loss: 164.3362\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 135.8963 - val_loss: 163.9810\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 123.0020 - val_loss: 163.0313\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 121.4555 - val_loss: 162.0953\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 135.2913 - val_loss: 161.3560\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 127.7222 - val_loss: 160.5633\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 130.1890 - val_loss: 159.6858\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 124.9713 - val_loss: 158.9211\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 120.8971 - val_loss: 158.3473\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 138.2107 - val_loss: 157.6203\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 133.3088 - val_loss: 157.1292\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 139.8769 - val_loss: 156.1528\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 123.5070 - val_loss: 155.5963\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 124.1676 - val_loss: 155.1033\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 125.9618 - val_loss: 154.3697\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 121.9983 - val_loss: 153.4256\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 126.9959 - val_loss: 152.7812\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 129.1551 - val_loss: 152.0364\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 114.0657 - val_loss: 151.5468\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 125.9060 - val_loss: 151.3420\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 119.9952 - val_loss: 150.5496\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 125.1032 - val_loss: 149.5059\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 125.6786 - val_loss: 148.8893\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 119.4231 - val_loss: 148.2643\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 123.7299 - val_loss: 147.6391\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 124.4601 - val_loss: 147.2840\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 125.0801 - val_loss: 146.0189\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 121.3850 - val_loss: 145.8424\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 115.7959 - val_loss: 145.1990\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 112.3817 - val_loss: 145.1994\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 117.0313 - val_loss: 144.2967\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 125.4921 - val_loss: 143.6693\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  43 \n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 13ms/step - loss: 1513.0046 - val_loss: 1422.6554\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1538.3417 - val_loss: 1405.7487\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1507.8976 - val_loss: 1385.9233\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1491.0763 - val_loss: 1363.6714\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1485.1085 - val_loss: 1337.9821\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1379.7717 - val_loss: 1308.2550\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 1393.6561 - val_loss: 1274.2054\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1333.4067 - val_loss: 1236.1670\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1223.2100 - val_loss: 1190.9818\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1321.5418 - val_loss: 1138.7020\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1164.2404 - val_loss: 1080.1091\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1200.7716 - val_loss: 1014.6674\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 988.7282 - val_loss: 945.2890\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 903.0856 - val_loss: 871.2576\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 897.9430 - val_loss: 792.8422\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 809.3754 - val_loss: 717.0726\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 701.3071 - val_loss: 643.4646\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 638.8797 - val_loss: 573.7798\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 634.1859 - val_loss: 509.3159\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 511.0090 - val_loss: 454.2118\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 429.4484 - val_loss: 407.7267\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 413.9699 - val_loss: 367.2689\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 387.1155 - val_loss: 334.7912\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 314.3349 - val_loss: 309.9315\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 317.5933 - val_loss: 289.4431\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 291.5123 - val_loss: 273.4342\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 278.8043 - val_loss: 259.9734\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 263.2014 - val_loss: 249.7373\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 244.5119 - val_loss: 241.3166\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 249.4900 - val_loss: 232.8346\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 234.0189 - val_loss: 226.2397\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 237.7526 - val_loss: 220.6260\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 221.2556 - val_loss: 215.3031\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 230.5741 - val_loss: 210.8063\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 208.9021 - val_loss: 206.8048\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 214.5241 - val_loss: 202.9028\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 231.2544 - val_loss: 199.6512\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 203.0533 - val_loss: 196.7799\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 208.9270 - val_loss: 193.8568\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 217.8970 - val_loss: 191.3363\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 220.6409 - val_loss: 188.9236\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 169.2036 - val_loss: 186.8553\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 186.4647 - val_loss: 185.0077\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 178.6628 - val_loss: 182.8770\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 179.5746 - val_loss: 180.7152\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 184.0275 - val_loss: 179.0174\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 160.7747 - val_loss: 177.3538\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 168.8595 - val_loss: 175.6003\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 190.1610 - val_loss: 174.0508\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 181.5112 - val_loss: 172.4540\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 168.4050 - val_loss: 170.8899\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 168.6208 - val_loss: 169.4176\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 160.4879 - val_loss: 168.0819\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 187.5541 - val_loss: 166.7850\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 170.9032 - val_loss: 165.2133\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 170.6059 - val_loss: 164.1025\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 167.8455 - val_loss: 162.8723\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 171.8676 - val_loss: 161.4503\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 167.6796 - val_loss: 160.4128\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 163.6789 - val_loss: 159.3396\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 159.4149 - val_loss: 158.3553\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 150.2404 - val_loss: 157.5798\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 162.6928 - val_loss: 156.7110\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 141.5023 - val_loss: 155.9116\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 157.1032 - val_loss: 154.9737\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 156.3668 - val_loss: 153.7228\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 156.3253 - val_loss: 152.8884\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 149.8751 - val_loss: 151.9590\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 162.9299 - val_loss: 151.0162\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 146.8835 - val_loss: 150.0555\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 151.5039 - val_loss: 149.1631\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 148.7558 - val_loss: 148.4202\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 151.1195 - val_loss: 147.5270\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 148.2725 - val_loss: 146.9272\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 150.6782 - val_loss: 146.4298\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 146.6223 - val_loss: 145.7769\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 137.6604 - val_loss: 145.2013\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 147.4535 - val_loss: 144.6029\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 148.3951 - val_loss: 144.2508\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 137.9311 - val_loss: 143.6458\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 146.4835 - val_loss: 142.9980\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 146.9037 - val_loss: 142.4562\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 147.5600 - val_loss: 141.9697\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 139.4730 - val_loss: 141.3102\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 128.7083 - val_loss: 141.0335\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 147.4712 - val_loss: 140.5397\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 131.7434 - val_loss: 139.9228\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 145.9162 - val_loss: 139.3866\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 142.2249 - val_loss: 139.1466\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 141.9020 - val_loss: 138.6111\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 133.3601 - val_loss: 138.2050\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 140.8297 - val_loss: 137.8108\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 133.6249 - val_loss: 137.3275\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 128.8400 - val_loss: 136.9931\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 138.6484 - val_loss: 136.7356\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 142.0160 - val_loss: 136.1618\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 140.6346 - val_loss: 136.0311\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 133.9340 - val_loss: 135.5815\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 141.0356 - val_loss: 135.3115\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 133.3435 - val_loss: 134.9206\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  44 \n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 15ms/step - loss: 1568.2158 - val_loss: 1416.7065\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1606.6611 - val_loss: 1397.6692\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1559.1250 - val_loss: 1377.0624\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1546.2734 - val_loss: 1354.9656\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1538.5725 - val_loss: 1329.9791\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1444.0531 - val_loss: 1301.4309\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1530.7364 - val_loss: 1267.6908\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1386.3271 - val_loss: 1228.2893\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1274.8712 - val_loss: 1181.0336\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 1288.2802 - val_loss: 1124.3463\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1276.6792 - val_loss: 1058.7100\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1155.7329 - val_loss: 985.4847\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1069.9295 - val_loss: 906.0262\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1037.8319 - val_loss: 819.4561\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 869.8927 - val_loss: 734.4530\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 819.9795 - val_loss: 646.6045\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 685.4184 - val_loss: 565.4221\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 623.2907 - val_loss: 489.5154\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 567.3986 - val_loss: 425.0197\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 445.9438 - val_loss: 372.6534\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 461.9837 - val_loss: 328.3683\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 354.7838 - val_loss: 297.4066\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 317.1664 - val_loss: 273.6260\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 304.1464 - val_loss: 257.1334\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 295.2731 - val_loss: 244.7357\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 277.2198 - val_loss: 235.7273\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 261.9777 - val_loss: 228.8421\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 213.1389 - val_loss: 223.6573\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 233.3509 - val_loss: 219.0102\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 209.8091 - val_loss: 214.9117\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 218.2415 - val_loss: 211.0903\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 203.1136 - val_loss: 207.9322\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 209.0494 - val_loss: 204.5984\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 196.4910 - val_loss: 202.2157\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 201.8475 - val_loss: 199.3937\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 184.8083 - val_loss: 197.2251\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 197.1220 - val_loss: 195.2451\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 185.2729 - val_loss: 192.8659\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 175.2323 - val_loss: 191.1039\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 182.5866 - val_loss: 189.6141\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 179.0509 - val_loss: 188.2379\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 169.9868 - val_loss: 186.6385\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 166.4937 - val_loss: 184.7995\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 170.6373 - val_loss: 182.4832\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 169.4072 - val_loss: 181.0555\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 177.3956 - val_loss: 179.6646\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 165.6066 - val_loss: 179.2524\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 160.2871 - val_loss: 177.6295\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 154.3758 - val_loss: 176.5279\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 169.2382 - val_loss: 174.9633\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 157.0026 - val_loss: 174.0880\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 160.5742 - val_loss: 173.2324\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 153.3626 - val_loss: 171.5799\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 139.5090 - val_loss: 170.2529\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 136.6753 - val_loss: 169.3059\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 144.0981 - val_loss: 168.8789\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 154.3071 - val_loss: 167.8158\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 156.7589 - val_loss: 166.4693\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 158.2529 - val_loss: 164.8583\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 158.1907 - val_loss: 164.9589\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 138.3518 - val_loss: 163.5968\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 152.0724 - val_loss: 163.4393\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 140.1635 - val_loss: 163.1894\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 137.8119 - val_loss: 161.6942\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 131.2455 - val_loss: 160.2747\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 137.9826 - val_loss: 159.8521\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 128.6828 - val_loss: 159.4091\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 140.3658 - val_loss: 158.5951\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 131.4736 - val_loss: 158.1814\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 145.4579 - val_loss: 157.1223\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 134.4064 - val_loss: 156.5963\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 132.5709 - val_loss: 156.6127\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 135.1904 - val_loss: 155.3765\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 137.7682 - val_loss: 154.5799\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 133.0219 - val_loss: 154.8588\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 123.7906 - val_loss: 154.3386\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 137.7526 - val_loss: 153.4146\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 137.2442 - val_loss: 152.3388\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 138.1263 - val_loss: 152.7997\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 128.9757 - val_loss: 152.2963\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 128.2167 - val_loss: 150.8783\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 134.5964 - val_loss: 151.3366\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 137.7808 - val_loss: 150.8362\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 124.8398 - val_loss: 150.2094\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 137.6816 - val_loss: 148.8009\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 119.0222 - val_loss: 149.0022\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 131.2632 - val_loss: 149.0036\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 126.3881 - val_loss: 148.8217\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 124.2149 - val_loss: 147.9753\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 127.2624 - val_loss: 147.5498\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 133.0602 - val_loss: 147.8406\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 116.6830 - val_loss: 147.3855\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 118.9742 - val_loss: 146.4902\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 123.6719 - val_loss: 146.0213\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 115.4586 - val_loss: 145.8717\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 123.1424 - val_loss: 144.8837\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 123.2472 - val_loss: 145.7887\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 115.9658 - val_loss: 144.7300\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 114.8791 - val_loss: 144.6171\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 127.4797 - val_loss: 143.7151\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  45 \n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 16ms/step - loss: 1571.3975 - val_loss: 1712.7867\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1575.0296 - val_loss: 1697.4319\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1538.5202 - val_loss: 1682.9752\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1606.4402 - val_loss: 1668.1124\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1534.0305 - val_loss: 1653.0155\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1611.5311 - val_loss: 1636.2411\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1525.3511 - val_loss: 1617.6718\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1584.7828 - val_loss: 1595.5795\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1487.1903 - val_loss: 1570.2412\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1546.9731 - val_loss: 1539.8575\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1463.3925 - val_loss: 1504.7522\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1433.2791 - val_loss: 1462.9893\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1349.3419 - val_loss: 1415.5447\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1331.5932 - val_loss: 1360.9999\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1338.3892 - val_loss: 1300.7473\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1298.5408 - val_loss: 1232.2217\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1171.6540 - val_loss: 1159.3860\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1102.8215 - val_loss: 1082.7659\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1041.5670 - val_loss: 1001.8985\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 973.2640 - val_loss: 917.5342\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 775.7076 - val_loss: 836.0925\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 746.8941 - val_loss: 751.4029\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 706.5284 - val_loss: 671.4925\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 656.3986 - val_loss: 597.0815\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 604.0352 - val_loss: 532.3898\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 461.7079 - val_loss: 479.3672\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 443.4414 - val_loss: 435.1535\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 406.6685 - val_loss: 399.4875\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 330.6158 - val_loss: 374.7844\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 312.1161 - val_loss: 354.4380\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 339.0270 - val_loss: 339.0369\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 285.2033 - val_loss: 327.2528\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 284.4382 - val_loss: 317.1049\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 273.5018 - val_loss: 308.0928\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 251.9159 - val_loss: 300.7251\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 241.0995 - val_loss: 293.5312\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 241.6865 - val_loss: 287.5019\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 250.4714 - val_loss: 280.2002\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 222.6387 - val_loss: 274.4646\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 206.4442 - val_loss: 269.8099\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 200.1922 - val_loss: 263.7130\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 220.9690 - val_loss: 258.8078\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 222.0760 - val_loss: 252.9565\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 205.0408 - val_loss: 249.3416\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 206.7724 - val_loss: 243.8798\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 194.6240 - val_loss: 239.8615\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 206.8958 - val_loss: 235.7141\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 209.2089 - val_loss: 231.6716\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 196.8947 - val_loss: 227.8621\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 182.1620 - val_loss: 224.1002\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 178.4301 - val_loss: 220.5031\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 178.1822 - val_loss: 216.9896\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 187.4456 - val_loss: 214.2425\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 192.4895 - val_loss: 210.8068\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 182.9882 - val_loss: 207.6569\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 175.1850 - val_loss: 205.1208\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 179.8873 - val_loss: 202.9474\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 176.4316 - val_loss: 200.2027\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 164.5496 - val_loss: 197.5711\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 174.2645 - val_loss: 195.5418\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 162.0773 - val_loss: 193.0327\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 162.5629 - val_loss: 190.5348\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 158.6813 - val_loss: 188.6407\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 152.5742 - val_loss: 186.3570\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 156.7826 - val_loss: 184.0077\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 140.4715 - val_loss: 182.6020\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 169.6927 - val_loss: 180.8055\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 160.1171 - val_loss: 178.7085\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 147.1712 - val_loss: 177.6932\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 152.4699 - val_loss: 176.2261\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 135.3617 - val_loss: 174.0191\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 143.1003 - val_loss: 172.5078\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 153.7972 - val_loss: 171.0841\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 150.1443 - val_loss: 170.3628\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 142.9361 - val_loss: 169.1009\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 147.7833 - val_loss: 168.3115\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 142.6031 - val_loss: 167.8324\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 129.6323 - val_loss: 166.8888\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 139.9367 - val_loss: 165.1366\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 143.8696 - val_loss: 164.3663\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 137.1736 - val_loss: 164.2562\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 127.5665 - val_loss: 162.7753\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 141.4273 - val_loss: 161.9899\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 137.3111 - val_loss: 161.3833\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 130.0496 - val_loss: 161.0126\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 132.8702 - val_loss: 160.5382\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 142.2802 - val_loss: 159.7202\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 142.2019 - val_loss: 158.5608\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 129.9354 - val_loss: 158.0057\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 135.2360 - val_loss: 157.9751\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 124.6160 - val_loss: 157.4461\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 142.0527 - val_loss: 156.5283\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 126.1210 - val_loss: 156.9231\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 124.9540 - val_loss: 155.8732\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 139.8313 - val_loss: 154.8612\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 138.6031 - val_loss: 154.6902\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 126.7334 - val_loss: 154.4373\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 132.6953 - val_loss: 153.1898\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 135.5748 - val_loss: 153.2377\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 131.6069 - val_loss: 152.6718\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  46 \n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 23ms/step - loss: 1562.0592 - val_loss: 1618.6964\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1597.9075 - val_loss: 1603.9150\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1559.6435 - val_loss: 1590.3246\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1569.3279 - val_loss: 1576.8062\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1607.6167 - val_loss: 1562.2543\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1546.4271 - val_loss: 1546.5110\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1513.5002 - val_loss: 1528.4382\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1534.7904 - val_loss: 1507.4238\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1417.2371 - val_loss: 1483.4116\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1503.3248 - val_loss: 1455.3667\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1392.9368 - val_loss: 1423.7349\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1485.8861 - val_loss: 1387.8118\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1341.4682 - val_loss: 1348.1520\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1297.3199 - val_loss: 1304.5011\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1242.4248 - val_loss: 1256.1405\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1217.4679 - val_loss: 1202.6448\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1205.0148 - val_loss: 1145.0305\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1058.7972 - val_loss: 1084.6190\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1041.5399 - val_loss: 1018.6575\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 945.4661 - val_loss: 950.6575\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 902.2407 - val_loss: 880.9344\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 853.4756 - val_loss: 809.5146\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 820.1614 - val_loss: 737.1342\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 672.6298 - val_loss: 666.1168\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 627.3973 - val_loss: 598.1014\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 606.5670 - val_loss: 534.3477\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 535.7387 - val_loss: 477.3491\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 466.9358 - val_loss: 427.2686\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 379.7450 - val_loss: 383.2481\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 375.6349 - val_loss: 345.6560\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 362.6249 - val_loss: 314.3336\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 330.6846 - val_loss: 288.6240\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 273.5427 - val_loss: 268.3346\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 275.3207 - val_loss: 250.5927\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 254.4829 - val_loss: 237.0498\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 217.7534 - val_loss: 226.5026\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 231.6703 - val_loss: 217.6093\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 192.0762 - val_loss: 210.7232\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 202.9082 - val_loss: 205.0931\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 187.9146 - val_loss: 200.6666\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 191.2874 - val_loss: 196.3638\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 179.9680 - val_loss: 193.4063\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 191.1737 - val_loss: 190.9466\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 173.5238 - val_loss: 188.7846\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 186.7688 - val_loss: 187.1151\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 163.4958 - val_loss: 185.5755\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 164.4119 - val_loss: 184.4986\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 157.0160 - val_loss: 183.2719\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 162.5869 - val_loss: 182.2438\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 163.8977 - val_loss: 181.2371\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 160.4605 - val_loss: 180.4401\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 155.6697 - val_loss: 179.7956\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 142.3642 - val_loss: 178.8926\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 146.0257 - val_loss: 178.0476\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 169.3420 - val_loss: 177.4619\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 141.6407 - val_loss: 176.7311\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 141.3368 - val_loss: 176.2475\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 151.6584 - val_loss: 175.5754\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 145.7778 - val_loss: 174.6408\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 148.2520 - val_loss: 174.2073\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 129.5633 - val_loss: 173.1733\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 130.9138 - val_loss: 172.7642\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 142.9206 - val_loss: 171.9340\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 143.8231 - val_loss: 171.6000\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 136.9406 - val_loss: 170.8772\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 140.2340 - val_loss: 170.0179\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 136.8610 - val_loss: 169.3146\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 134.3518 - val_loss: 168.6728\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 136.9333 - val_loss: 167.7433\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 140.1362 - val_loss: 167.0418\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 141.2984 - val_loss: 166.5656\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 130.9493 - val_loss: 165.7724\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 140.3040 - val_loss: 165.3246\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 145.1407 - val_loss: 164.4105\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 134.0602 - val_loss: 164.0337\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 145.4171 - val_loss: 163.0587\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 126.0536 - val_loss: 162.4972\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 130.4402 - val_loss: 162.0910\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 134.8439 - val_loss: 161.1219\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 123.2287 - val_loss: 160.6109\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 122.6681 - val_loss: 160.1272\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 123.3210 - val_loss: 159.5158\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 121.6210 - val_loss: 159.0962\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 126.8705 - val_loss: 158.1763\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 115.8583 - val_loss: 157.6414\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 120.5611 - val_loss: 157.1227\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 124.3664 - val_loss: 156.6329\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 126.0414 - val_loss: 155.8862\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 113.5549 - val_loss: 155.2859\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 118.2414 - val_loss: 154.9187\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 113.7202 - val_loss: 154.0861\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 111.5833 - val_loss: 153.8989\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 113.7062 - val_loss: 153.2794\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 113.5966 - val_loss: 152.9673\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 118.5479 - val_loss: 152.0332\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 107.5653 - val_loss: 151.5974\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 108.5641 - val_loss: 150.9862\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 122.6738 - val_loss: 150.7079\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 116.5981 - val_loss: 149.5905\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 120.8655 - val_loss: 149.4291\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  47 \n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 15ms/step - loss: 1498.2854 - val_loss: 1654.1725\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1437.6936 - val_loss: 1635.2749\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1394.8495 - val_loss: 1612.5758\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1427.5303 - val_loss: 1585.5784\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 1399.3642 - val_loss: 1553.5535\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1310.9917 - val_loss: 1514.9526\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1290.7463 - val_loss: 1469.3125\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1288.3024 - val_loss: 1415.5236\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1155.8511 - val_loss: 1355.7690\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1139.4971 - val_loss: 1288.0214\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1140.0727 - val_loss: 1212.3271\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1080.0270 - val_loss: 1129.9429\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1009.5377 - val_loss: 1040.7103\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 879.4297 - val_loss: 947.3265\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 737.5478 - val_loss: 852.8927\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 777.7796 - val_loss: 755.9006\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 620.4410 - val_loss: 665.3221\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 546.6288 - val_loss: 580.5692\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 479.6262 - val_loss: 503.6015\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 403.4783 - val_loss: 436.8567\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 371.1545 - val_loss: 380.4869\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 324.2601 - val_loss: 334.6266\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 286.4376 - val_loss: 299.0642\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 245.3490 - val_loss: 270.5265\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 237.2531 - val_loss: 251.6364\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 209.6731 - val_loss: 235.3765\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 219.0572 - val_loss: 222.3407\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 186.7260 - val_loss: 214.2955\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 160.9818 - val_loss: 207.6599\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 176.5471 - val_loss: 200.6126\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 172.4884 - val_loss: 196.0480\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 181.2505 - val_loss: 192.3514\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 163.0047 - val_loss: 189.3003\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 180.6298 - val_loss: 185.9358\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 173.2760 - val_loss: 182.6892\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 158.1475 - val_loss: 180.8487\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 158.2167 - val_loss: 178.5693\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 155.2325 - val_loss: 176.4653\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 160.0296 - val_loss: 174.3579\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 149.4898 - val_loss: 172.5619\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 167.8252 - val_loss: 170.3861\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 152.1522 - val_loss: 168.9066\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 173.4594 - val_loss: 168.0726\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 162.5691 - val_loss: 166.6366\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 162.4883 - val_loss: 165.3129\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 149.0484 - val_loss: 163.7352\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 155.0775 - val_loss: 162.3171\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 151.2105 - val_loss: 161.3878\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 149.2546 - val_loss: 160.3118\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 151.5032 - val_loss: 159.0099\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 152.0263 - val_loss: 158.4466\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 149.2026 - val_loss: 157.9945\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 148.9395 - val_loss: 156.7021\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 143.9892 - val_loss: 155.7450\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 144.4928 - val_loss: 155.3070\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 150.0196 - val_loss: 154.5597\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 150.0370 - val_loss: 154.1844\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 136.5602 - val_loss: 152.9936\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 138.4176 - val_loss: 151.9195\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 122.6787 - val_loss: 151.4398\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 138.3485 - val_loss: 151.1467\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 128.4107 - val_loss: 150.2208\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 138.6224 - val_loss: 149.8169\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 133.5104 - val_loss: 148.6475\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 139.1957 - val_loss: 148.5456\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 140.4070 - val_loss: 148.2531\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 132.9653 - val_loss: 146.6054\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 145.9433 - val_loss: 146.5959\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 141.9067 - val_loss: 145.6551\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 137.2965 - val_loss: 145.0140\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 139.3254 - val_loss: 144.5649\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 128.8160 - val_loss: 144.1051\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 127.5708 - val_loss: 143.7888\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 136.7518 - val_loss: 143.1491\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 141.7489 - val_loss: 142.9655\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 134.4268 - val_loss: 142.3430\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 128.3675 - val_loss: 141.6940\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 134.1476 - val_loss: 141.1998\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 129.5064 - val_loss: 141.2674\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 136.0753 - val_loss: 140.6182\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 132.8305 - val_loss: 140.0446\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 131.8712 - val_loss: 139.3345\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 128.3440 - val_loss: 139.1118\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 134.4687 - val_loss: 139.0766\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 120.7389 - val_loss: 138.6293\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 126.6119 - val_loss: 138.1341\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 114.2403 - val_loss: 137.8259\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 141.8580 - val_loss: 137.3059\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 123.7632 - val_loss: 136.7123\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 128.6870 - val_loss: 136.1751\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 134.3822 - val_loss: 136.1288\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 134.0939 - val_loss: 136.2294\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 122.5124 - val_loss: 135.6319\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 122.6463 - val_loss: 135.1973\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 133.1704 - val_loss: 134.5600\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 119.3188 - val_loss: 134.1474\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 122.8852 - val_loss: 133.9445\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 138.1374 - val_loss: 133.9010\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 121.2804 - val_loss: 133.0381\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 129.9115 - val_loss: 133.2069\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  48 \n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 17ms/step - loss: 1610.9228 - val_loss: 1506.6650\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1624.6760 - val_loss: 1491.2286\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1506.4396 - val_loss: 1475.5441\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1530.5894 - val_loss: 1457.8975\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1598.4331 - val_loss: 1437.9092\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1507.9069 - val_loss: 1414.0940\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1462.2305 - val_loss: 1385.4573\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1410.2652 - val_loss: 1351.0720\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1399.2379 - val_loss: 1310.7723\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1298.1338 - val_loss: 1263.9856\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1315.9075 - val_loss: 1210.2657\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1337.0283 - val_loss: 1149.0671\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1147.5310 - val_loss: 1079.2852\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1121.1527 - val_loss: 1000.4360\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1148.0477 - val_loss: 915.9472\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1050.3974 - val_loss: 827.3212\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 851.0615 - val_loss: 739.7640\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 745.7480 - val_loss: 654.2903\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 663.2424 - val_loss: 571.1124\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 584.7917 - val_loss: 496.5737\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 552.7709 - val_loss: 431.7971\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 454.5177 - val_loss: 379.0160\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 427.7536 - val_loss: 334.6999\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 368.9640 - val_loss: 301.1293\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 330.3747 - val_loss: 276.2378\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 303.6874 - val_loss: 257.9533\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 294.8419 - val_loss: 243.0507\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 272.8940 - val_loss: 233.2765\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 280.0168 - val_loss: 224.4765\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 253.9796 - val_loss: 217.3855\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 269.0733 - val_loss: 212.0940\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 248.3960 - val_loss: 207.3157\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 225.2134 - val_loss: 202.9987\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 239.7109 - val_loss: 199.1497\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 221.6181 - val_loss: 195.3227\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 243.8313 - val_loss: 192.4756\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 239.1559 - val_loss: 189.3537\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 245.8460 - val_loss: 186.1541\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 233.1653 - val_loss: 183.7760\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 213.2066 - val_loss: 181.4720\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 221.8672 - val_loss: 178.4088\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 215.0661 - val_loss: 175.9240\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 210.7750 - val_loss: 174.1372\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 208.3188 - val_loss: 170.9830\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 217.2267 - val_loss: 169.1095\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 189.4696 - val_loss: 166.4056\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 201.7274 - val_loss: 164.6783\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 200.1136 - val_loss: 162.6521\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 184.9671 - val_loss: 160.6676\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 202.1835 - val_loss: 158.9405\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 187.8725 - val_loss: 157.3590\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 192.4853 - val_loss: 155.3942\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 185.2541 - val_loss: 153.3548\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 181.1278 - val_loss: 151.5484\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 183.2386 - val_loss: 149.8328\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 186.3950 - val_loss: 148.4963\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 168.4559 - val_loss: 147.2200\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 173.6087 - val_loss: 145.7715\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 179.2334 - val_loss: 143.9461\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 174.7370 - val_loss: 143.2589\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 161.6535 - val_loss: 141.5851\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 161.4089 - val_loss: 140.6229\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 150.6793 - val_loss: 139.7833\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 161.1185 - val_loss: 138.1286\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 154.9647 - val_loss: 137.3084\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 166.5996 - val_loss: 135.7947\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 161.9155 - val_loss: 135.1741\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 162.9781 - val_loss: 134.1173\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 156.0172 - val_loss: 133.3793\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 162.9076 - val_loss: 132.0936\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 166.3903 - val_loss: 131.5390\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 166.3883 - val_loss: 130.3452\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 160.4362 - val_loss: 128.9642\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 144.5121 - val_loss: 128.5866\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 156.8264 - val_loss: 127.6343\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 161.3938 - val_loss: 126.5512\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 145.3674 - val_loss: 125.8496\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 140.0171 - val_loss: 125.1100\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 162.1491 - val_loss: 124.6605\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 154.7601 - val_loss: 123.7759\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 157.4556 - val_loss: 122.8922\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 147.5261 - val_loss: 122.3266\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 153.8813 - val_loss: 121.9238\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 145.4685 - val_loss: 121.3913\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 147.7848 - val_loss: 120.4995\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 144.5539 - val_loss: 119.9746\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 133.5648 - val_loss: 119.5851\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 142.3897 - val_loss: 118.8682\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 135.5068 - val_loss: 118.3786\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 137.9949 - val_loss: 118.2845\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 141.3583 - val_loss: 117.8533\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 136.3403 - val_loss: 116.8315\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 130.0602 - val_loss: 117.0238\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 149.6096 - val_loss: 116.2869\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 142.1002 - val_loss: 116.3839\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 140.0014 - val_loss: 115.4838\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 123.5207 - val_loss: 115.2027\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 139.4740 - val_loss: 114.6563\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 137.8649 - val_loss: 113.9675\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 144.2228 - val_loss: 113.5283\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  49 \n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 14ms/step - loss: 1567.1856 - val_loss: 1466.1101\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1677.9904 - val_loss: 1446.5594\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1590.2910 - val_loss: 1425.6371\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1541.2517 - val_loss: 1401.2030\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1544.9783 - val_loss: 1372.5684\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1481.1999 - val_loss: 1339.2961\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1393.7327 - val_loss: 1300.8564\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1421.0704 - val_loss: 1256.4894\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1391.8166 - val_loss: 1208.5792\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1259.2421 - val_loss: 1155.0592\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1218.3519 - val_loss: 1096.5973\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1189.8897 - val_loss: 1033.2667\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1090.5379 - val_loss: 965.9442\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1071.2345 - val_loss: 896.9564\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 940.2019 - val_loss: 825.4817\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 904.4964 - val_loss: 753.5916\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 803.8485 - val_loss: 682.0123\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 746.2635 - val_loss: 614.7496\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 666.7598 - val_loss: 552.3499\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 597.4900 - val_loss: 494.2897\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 514.3164 - val_loss: 444.0412\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 452.9351 - val_loss: 400.4682\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 456.2012 - val_loss: 362.9846\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 409.3300 - val_loss: 332.5490\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 361.7194 - val_loss: 308.3306\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 376.0904 - val_loss: 287.7946\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 333.3733 - val_loss: 271.4255\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 295.2874 - val_loss: 258.0060\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 306.4483 - val_loss: 246.6229\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 267.6633 - val_loss: 236.7664\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 260.9659 - val_loss: 228.4056\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 233.4630 - val_loss: 221.2106\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 278.9201 - val_loss: 214.6320\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 222.3134 - val_loss: 209.2828\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 236.0494 - val_loss: 204.0434\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 233.5190 - val_loss: 199.1380\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 204.7150 - val_loss: 194.9847\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 213.5961 - val_loss: 191.2056\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 200.1691 - val_loss: 187.4318\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 211.5147 - val_loss: 184.3420\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 199.7965 - val_loss: 181.9223\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 193.7892 - val_loss: 179.4229\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 203.7281 - val_loss: 176.7518\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 192.4787 - val_loss: 174.3172\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 186.3706 - val_loss: 172.0654\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 202.2568 - val_loss: 169.9432\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 186.3588 - val_loss: 168.0955\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 189.5424 - val_loss: 166.1285\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 175.4552 - val_loss: 164.4240\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 165.1455 - val_loss: 162.7698\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 154.7335 - val_loss: 160.5707\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 158.0743 - val_loss: 159.2430\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 169.1823 - val_loss: 157.5114\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 162.4910 - val_loss: 156.2878\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 168.2144 - val_loss: 154.3670\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 177.2284 - val_loss: 153.3603\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 149.4123 - val_loss: 152.4550\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 147.6123 - val_loss: 150.3242\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 159.3532 - val_loss: 148.5688\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 150.7427 - val_loss: 146.9708\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 138.9282 - val_loss: 146.0822\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 150.2335 - val_loss: 144.2110\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 148.6174 - val_loss: 142.7792\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 138.7141 - val_loss: 141.4928\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 136.7922 - val_loss: 140.0911\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 135.3296 - val_loss: 138.2318\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 135.2605 - val_loss: 136.8677\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 136.6086 - val_loss: 135.6294\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 136.6226 - val_loss: 134.5829\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 123.6932 - val_loss: 133.2405\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 125.9729 - val_loss: 131.9525\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 131.6867 - val_loss: 131.3427\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 128.6536 - val_loss: 129.8312\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 130.4104 - val_loss: 127.6618\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 117.4572 - val_loss: 126.4471\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 126.0785 - val_loss: 125.3069\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 129.1166 - val_loss: 125.2020\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 122.4594 - val_loss: 123.0847\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 117.9183 - val_loss: 122.0791\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 114.9371 - val_loss: 121.1529\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 107.5831 - val_loss: 119.8087\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 116.0870 - val_loss: 118.4906\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 108.3518 - val_loss: 118.1166\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 96.4242 - val_loss: 116.7417\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 111.4269 - val_loss: 115.1699\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 111.8840 - val_loss: 114.0929\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 112.8810 - val_loss: 113.3932\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 103.6148 - val_loss: 112.5398\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 105.6476 - val_loss: 111.5461\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 113.8920 - val_loss: 110.7669\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 92.2245 - val_loss: 110.1475\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 104.4928 - val_loss: 108.8496\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 102.6678 - val_loss: 108.1155\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 100.6765 - val_loss: 107.2400\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 101.3884 - val_loss: 106.4780\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 98.1037 - val_loss: 106.1552\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 97.8728 - val_loss: 105.1240\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 99.0499 - val_loss: 104.4717\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 94.8139 - val_loss: 103.7637\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 99.3482 - val_loss: 103.4550\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Model #  50 \n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 15ms/step - loss: 1630.8266 - val_loss: 1616.2838\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1563.1717 - val_loss: 1593.9473\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1560.0214 - val_loss: 1570.5848\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1450.7759 - val_loss: 1545.4749\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1537.3254 - val_loss: 1517.1532\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1474.7743 - val_loss: 1485.7770\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1442.2870 - val_loss: 1450.3903\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1335.1414 - val_loss: 1410.4585\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1357.8357 - val_loss: 1364.7218\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1284.0204 - val_loss: 1313.4945\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1201.0085 - val_loss: 1256.5326\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1210.1534 - val_loss: 1192.7709\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1081.7203 - val_loss: 1126.1696\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1082.8908 - val_loss: 1054.7871\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 966.0627 - val_loss: 982.1962\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 899.3102 - val_loss: 908.9636\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 794.5846 - val_loss: 836.1267\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 734.3136 - val_loss: 765.7811\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 672.2498 - val_loss: 698.9313\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 589.4888 - val_loss: 636.9355\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 576.9910 - val_loss: 576.9587\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 509.0721 - val_loss: 523.1866\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 442.0453 - val_loss: 474.0074\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 423.9069 - val_loss: 428.5012\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 401.4848 - val_loss: 388.4226\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 335.6635 - val_loss: 353.0195\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 325.0970 - val_loss: 321.2474\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 306.3286 - val_loss: 295.1780\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 255.4590 - val_loss: 273.6796\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 239.5049 - val_loss: 256.3740\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 242.0825 - val_loss: 242.2998\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 215.4799 - val_loss: 231.7779\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 217.5978 - val_loss: 223.8873\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 208.2568 - val_loss: 216.4441\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 196.9338 - val_loss: 210.6835\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 191.8445 - val_loss: 205.6199\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 188.3929 - val_loss: 201.4331\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 180.7493 - val_loss: 198.3350\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 180.4136 - val_loss: 195.1531\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 178.4244 - val_loss: 192.1944\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 190.6547 - val_loss: 189.9595\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 171.3380 - val_loss: 187.9147\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 176.6030 - val_loss: 185.2997\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 183.9763 - val_loss: 183.3486\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 175.2966 - val_loss: 181.6253\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 167.3451 - val_loss: 179.7626\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 169.8445 - val_loss: 178.3311\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 171.1115 - val_loss: 176.6116\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 159.1773 - val_loss: 175.1486\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 152.1408 - val_loss: 173.9582\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 159.0922 - val_loss: 172.9591\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 170.5465 - val_loss: 171.3326\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 156.8376 - val_loss: 170.5912\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 143.0150 - val_loss: 169.1581\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 150.4305 - val_loss: 168.5601\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 147.8292 - val_loss: 166.9984\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 156.5854 - val_loss: 165.9555\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 141.3009 - val_loss: 164.8762\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 156.9170 - val_loss: 164.0051\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 141.4623 - val_loss: 163.1448\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 139.7409 - val_loss: 162.0607\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 143.3610 - val_loss: 161.6399\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 148.7238 - val_loss: 160.8915\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 149.0586 - val_loss: 160.2016\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 146.6749 - val_loss: 159.6319\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 142.9182 - val_loss: 158.6771\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 128.8744 - val_loss: 157.6087\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 134.3134 - val_loss: 156.6634\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 140.0054 - val_loss: 156.5827\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 125.8378 - val_loss: 155.9436\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 135.9889 - val_loss: 155.0744\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 134.7203 - val_loss: 154.4447\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 136.3099 - val_loss: 154.2287\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 130.1418 - val_loss: 153.8072\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 132.1018 - val_loss: 152.9878\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 127.6758 - val_loss: 152.8785\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 144.0235 - val_loss: 151.9675\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 132.7571 - val_loss: 151.6806\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 137.0687 - val_loss: 150.8490\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 127.3804 - val_loss: 150.2880\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 135.8924 - val_loss: 149.6945\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 136.2711 - val_loss: 149.2908\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 131.5656 - val_loss: 148.7325\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 134.2547 - val_loss: 148.5469\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 129.5420 - val_loss: 148.1174\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 125.7645 - val_loss: 147.4714\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 122.7370 - val_loss: 146.4676\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 145.9800 - val_loss: 145.7760\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 127.2147 - val_loss: 145.7886\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 121.6359 - val_loss: 145.1595\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 114.2042 - val_loss: 145.2794\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 123.5379 - val_loss: 143.9445\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 120.7049 - val_loss: 143.6500\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 129.3844 - val_loss: 143.3816\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 112.9204 - val_loss: 142.4494\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 125.8830 - val_loss: 142.7284\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 124.3569 - val_loss: 141.9990\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 117.8179 - val_loss: 141.2916\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 110.0561 - val_loss: 141.2983\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 123.7497 - val_loss: 140.8853\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Total Execution Time :  0:08:53.699213\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zc8vHYZC5Vzz",
        "outputId": "23b39f69-d9e4-4b38-e65f-560c66bc90c2"
      },
      "source": [
        "# Calculate the Mean of the MSE of 50 models\r\n",
        "mean_of_mse = stats.mean(list_of_mse)\r\n",
        "\r\n",
        "# Calculate the Standard Deviation of the MSE of 50 models\r\n",
        "std_of_mse = stats.stdev(list_of_mse)\r\n",
        "\r\n",
        "# Print the Mean and Standard Deviation of MSE of 50 models\r\n",
        "print('\\n\\nMean of the MSE of 50 Models : ' , str(mean_of_mse))\r\n",
        "print('Standard Deviation of MSE of 50 Models : ' , str(std_of_mse))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Mean of the MSE of 50 Models :  133.6545873251317\n",
            "Standard Deviation of MSE of 50 Models :  15.60317835661413\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UD6V9WKx5JSR"
      },
      "source": [
        "#### <font color = green> Comparision of Mean of MSE with Mean of MSE with PART C </font>\r\n",
        "<table style=\"width:30%\">\r\n",
        "  <tr>\r\n",
        "    <th>Mean of MSE of PART A</th>\r\n",
        "    <th>Mean of MSE of PART B</th>\r\n",
        "    <th>Mean of MSE of PART C</th>\r\n",
        "    <th>Mean of MSE of PART D</th>\r\n",
        "  </tr>\r\n",
        "  <tr>\r\n",
        "    <td>177.27</td>\r\n",
        "    <td>176.27</td>\r\n",
        "    <td>133.79</td>\r\n",
        "    <td>133.65</td>\r\n",
        "  </tr>\r\n",
        "</table>\r\n",
        "\r\n",
        "The table above compares the Mean of **MSE for PART A**, **Mean of MSE for PART B**, **Mean of MSE for PART C** and **Mean of MSE for PART D**. As can be seen, the value of Mean of MSE of PART D is marginally smaller than that of PART C and is the smallest value obtained. This shows that the effect of **normalizing the features** as well as **increasing the number of epochs by 2** yield the best results in terms of the performance of the regression model and helps it in finding the line of best fit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51Q6yhV95dCm"
      },
      "source": [
        "# <font color = ac36e3> END NOTE </font>\r\n",
        "\r\n",
        "Although the results above table show that the best performance is achieved by normalizing the features, increasing the number of epochs **and** increasing the number of hidden layers, this might not be decisive. Repeating ***TASK 2*** for **PART A**, **PART B**, **PART C** and **PART D** several times shows different results. However, for the purposes of this project, those results are not included. "
      ]
    }
  ]
}