{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WEEK2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KWpESuO-oFu"
      },
      "source": [
        "#1. Regression is the process of predicting a continous value\r\n",
        "##1.1 Type of regression models\r\n",
        "- Simple Regression:\r\n",
        "  - Simple Linear Regression \r\n",
        "  - Simple non-linear Regression\r\n",
        "- Multiple Regression:\r\n",
        "  - Multiple Linear Regression \r\n",
        "  - Multiple non-linear Regression\r\n",
        "\r\n",
        "##1.2 Applications of regression\r\n",
        "- Sale forecasting\r\n",
        "- Satisfaction analysis\r\n",
        "- Price estimation\r\n",
        "- Employment income\r\n",
        "\r\n",
        "##1.3 Regression algorithms\r\n",
        "- Ordinal regression\r\n",
        "- Poisson regression\r\n",
        "- Fast forest quantile regression\r\n",
        "- Linear, Polynomial, Lasso, Stepwise, Ridge regression\r\n",
        "- Bayersian network regression\r\n",
        "- Neral network regression\r\n",
        "- Decision forest regression\r\n",
        "- Boosted decision tree regression \r\n",
        "- KNN \r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGI1BJjtBr68"
      },
      "source": [
        "#2. Simple Linear Regression\r\n",
        "$\\hat{y}= \\theta_{0}+\\theta_{1}.x_{1}$\r\n",
        "\r\n",
        "find the best fit\r\n",
        "\r\n",
        "$MSE = \\frac{1}{n}.\\sum_{i=1}^{n} (y_{i}-\\hat{y}_{i})^2$\r\n",
        "\r\n",
        "$\\theta_{1}= \\frac{\\sum_{i=1}^{s} (x_{i}-\\overline{x}_{i})(y_{i}-\\overline{y}_{i})}{\\sum_{i=1}^{s} (x_{i}-\\overline{x}_{i})^2}$\r\n",
        "\r\n",
        "$\\theta_{0}=\\overline{y}-\\theta_{1}.\\overline{x}$\r\n",
        "\r\n",
        "=> Pros of linear regression \r\n",
        "- very fast\r\n",
        "- No parameter tuning\r\n",
        "- Easy to understand, and highly interpretable\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H18hhBWJEZ26"
      },
      "source": [
        "#3. Model Evaluate and Regression model\r\n",
        "#3.1. Model evaluation approaches\r\n",
        "- Train and Test on the Same Dataset\r\n",
        "- Train/Test Split\r\n",
        "=> Models: Regression Evaluation Metrics\r\n",
        "\r\n",
        ". Traing Accuracy\r\n",
        "  - High training accuracy isn't necessarily a good thing\r\n",
        "  - Result of over-fitting (the model is overly trained to the dataset, which may capture noise and produce a non generalized model)\r\n",
        ". Out-of-Sample Accuracy\r\n",
        "  - It's important that our models have a high, out-of-sample accuracy\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOJjtSlyT3eR"
      },
      "source": [
        "#4. Evaluation Metrics in Regression Models\r\n",
        "\r\n",
        "$MAE=\\frac{1}{n}\\sum_{i=1}^{n}|y_{i}-\\hat{y}_{i}|$\r\n",
        "\r\n",
        "$MSE=\\frac{1}{n}\\sum_{i=1}^{n}(y_{i}-\\hat{y}_{i})^2$\r\n",
        "\r\n",
        "$RMSE=\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_{i}-\\hat{y}_{i})^2}$\r\n",
        "\r\n",
        "$RAE=\\frac{\\sum_{i=1}^{n}|y_{i}-\\hat{y}_{i}|}{\\sum_{i=1}^{n}|y_{i}-\\overline{y}_{i}|}$\r\n",
        "\r\n",
        "$RSE=\\frac{\\sum_{i=1}^{n}(y_{i}-\\hat{y}_{i})^2}{\\sum_{i=1}^{n}(y_{i}-\\overline{y}_{i})^2}$\r\n",
        "\r\n",
        "$R_squared: R^2 = 1- RSE$"
      ]
    }
  ]
}